{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료 : KoNLPY 공식홈페이지\n",
    "https://konlpy.org/ko/v0.4.3/api/konlpy.tag/#konlpy.tag._kkma.Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:39:50.912079Z",
     "start_time": "2021-03-25T15:39:50.229713Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train= pd.read_csv('korean-hate-speech/labeled/train.tsv' ,sep='\\t')\n",
    "dev= pd.read_csv('korean-hate-speech/labeled/dev.tsv' ,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hannanum\n",
    "\n",
    "#### KAIST의 SWRC (Semantic Web Research Center)에서 Java로 작성된 형태소 분석기 및 POS 태거\n",
    "- analyze(phrase) : 구문 분석기\n",
    "\n",
    "- morphs(phrase) : 형태소 분석 Parse phrase to morphemes.\n",
    "\n",
    "- nouns(phrase) : 명사 추출기\n",
    "\n",
    "- pos(phrase, ntags=9, flatten=True) :POS tagger.(HMM 기반이며, 태그의 확률을 계산함)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:30:50.629239Z",
     "start_time": "2021-03-25T15:30:47.669637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "구문 : \n",
      "[[[('롯데마트', 'ncn'), ('의', 'jcm')], [('롯데마트의', 'ncn')], [('롯데마트', 'nqq'), ('의', 'jcm')], [('롯데마트의', 'nqq')]], [[('흑마늘', 'ncn')], [('흑마늘', 'nqq')]], [[('양념', 'ncn')]], [[('치킨', 'ncn'), ('이', 'jcc')], [('치킨', 'ncn'), ('이', 'jcs')], [('치킨', 'ncn'), ('이', 'ncn')]], [[('논란', 'ncpa'), ('이', 'jcc')], [('논란', 'ncpa'), ('이', 'jcs')], [('논란', 'ncpa'), ('이', 'ncn')]], [[('되', 'nbu'), ('고', 'jcj')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecc')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecs')], [('되', 'nbu'), ('이', 'jp'), ('고', 'ecx')], [('되', 'paa'), ('고', 'ecc')], [('되', 'paa'), ('고', 'ecs')], [('되', 'paa'), ('고', 'ecx')], [('되', 'pvg'), ('고', 'ecc')], [('되', 'pvg'), ('고', 'ecs')], [('되', 'pvg'), ('고', 'ecx')], [('되', 'px'), ('고', 'ecc')], [('되', 'px'), ('고', 'ecs')], [('되', 'px'), ('고', 'ecx')]], [[('있', 'paa'), ('다', 'ef')], [('있', 'px'), ('다', 'ef')]], [[('.', 'sf')], [('.', 'sy')]]]\n",
      "\n",
      "형태소 : \n",
      "['롯데마트', '의', '흑마늘', '양념', '치킨', '이', '논란', '이', '되', '고', '있', '다', '.']\n",
      "\n",
      "명사 : \n",
      "['롯데마트', '흑마늘', '양념', '치킨', '논란']\n",
      "\n",
      "POS tagger : \n",
      "[('롯데마트', 'N'), ('의', 'J'), ('흑마늘', 'N'), ('양념', 'N'), ('치킨', 'N'), ('이', 'J'), ('논란', 'N'), ('이', 'J'), ('되', 'P'), ('고', 'E'), ('있', 'P'), ('다', 'E'), ('.', 'S')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "print(\"\\n구문 : \")\n",
    "print(hannanum.analyze('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))\n",
    "print(\"\\n형태소 : \")\n",
    "print(hannanum.morphs('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))\n",
    "print(\"\\n명사 : \")\n",
    "print(hannanum.nouns('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))\n",
    "print(\"\\nPOS tagger : \")\n",
    "print(hannanum.pos('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Kkma\n",
    "\n",
    "#### 서울대 지능형 데이터 시스템 (IDS) 연구실에서 개발 한 자바로 작성된 형태소 분석기 및 자연어 처리 시스템\n",
    "- morphs(phrase) : Parse phrase to morphemes.\n",
    "\n",
    "- nouns(phrase) : Noun extractor.\n",
    "\n",
    "- pos(phrase, flatten=True) :POS tagger.\n",
    "\n",
    "- sentences(phrase) :Sentence detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:31:01.901370Z",
     "start_time": "2021-03-25T15:30:50.631235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "형태소 :\n",
      "['롯데', '마트', '의', '흑', '마늘', '양념', '치킨', '이', '논란', '이', '되', '고', '있', '다', '.']\n",
      "\n",
      "명사 :\n",
      "['롯데', '롯데마트', '마트', '흑', '흑마늘', '마늘', '양념', '치킨', '논란']\n",
      "\n",
      "pos태거 :\n",
      "[('롯데', 'NNP'), ('마트', 'NNG'), ('의', 'JKG'), ('흑', 'NNG'), ('마늘', 'NNG'), ('양념', 'NNG'), ('치킨', 'NNG'), ('이', 'JKS'), ('논란', 'NNG'), ('이', 'JKC'), ('되', 'VV'), ('고', 'ECE'), ('있', 'VXV'), ('다', 'EFN'), ('.', 'SF')]\n",
      "\n",
      "문장감지 :\n",
      "['롯데 마트의 흑 마늘 양념 치킨이 논란이 되고 있다.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "print(\"\\n형태소 :\")\n",
    "print(kkma.morphs('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))\n",
    "print(\"\\n명사 :\")\n",
    "print(kkma.nouns('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))\n",
    "print(\"\\npos태거 :\")\n",
    "print(kkma.pos('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))\n",
    "print(\"\\n문장감지 :\")\n",
    "print(kkma.sentences('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **명사만 추출한 경우, 합성어에 존재하는 모든 명사를 분해해서 제공 (빈도를 기반으로 분석한다면 처리해야 함)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Komoran\n",
    "\n",
    "#### 2013 년부터 Shineware에서 개발한 Java로 작성된 비교적 새로운 오픈 소스 한국어 형태소 분석기\n",
    "\n",
    "- morphs : 형태소\n",
    "- nouns : 명사\n",
    "- pos : pos태거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:31:04.238850Z",
     "start_time": "2021-03-25T15:31:01.905091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "형태소 :pos\n",
      "['롯데마트', '의', '흑', '마늘', '양념', '치킨', '이', '논란', '이', '되', '고', '있', '다', '.']\n",
      "\n",
      "명사 :pos\n",
      "['롯데마트', '흑', '마늘', '양념', '치킨', '논란']\n",
      "\n",
      "pos태거 :pos\n",
      "[('롯데마트', 'NNP'), ('의', 'JKG'), ('흑', 'NNG'), ('마늘', 'NNP'), ('양념', 'NNG'), ('치킨', 'NNP'), ('이', 'JKS'), ('논란', 'NNG'), ('이', 'JKS'), ('되', 'VV'), ('고', 'EC'), ('있', 'VX'), ('다', 'EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "Komoran = Komoran()\n",
    "print(\"\\n형태소 :pos\")\n",
    "print(Komoran.morphs('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))\n",
    "print(\"\\n명사 :pos\")\n",
    "print(Komoran.nouns('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))\n",
    "print(\"\\npos태거 :pos\")\n",
    "print(Komoran.pos('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Mecab\n",
    "\n",
    "#### 일본 형태소 분석가에 의해 개발되었지만 추후 한국어에 적합하게 수정됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:31:04.253844Z",
     "start_time": "2021-03-25T15:31:04.240846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "형태소 :\n",
      "['롯데마트', '의', '흑', '마늘', '양념', '치킨', '이', '논란', '이', '되', '고', '있', '다']\n",
      "\n",
      "명사 :\n",
      "['롯데마트', '마늘', '양념', '치킨', '논란']\n",
      "\n",
      "pos태거 :\n",
      "[('롯데마트', 'NNP'), ('의', 'JKG'), ('흑', 'IC'), ('마늘', 'NNG'), ('양념', 'NNG'), ('치킨', 'NNG'), ('이', 'JKS'), ('논란', 'NNG'), ('이', 'JKS'), ('되', 'VV'), ('고', 'EC'), ('있', 'VX'), ('다', 'EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab(dicpath=\"C:\\\\mecab\\\\mecab-ko-dic\")\n",
    "\n",
    "print(\"\\n형태소 :\")\n",
    "print(mecab.morphs('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다'))\n",
    "print(\"\\n명사 :\")\n",
    "print(mecab.nouns('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))\n",
    "print(\"\\npos태거 :\")\n",
    "print(mecab.pos('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Okt : 과거 Twitter\n",
    "\n",
    "#### 한국어 트위터 기반으로 작성된 라이브러리\n",
    "- morphs : 형태소\n",
    "- nouns : 명사\n",
    "- pos : pos태거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:31:10.833218Z",
     "start_time": "2021-03-25T15:31:04.255805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "형태소 :pos\n",
      "['롯데', '마트', '의', '흑마', '늘', '양념', '치킨', '이', '논란', '이', '되고', '있다', '.']\n",
      "\n",
      "명사 :pos\n",
      "['롯데', '마트', '흑마', '늘', '양념', '치킨', '논란']\n",
      "\n",
      "pos태거 :pos\n",
      "[('롯데', 'Noun'), ('마트', 'Noun'), ('의', 'Josa'), ('흑마', 'Noun'), ('늘', 'Noun'), ('양념', 'Noun'), ('치킨', 'Noun'), ('이', 'Josa'), ('논란', 'Noun'), ('이', 'Josa'), ('되고', 'Verb'), ('있다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "print(\"\\n형태소 :pos\")\n",
    "print(okt.morphs('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))\n",
    "print(\"\\n명사 :pos\")\n",
    "print(okt.nouns('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))\n",
    "print(\"\\npos태거 :pos\")\n",
    "print(okt.pos('롯데마트의 흑마늘 양념 치킨이 논란이 되고 있다.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 명사만 추출한 경우, 합성어에 존재하는 모든 명사를 분해해서 제공 (빈도를 기반으로 분석한다면 처리해야 함)\n",
    "- 심지어, 흑마늘에서 '흑마'와 '늘'을 따로 구분함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석기 속도 비교 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:31:10.880093Z",
     "start_time": "2021-03-25T15:31:10.836211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>contain_gender_bias</th>\n",
       "      <th>bias</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(현재 호텔주인 심정) 아18 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속...</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...못된 넘들...남의 고통을 즐겼던 넘들..이젠 마땅한 처벌을 받아야지..,그래...</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1. 사람 얼굴 손톱으로 긁은것은 인격살해이고2. 동영상이 몰카냐? 메걸리안들 생각...</td>\n",
       "      <td>True</td>\n",
       "      <td>gender</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>힘내세요~ 응원합니다!!</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7892</th>\n",
       "      <td>힘내세요~~삼가 고인의 명복을 빕니다..</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7893</th>\n",
       "      <td>힘내세용 ^^ 항상 응원합니닷 ^^ !</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7894</th>\n",
       "      <td>힘내소...연기로 답해요.나도 53살 인데 이런일 저런일 다 있더라구요.인격을 믿습...</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7895</th>\n",
       "      <td>힘들면 관뒀어야지 그게 현명한거다</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7896 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comments  contain_gender_bias  \\\n",
       "0     (현재 호텔주인 심정) 아18 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속...                False   \n",
       "1     ....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...                False   \n",
       "2     ...못된 넘들...남의 고통을 즐겼던 넘들..이젠 마땅한 처벌을 받아야지..,그래...                False   \n",
       "3                    1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데                False   \n",
       "4     1. 사람 얼굴 손톱으로 긁은것은 인격살해이고2. 동영상이 몰카냐? 메걸리안들 생각...                 True   \n",
       "...                                                 ...                  ...   \n",
       "7891                                      힘내세요~ 응원합니다!!                False   \n",
       "7892                             힘내세요~~삼가 고인의 명복을 빕니다..                False   \n",
       "7893                              힘내세용 ^^ 항상 응원합니닷 ^^ !                False   \n",
       "7894  힘내소...연기로 답해요.나도 53살 인데 이런일 저런일 다 있더라구요.인격을 믿습...                False   \n",
       "7895                                 힘들면 관뒀어야지 그게 현명한거다                False   \n",
       "\n",
       "        bias  hate  \n",
       "0     others  hate  \n",
       "1       none  none  \n",
       "2       none  hate  \n",
       "3       none  none  \n",
       "4     gender  hate  \n",
       "...      ...   ...  \n",
       "7891    none  none  \n",
       "7892    none  none  \n",
       "7893    none  none  \n",
       "7894    none  none  \n",
       "7895    none  none  \n",
       "\n",
       "[7896 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7896개 데이터셋 적용\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 시간 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-25T15:30:53.939Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagger name =   hannanum, 36.426 secs\n",
      "tagger name =       kkma, 285.910 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from konlpy.tag import Hannanum,Kkma, Mecab,Twitter, Komoran\n",
    "\n",
    "library_list = [('hannanum',Hannanum()),\n",
    "                ('kkma', Kkma()), ('Komoran', Komoran()),\n",
    "                ('mecab',Mecab(dicpath=\"C:\\\\mecab\\\\mecab-ko-dic\")),\n",
    "                ('twitter', Twitter())]\n",
    "results = []\n",
    "for name, tagger in library_list:\n",
    "    nouns = []\n",
    "    process_time = time.time()\n",
    "    \n",
    "    for text in train['comments']:\n",
    "        nouns.append(tagger.nouns(text))\n",
    "    process_time = time.time() - process_time\n",
    "    print('tagger name = %10s, %.3f secs' % (name, process_time))\n",
    "    results.append((name,process_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-25T15:30:54.403Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, tagger in library_list:\n",
    "    nouns = []\n",
    "    process_time = time.time()\n",
    "    \n",
    "    for text in train['comments']:\n",
    "        nouns.append(tagger.nouns(text))\n",
    "    process_time = time.time() - process_time\n",
    "    print('tagger name = %10s, %.3f secs' % (name, process_time))\n",
    "    results.append((name,process_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5가지 명사 분석 비교\n",
    "\n",
    "실질적으로 악플데이터셋을 분석하기 위해서 악플 데이터셋에 최적화된 형태소 분석기가 필요함<br>\n",
    "즉, 아래 5가지 샘플을 통해 각각의 형태소 분석기의 성능을 비교하여 추후 사용할 라이브러리를 최종선정하고자 함\n",
    "1. '(현재 호텔주인 심정) 아18 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속 추모받네....'\n",
    "    - 띄어쓰기 올바르지 않은 구조, 18의 욕설\n",
    "\n",
    "\n",
    "2. '....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을 미처 알지못했네요ㅠ'\n",
    "     - 띄어쓰기 올바르지 않은 구조\n",
    "\n",
    "\n",
    "3. '1. 사람 얼굴 손톱으로 긁은것은 인격살해이고2. 동영상이 몰카냐? 메걸리안들 생각이 없노'\n",
    "    - 숫자, 매걸리안 등 새로운 단어\n",
    "  \n",
    "  \n",
    "4. '10+8 진짜 이승기랑 비교된다'\n",
    "    - 숫자로 비꼬는 문장\n",
    "  \n",
    "  \n",
    "5. '180이하 호빗 한남들은 결혼 하지마셈 ㅋ 돈없으면 연애도 하지마셈 ㅋ 니들 호빗 유전자 받아서 고통받을 네 후손은 뭔죄임?'\n",
    "    - ㅋ 하나 있는 문장, 음슴체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:39:22.773549Z",
     "start_time": "2021-03-25T15:39:22.766567Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = ['(현재 호텔주인 심정) 아18 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속 추모받네....',\n",
    "        '....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을 미처 알지못했네요ㅠ',\n",
    "        '1. 사람 얼굴 손톱으로 긁은것은 인격살해이고2. 동영상이 몰카냐? 메걸리안들 생각이 없노',\n",
    "        '10+8 진짜 이승기랑 비교된다',\n",
    "        '180이하 호빗 한남들은 결혼 하지마셈 ㅋ 돈없으면 연애도 하지마셈 ㅋ 니들 호빗 유전자 받아서 고통받을 네 후손은 뭔죄임?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:39:42.312960Z",
     "start_time": "2021-03-25T15:39:23.555503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagger name =   hannanum, 1.152 secs\n",
      "tagger name =       kkma, 11.624 secs\n",
      "tagger name =    Komoran, 0.025 secs\n",
      "tagger name =      mecab, 0.002 secs\n",
      "tagger name =        Okt, 3.147 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from konlpy.tag import Hannanum,Kkma, Mecab,Okt, Komoran\n",
    "\n",
    "library_list = [('hannanum',Hannanum()),\n",
    "                ('kkma', Kkma()), ('Komoran', Komoran()),\n",
    "                ('mecab',Mecab(dicpath=\"C:\\\\mecab\\\\mecab-ko-dic\")),\n",
    "                ('Okt', Okt())]\n",
    "results = []\n",
    "for name, tagger in library_list:\n",
    "    nouns = []\n",
    "    process_time = time.time()\n",
    "    \n",
    "    for text in texts:\n",
    "        nouns.append(tagger.nouns(text))\n",
    "    process_time = time.time() - process_time\n",
    "    print('tagger name = %10s, %.3f secs' % (name, process_time))\n",
    "    results.append((name,nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:43:35.201766Z",
     "start_time": "2021-03-25T15:43:35.181808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>형태소 분석기</th>\n",
       "      <th>내용</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>형태소 분석기</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hannanum</th>\n",
       "      <td>hannanum</td>\n",
       "      <td>[[호텔주인, 심정, 아18, 마른하늘, 날벼락맞고, 호텔망하게생겼는데, 누, 추모...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kkma</th>\n",
       "      <td>kkma</td>\n",
       "      <td>[[현재, 호텔, 호텔주인, 주인, 심정, 18, 마른, 마른하늘, 하늘, 날벼락,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Komoran</th>\n",
       "      <td>Komoran</td>\n",
       "      <td>[[호텔, 주인, 심정, 아, 18, 하늘, 날벼락, 호텔, 망, 추모], [한국,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mecab</th>\n",
       "      <td>mecab</td>\n",
       "      <td>[[현재, 호텔, 주인, 심정, 난, 하늘, 날벼락, 호텔, 누군, 추모], [한국...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Okt</th>\n",
       "      <td>Okt</td>\n",
       "      <td>[[현재, 호텔, 주인, 심정, 난, 마른하늘, 날벼락, 호텔, 누, 계속, 추모]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           형태소 분석기                                                 내용\n",
       "형태소 분석기                                                              \n",
       "hannanum  hannanum  [[호텔주인, 심정, 아18, 마른하늘, 날벼락맞고, 호텔망하게생겼는데, 누, 추모...\n",
       "kkma          kkma  [[현재, 호텔, 호텔주인, 주인, 심정, 18, 마른, 마른하늘, 하늘, 날벼락,...\n",
       "Komoran    Komoran  [[호텔, 주인, 심정, 아, 18, 하늘, 날벼락, 호텔, 망, 추모], [한국,...\n",
       "mecab        mecab  [[현재, 호텔, 주인, 심정, 난, 하늘, 날벼락, 호텔, 누군, 추모], [한국...\n",
       "Okt            Okt  [[현재, 호텔, 주인, 심정, 난, 마른하늘, 날벼락, 호텔, 누, 계속, 추모]..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(results, columns=['형태소 분석기','내용'])\n",
    "result_df.set_index(result_df['형태소 분석기'], drop=True,inplace=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:49:19.766429Z",
     "start_time": "2021-03-25T15:49:19.756424Z"
    }
   },
   "outputs": [],
   "source": [
    "def name_num(name,num):\n",
    "    print('\\n\\'',name, '분석 결과입니다.')\n",
    "    print(result_df.loc[name,'내용'][num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:52:28.609813Z",
     "start_time": "2021-03-25T15:52:28.594853Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "' hannanum 분석 결과입니다.\n",
      "['호텔주인', '심정', '아18', '마른하늘', '날벼락맞고', '호텔망하게생겼는데', '누', '추모']\n",
      "\n",
      "' kkma 분석 결과입니다.\n",
      "['현재', '호텔', '호텔주인', '주인', '심정', '18', '마른', '마른하늘', '하늘', '날벼락', '누', '누군', '군', '계속', '추모']\n",
      "\n",
      "' Komoran 분석 결과입니다.\n",
      "['호텔', '주인', '심정', '아', '18', '하늘', '날벼락', '호텔', '망', '추모']\n",
      "\n",
      "' mecab 분석 결과입니다.\n",
      "['현재', '호텔', '주인', '심정', '난', '하늘', '날벼락', '호텔', '누군', '추모']\n",
      "\n",
      "' Okt 분석 결과입니다.\n",
      "['현재', '호텔', '주인', '심정', '난', '마른하늘', '날벼락', '호텔', '누', '계속', '추모']\n"
     ]
    }
   ],
   "source": [
    "name_num('hannanum',0)\n",
    "name_num('kkma',0)\n",
    "name_num('Komoran',0)\n",
    "name_num('mecab',0)\n",
    "name_num('Okt',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hannanum : 명사 추출 시, 동사 및 띄어쓰기 구분 못함\n",
    "- kkma : 명사 추출 시, 합성어의 경우 반복적으로 명사 도출 => 빈도수 분석 불가\n",
    "- **Komaran : 명사 추출 시, 18의 숫자도 들어옴**\n",
    "- **Mecab : 명사 추출 시 가장 깔끔하게 명사를 뽑음**\n",
    "- Okt : 명사 추출 시, 나름 괜찮은 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:52:28.941956Z",
     "start_time": "2021-03-25T15:52:28.925968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "' hannanum 분석 결과입니다.\n",
      "['한국적', '미인', '대표적', '분', '곱고아름다운모습', '그모습뒤', '알지못했네요ㅠ']\n",
      "\n",
      "' kkma 분석 결과입니다.\n",
      "['한국적', '미인', '대표적', '분', '모습', '모습뒤', '뒤', '슬픔', 'ㅠ']\n",
      "\n",
      "' Komoran 분석 결과입니다.\n",
      "['한국', '미인', '대표', '분', '모습', '모습', '뒤', '슬픔']\n",
      "\n",
      "' mecab 분석 결과입니다.\n",
      "['한국', '미인', '대표', '분', '모습', '모습', '뒤', '슬픔']\n",
      "\n",
      "' Okt 분석 결과입니다.\n",
      "['한국', '미인', '대표', '분', '곱', '모습', '모습', '뒤', '슬픔', '미처']\n"
     ]
    }
   ],
   "source": [
    "name_num('hannanum',1)\n",
    "name_num('kkma',1)\n",
    "name_num('Komoran',1)\n",
    "name_num('mecab',1)\n",
    "name_num('Okt',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:52:29.076565Z",
     "start_time": "2021-03-25T15:52:29.068586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "' hannanum 분석 결과입니다.\n",
      "['1.', '사람', '얼굴', '손톱', '것', '인격살해이고2', '동영상', '몰카냐', '메걸리안들', '생각', '없노']\n",
      "\n",
      "' kkma 분석 결과입니다.\n",
      "['1', '사람', '얼굴', '손톱', '인격', '인격살해', '살해', '2', '동영상', '카', '메', '메걸리안', '걸리', '안', '생각', '없노']\n",
      "\n",
      "' Komoran 분석 결과입니다.\n",
      "['사람', '얼굴', '손톱', '것', '인격', '살해', '이고', '동영상', '몰', '메', '걸리', '안', '생각']\n",
      "\n",
      "' mecab 분석 결과입니다.\n",
      "['사람', '얼굴', '손톱', '것', '인격', '살해', '동영상', '몰카', '걸리', '안들', '생각']\n",
      "\n",
      "' Okt 분석 결과입니다.\n",
      "['사람', '얼굴', '손톱', '인격', '살해', '동영상', '몰카', '메걸', '리안', '생각']\n"
     ]
    }
   ],
   "source": [
    "name_num('hannanum',2)\n",
    "name_num('kkma',2)\n",
    "name_num('Komoran',2)\n",
    "name_num('mecab',2)\n",
    "name_num('Okt',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:52:29.226166Z",
     "start_time": "2021-03-25T15:52:29.218190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "' hannanum 분석 결과입니다.\n",
      "['10+8', '진짜', '이승기', '비교']\n",
      "\n",
      "' kkma 분석 결과입니다.\n",
      "['10', '8', '승기', '비교']\n",
      "\n",
      "' Komoran 분석 결과입니다.\n",
      "['이승기', '비교']\n",
      "\n",
      "' mecab 분석 결과입니다.\n",
      "['이승기', '비교']\n",
      "\n",
      "' Okt 분석 결과입니다.\n",
      "['진짜', '이승기', '비교']\n"
     ]
    }
   ],
   "source": [
    "name_num('hannanum',3)\n",
    "name_num('kkma',3)\n",
    "name_num('Komoran',3)\n",
    "name_num('mecab',3)\n",
    "name_num('Okt',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T15:52:29.375767Z",
     "start_time": "2021-03-25T15:52:29.367787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "' hannanum 분석 결과입니다.\n",
      "['180', '호빗', '한남들', '결혼', '하지마셈', 'ㅋ', '돈', '연애', '하지마셈', 'ㅋ', '니들', '호빗', '유전자', '네', '후손', '뭔죄']\n",
      "\n",
      "' kkma 분석 결과입니다.\n",
      "['180', '180이하', '이하', '호', '호빗', '빗', '한남', '결혼', '마', '마셈', '셈', 'ㅋ', '돈', '연애', '니들', '유전자', '고통', '후손', '죄', '죄임', '임']\n",
      "\n",
      "' Komoran 분석 결과입니다.\n",
      "['이하', '호빗', '남', '결혼', '셈', '돈', '연애', '셈', '니들', '호빗', '유전자', '고통', '후손', '죄']\n",
      "\n",
      "' mecab 분석 결과입니다.\n",
      "['이하', '남', '결혼', '셈', '돈', '연애', '셈', '니', '유전자', '고통', '후손', '죄']\n",
      "\n",
      "' Okt 분석 결과입니다.\n",
      "['이하', '호빗', '한남', '결혼', '셈', '돈', '연애', '셈', '니', '호빗', '유전자', '고통', '네', '후손', '죄임']\n"
     ]
    }
   ],
   "source": [
    "name_num('hannanum',4)\n",
    "name_num('kkma',4)\n",
    "name_num('Komoran',4)\n",
    "name_num('mecab',4)\n",
    "name_num('Okt',4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분석 결과\n",
    "\n",
    "- Komoran 과 mecab 형태소 분석기가 가장 적합하다고 판단\n",
    "1. Komoran\n",
    "    - 많은 단어 명사를 포함시킴 (ex. 호빗까지도) \n",
    "    - 간혹 noise 존재 가능( ex. '몰', '메', '걸리', '안', )\n",
    "2. Mecab \n",
    "    - 가장 정제되고 안정된 단어 print\n",
    "    - 그러나 호빗 같은 단어는 없음 ( 단어장에 없는 단어는 추출 안되는 현상 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
