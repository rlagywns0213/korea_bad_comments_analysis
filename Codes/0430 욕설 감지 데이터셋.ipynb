{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\t욕설 감지 데이터셋\n",
    "\n",
    "데이터\n",
    "- 일간베스트(일베), 오늘의 유머와 같은 각종 커뮤니티 사이트의 댓글에 대해 총 5,825문장을 분류했습니다.\n",
    "- 수직선 기호( | )를 기준으로 좌측에는 댓글 내용, 우측에는 욕설 여부(0,1)가 기록되어 있습니다.\n",
    "- '존맛', '개이득' 등의 말은 비속어를 포함하고 있으므로 욕설이라 볼 수 있으나 최근에는 강조의 의미로 흔히 쓰이고 있으므로 악의가 없는 단순 강조의 의미로 쓰였다고 판단될 경우 욕설로 분류하지 않았습니다.\n",
    "- 상황에 따라 욕일 수도 있고, 아닐 수도 있는 댓글은 최대한 비욕설로 구분했습니다.\n",
    "- 출처 : https://github.com/2runo/Curse-detection-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Curse-detection-data'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/2runo/Curse-detection-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:06.431740Z",
     "start_time": "2021-04-30T00:27:05.382547Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "contents = []\n",
    "\n",
    "with open('Curse-detection-data/dataset.txt', 'r',encoding='UTF8') as f:\n",
    "    reader = csv.reader(f, delimiter = '\\t')\n",
    "    for row in f:\n",
    "        contents.append(row.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:06.461660Z",
     "start_time": "2021-04-30T00:27:06.433735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>좌배 까는건 ㅇㅂ</td>\n",
       "      <td>1\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>집에 롱 패딩만 세 개다. 10년 더 입어야지 ㅋㅋ</td>\n",
       "      <td>0\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>개소리야 니가 빨갱이를 옹호하고 드루킹을 ㅇㅇ짓이라고 말못해서 삐진거야 빨갱아</td>\n",
       "      <td>1\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>세탁이라고 봐도 된다</td>\n",
       "      <td>0\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>애새끼가 초딩도 아니고 ㅋㅋㅋㅋ</td>\n",
       "      <td>1\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5820</th>\n",
       "      <td>좌우 헬파이어 3개씩 6개 장착에 아파치보다 약하지만 20mm 기관포 장착임</td>\n",
       "      <td>0\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5821</th>\n",
       "      <td>세금 내놓으라고 데모질 중 ㅋㅋ간첩, 도둑놈 새끼들이 대통령 해처먹으니까  나도 같...</td>\n",
       "      <td>1\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>너가 한 말 중에</td>\n",
       "      <td>0\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823</th>\n",
       "      <td>제갈대중 ㅇㅂ</td>\n",
       "      <td>0\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5824</th>\n",
       "      <td>우리나라교회는 악마들이모여 주뎅이 처벌리고</td>\n",
       "      <td>1\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5825 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               document label  none\n",
       "0                                             좌배 까는건 ㅇㅂ   1\\n  None\n",
       "1                          집에 롱 패딩만 세 개다. 10년 더 입어야지 ㅋㅋ   0\\n  None\n",
       "2           개소리야 니가 빨갱이를 옹호하고 드루킹을 ㅇㅇ짓이라고 말못해서 삐진거야 빨갱아   1\\n  None\n",
       "3                                           세탁이라고 봐도 된다   0\\n  None\n",
       "4                                    애새끼가 초딩도 아니고 ㅋㅋㅋㅋ    1\\n  None\n",
       "...                                                 ...   ...   ...\n",
       "5820         좌우 헬파이어 3개씩 6개 장착에 아파치보다 약하지만 20mm 기관포 장착임   0\\n  None\n",
       "5821  세금 내놓으라고 데모질 중 ㅋㅋ간첩, 도둑놈 새끼들이 대통령 해처먹으니까  나도 같...   1\\n  None\n",
       "5822                                          너가 한 말 중에   0\\n  None\n",
       "5823                                            제갈대중 ㅇㅂ   0\\n  None\n",
       "5824                           우리나라교회는 악마들이모여 주뎅이 처벌리고    1\\n  None\n",
       "\n",
       "[5825 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(contents, columns=['document','label','none'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label 값 \\n 처리 , none 열 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:06.491581Z",
     "start_time": "2021-04-30T00:27:06.464654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>여자 형제 둔 남성 보수 성향 강해… 美조사</td>\n",
       "      <td>한경닷컴 - 한국경제여자 형제가 인생을 더 행복하게 만든다 - 동아일보여자 형제 ...</td>\n",
       "      <td>0\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      document  \\\n",
       "455  여자 형제 둔 남성 보수 성향 강해… 美조사    \n",
       "\n",
       "                                                 label none  \n",
       "455   한경닷컴 - 한국경제여자 형제가 인생을 더 행복하게 만든다 - 동아일보여자 형제 ...  0\\n  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label'].apply(lambda x: '한경' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:06.506540Z",
     "start_time": "2021-04-30T00:27:06.493575Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(455, axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:06.536461Z",
     "start_time": "2021-04-30T00:27:06.508535Z"
    }
   },
   "outputs": [],
   "source": [
    "df['label'] = df['label'].apply(lambda x : int(x.replace(\"\\n\",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:06.566380Z",
     "start_time": "2021-04-30T00:27:06.538455Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop('none',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:06.641156Z",
     "start_time": "2021-04-30T00:27:06.629189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3780\n",
       "1    2044\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고 : https://wikidocs.net/94748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:14.859361Z",
     "start_time": "2021-04-30T00:27:07.020144Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:14.890275Z",
     "start_time": "2021-04-30T00:27:14.862349Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5824, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['document'].nunique(),df['label'].nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:14.905239Z",
     "start_time": "2021-04-30T00:27:14.894265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:14.952113Z",
     "start_time": "2021-04-30T00:27:14.909224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 리뷰의 개수 : 4368\n",
      "테스트용 리뷰의 개수 : 1456\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size = 0.25, random_state = 42)\n",
    "print('훈련용 리뷰의 개수 :', len(train_data))\n",
    "print('테스트용 리뷰의 개수 :', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:15.163543Z",
     "start_time": "2021-04-30T00:27:14.954104Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29893cf8a00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['label'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화\n",
    "\n",
    "1. Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불용어 제거 \n",
    "- stopwords 생성\n",
    "- 갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업 필요\n",
    "- 큰 의미가 없다라는 것은 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:15.178505Z",
     "start_time": "2021-04-30T00:27:15.165540Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '만', '게임', '겜', '되', '음', '면']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:16.091348Z",
     "start_time": "2021-04-30T00:27:15.180498Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-ff1de90eb13d>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['tokenized'] = train_data['document'].apply(mecab.morphs)\n",
      "<ipython-input-14-ff1de90eb13d>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
      "<ipython-input-14-ff1de90eb13d>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['tokenized'] = test_data['document'].apply(mecab.morphs)\n",
      "<ipython-input-14-ff1de90eb13d>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n"
     ]
    }
   ],
   "source": [
    "mecab = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "\n",
    "\n",
    "train_data['tokenized'] = train_data['document'].apply(mecab.morphs)\n",
    "train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "test_data['tokenized'] = test_data['document'].apply(mecab.morphs)\n",
    "test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어와 길이 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:16.121320Z",
     "start_time": "2021-04-30T00:27:16.094309Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\n",
    "negative_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:16.151158Z",
     "start_time": "2021-04-30T00:27:16.124262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ㅋㅋㅋ', 818), ('.', 660), ('?', 354), ('거', 342), ('ㅋㅋ', 316), ('새끼', 274), ('있', 236), ('아', 188), ('안', 180), ('냐', 175), ('나', 170), ('보', 165), ('니', 147), ('어', 146), ('존나', 142), ('없', 142), ('같', 140), ('는데', 139), ('말', 136), ('일', 135)]\n"
     ]
    }
   ],
   "source": [
    "negative_word_count = Counter(negative_words)\n",
    "print(negative_word_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:16.181078Z",
     "start_time": "2021-04-30T00:27:16.153152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 1052), ('?', 576), ('ㅋㅋㅋ', 364), ('있', 343), ('거', 333), ('ㅋㅋ', 294), ('냐', 242), ('안', 223), ('없', 218), ('나', 216), ('는데', 199), (',', 172), ('에서', 164), ('말', 160), ('으로', 158), ('아', 155), ('아니', 153), ('로', 152), ('보', 148), ('..', 145)]\n"
     ]
    }
   ],
   "source": [
    "positive_word_count = Counter(positive_words)\n",
    "print(positive_word_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:16.481535Z",
     "start_time": "2021-04-30T00:27:16.182075Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정 리뷰의 평균 길이 : 18.443129520052597\n",
      "긍정 리뷰의 평균 길이 : 10.814892869687391\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
    "text_len = train_data[train_data['label']==1]['tokenized'].map(lambda x: len(x))\n",
    "ax1.hist(text_len, color='red')\n",
    "ax1.set_title('negative Reviews')\n",
    "ax1.set_xlabel('length of samples')\n",
    "ax1.set_ylabel('number of samples')\n",
    "print('부정 리뷰의 평균 길이 :', np.mean(text_len))\n",
    "\n",
    "text_len = train_data[train_data['label']==0]['tokenized'].map(lambda x: len(x))\n",
    "ax2.hist(text_len, color='blue')\n",
    "ax2.set_title('positive Reviews')\n",
    "fig.suptitle('Words in texts')\n",
    "ax2.set_xlabel('length of samples')\n",
    "ax2.set_ylabel('number of samples')\n",
    "print('긍정 리뷰의 평균 길이 :', np.mean(text_len))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:16.496464Z",
     "start_time": "2021-04-30T00:27:16.482503Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_data['tokenized'].values\n",
    "y_train = train_data['label'].values\n",
    "X_test= test_data['tokenized'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:16.571263Z",
     "start_time": "2021-04-30T00:27:16.498462Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:16.631104Z",
     "start_time": "2021-04-30T00:27:16.573259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('안', 403),\n",
       "             ('그래도', 28),\n",
       "             ('요즘', 35),\n",
       "             ('숙박업', 1),\n",
       "             ('울상', 1),\n",
       "             ('일텐데', 1),\n",
       "             ('.', 1712),\n",
       "             ('.....', 4),\n",
       "             ('33', 1),\n",
       "             ('오', 69),\n",
       "             ('모이', 3),\n",
       "             ('말', 296),\n",
       "             ('시', 77),\n",
       "             ('키', 20),\n",
       "             ('아', 343),\n",
       "             ('그냥', 131),\n",
       "             ('홀딩', 1),\n",
       "             ('세요', 24),\n",
       "             ('빙신', 4),\n",
       "             ('!', 146),\n",
       "             ('!!!!!', 4),\n",
       "             ('저', 138),\n",
       "             ('꾸', 5),\n",
       "             ('피지컬', 1),\n",
       "             ('한국어', 1),\n",
       "             ('좀', 107),\n",
       "             ('배워서', 6),\n",
       "             ('김치', 19),\n",
       "             ('처먹', 17),\n",
       "             ('어도', 36),\n",
       "             ('월', 24),\n",
       "             ('10', 36),\n",
       "             ('억', 37),\n",
       "             ('간단', 2),\n",
       "             ('땡기', 2),\n",
       "             ('는데', 338),\n",
       "             ('왜', 122),\n",
       "             ('힘들', 20),\n",
       "             ('몸', 18),\n",
       "             ('파', 9),\n",
       "             ('냐', 417),\n",
       "             ('이기', 43),\n",
       "             ('야', 212),\n",
       "             ('정신', 31),\n",
       "             ('피폐', 2),\n",
       "             ('해', 213),\n",
       "             (',', 305),\n",
       "             ('돈', 157),\n",
       "             ('나눠', 3),\n",
       "             ('줘야', 8),\n",
       "             ('애', 113),\n",
       "             ('까지', 92),\n",
       "             ('낳', 8),\n",
       "             ('으면', 122),\n",
       "             ('어쩔', 8),\n",
       "             ('예', 20),\n",
       "             ('조선', 34),\n",
       "             ('-', 50),\n",
       "             ('100', 13),\n",
       "             ('->', 2),\n",
       "             ('일제', 4),\n",
       "             ('50', 20),\n",
       "             ('라는', 78),\n",
       "             ('거', 675),\n",
       "             ('둘', 25),\n",
       "             ('지금', 90),\n",
       "             ('처럼', 37),\n",
       "             ('완전히', 3),\n",
       "             ('자유', 10),\n",
       "             ('롭', 5),\n",
       "             ('못', 177),\n",
       "             ('해서', 109),\n",
       "             ('마이너스', 3),\n",
       "             ('지만', 81),\n",
       "             ('어찌', 9),\n",
       "             ('됐', 15),\n",
       "             ('건', 122),\n",
       "             ('보단', 8),\n",
       "             ('나았', 2),\n",
       "             ('다는', 67),\n",
       "             ('것', 184),\n",
       "             ('시발', 59),\n",
       "             ('존나', 151),\n",
       "             ('잘', 169),\n",
       "             ('함', 155),\n",
       "             ('많이', 60),\n",
       "             ('들어가', 21),\n",
       "             ('병', 23),\n",
       "             ('나', 386),\n",
       "             ('큰', 21),\n",
       "             ('의료비', 1),\n",
       "             ('나가', 28),\n",
       "             ('닉', 6),\n",
       "             ('값', 9),\n",
       "             ('ㅍㅌㅊ', 1),\n",
       "             ('좋', 156),\n",
       "             ('겠', 199),\n",
       "             ('저런', 58),\n",
       "             ('시장', 20),\n",
       "             ('가진', 8),\n",
       "             ('대구', 23),\n",
       "             ('~', 103),\n",
       "             ('선거', 12),\n",
       "             ('밥', 19),\n",
       "             ('사', 119),\n",
       "             ('먹', 131),\n",
       "             ('으라고', 6),\n",
       "             ('준다', 5),\n",
       "             ('잔', 4),\n",
       "             ('전라도', 22),\n",
       "             ('홍어', 47),\n",
       "             ('년놈', 5),\n",
       "             ('주', 173),\n",
       "             ('사양', 1),\n",
       "             ('가장', 10),\n",
       "             ('전하', 3),\n",
       "             ('공익', 2),\n",
       "             ('신고', 11),\n",
       "             ('제출', 6),\n",
       "             ('사항', 3),\n",
       "             ('으로', 292),\n",
       "             ('1', 94),\n",
       "             ('신고자', 1),\n",
       "             ('성명', 1),\n",
       "             ('연락처', 1),\n",
       "             ('2', 80),\n",
       "             ('위반', 5),\n",
       "             ('일시', 2),\n",
       "             ('장소', 2),\n",
       "             ('차량', 4),\n",
       "             ('번호', 5),\n",
       "             ('3', 54),\n",
       "             ('교통위반', 1),\n",
       "             ('증거', 6),\n",
       "             ('영상', 13),\n",
       "             ('또는', 2),\n",
       "             ('사진', 29),\n",
       "             ('이게', 35),\n",
       "             ('나라', 97),\n",
       "             ('?', 930),\n",
       "             ('공', 14),\n",
       "             ('수처', 3),\n",
       "             ('통과', 4),\n",
       "             ('될', 39),\n",
       "             ('알', 124),\n",
       "             ('있', 579),\n",
       "             ('었', 151),\n",
       "             ('마당', 2),\n",
       "             ('대체', 11),\n",
       "             ('어딜', 4),\n",
       "             ('봐서', 9),\n",
       "             ('파멸', 1),\n",
       "             ('보이', 31),\n",
       "             ('노', 231),\n",
       "             ('미국', 48),\n",
       "             ('년', 193),\n",
       "             ('인지', 33),\n",
       "             ('일본', 106),\n",
       "             ('수', 158),\n",
       "             ('없', 360),\n",
       "             ('인민', 2),\n",
       "             ('감시', 2),\n",
       "             ('시스템', 6),\n",
       "             ('구축', 3),\n",
       "             ('위해서', 7),\n",
       "             ('안면', 1),\n",
       "             ('인식', 14),\n",
       "             ('기술', 29),\n",
       "             ('데이터', 3),\n",
       "             ('로', 285),\n",
       "             ('쓰', 64),\n",
       "             ('려고', 24),\n",
       "             ('중국', 52),\n",
       "             ('공산당', 2),\n",
       "             ('그림', 1),\n",
       "             ('그린', 1),\n",
       "             ('영화', 17),\n",
       "             ('비', 15),\n",
       "             ('가족', 14),\n",
       "             ('기생충', 5),\n",
       "             ('조커', 3),\n",
       "             ('화두', 1),\n",
       "             ('빈부', 2),\n",
       "             ('격차', 2),\n",
       "             ('이유', 26),\n",
       "             ('세상', 30),\n",
       "             ('젊은이', 5),\n",
       "             ('시달리', 1),\n",
       "             ('기', 179),\n",
       "             ('때문', 45),\n",
       "             ('글', 116),\n",
       "             ('팩', 28),\n",
       "             ('트', 28),\n",
       "             ('조지', 3),\n",
       "             ('경기', 11),\n",
       "             ('아서', 57),\n",
       "             ('취직', 7),\n",
       "             ('된다고', 8),\n",
       "             ('아니', 271),\n",
       "             ('일', 252),\n",
       "             ('할', 183),\n",
       "             ('(', 35),\n",
       "             ('해도', 49),\n",
       "             ('일자리', 5),\n",
       "             ('질', 46),\n",
       "             ('적', 163),\n",
       "             ('부분', 14),\n",
       "             ('떨어짐', 2),\n",
       "             (')', 41),\n",
       "             ('그만큼', 3),\n",
       "             ('노인', 8),\n",
       "             ('인구', 12),\n",
       "             ('상당', 4),\n",
       "             ('골치', 1),\n",
       "             ('덩', 1),\n",
       "             ('명', 41),\n",
       "             ('당', 30),\n",
       "             ('근데', 92),\n",
       "             ('무슨', 37),\n",
       "             ('사토리', 1),\n",
       "             ('세대', 2),\n",
       "             ('행복', 8),\n",
       "             ('이건', 20),\n",
       "             ('반', 17),\n",
       "             ('대로', 26),\n",
       "             ('우리', 58),\n",
       "             ('한테', 94),\n",
       "             ('반면', 1),\n",
       "             ('교사', 5),\n",
       "             ('앞', 26),\n",
       "             ('2030', 1),\n",
       "             ('2040', 1),\n",
       "             ('보다', 80),\n",
       "             ('할테', 1),\n",
       "             ('거기', 30),\n",
       "             ('대한', 13),\n",
       "             ('복지', 9),\n",
       "             ('비용', 4),\n",
       "             ('생각', 136),\n",
       "             ('앞날', 1),\n",
       "             ('깜깜', 1),\n",
       "             ('그때', 12),\n",
       "             ('줄어들', 1),\n",
       "             ('테', 11),\n",
       "             ('현', 6),\n",
       "             ('초딩', 4),\n",
       "             ('20', 34),\n",
       "             ('대', 111),\n",
       "             ('30', 22),\n",
       "             ('별', 12),\n",
       "             ('하자', 1),\n",
       "             ('바로', 31),\n",
       "             ('다만', 7),\n",
       "             ('참고', 9),\n",
       "             ('해야', 63),\n",
       "             ('한다', 56),\n",
       "             ('일본인', 3),\n",
       "             ('과연', 3),\n",
       "             ('어떤', 19),\n",
       "             ('..', 219),\n",
       "             ('디플레', 1),\n",
       "             ('고통', 5),\n",
       "             ('받', 106),\n",
       "             ('아베', 5),\n",
       "             ('엔', 26),\n",
       "             ('정책', 3),\n",
       "             ('마치', 5),\n",
       "             ('실상', 4),\n",
       "             ('부채', 2),\n",
       "             ('내', 181),\n",
       "             ('불만', 1),\n",
       "             ('여론', 10),\n",
       "             ('잠식', 2),\n",
       "             ('시키', 21),\n",
       "             ('한국', 79),\n",
       "             ('이나', 92),\n",
       "             ('특히', 9),\n",
       "             ('뉴스', 11),\n",
       "             ('매일', 11),\n",
       "             ('시간', 31),\n",
       "             ('때우', 2),\n",
       "             ('예전', 15),\n",
       "             ('에서', 290),\n",
       "             ('보', 313),\n",
       "             ('던', 70),\n",
       "             ('모습', 7),\n",
       "             ('일까', 6),\n",
       "             (';;;', 5),\n",
       "             ('현실', 23),\n",
       "             ('직시', 1),\n",
       "             ('했', 182),\n",
       "             ('맞', 108),\n",
       "             ('인데', 111),\n",
       "             ('백수', 3),\n",
       "             ('천지', 3),\n",
       "             ('구나', 19),\n",
       "             ('제주도', 1),\n",
       "             ('까스', 2),\n",
       "             ('으러', 5),\n",
       "             ('간다고', 2),\n",
       "             ('부모', 16),\n",
       "             ('한숨', 1),\n",
       "             ('나오', 50),\n",
       "             ('아직', 30),\n",
       "             ('방독면', 1),\n",
       "             ('앉', 19),\n",
       "             ('쏴', 2),\n",
       "             ('옆', 17),\n",
       "             ('후임', 2),\n",
       "             ('표적', 2),\n",
       "             ('맞춰', 5),\n",
       "             ('넘겼', 2),\n",
       "             ('때', 147),\n",
       "             ('중대', 6),\n",
       "             ('간부', 5),\n",
       "             ('님', 62),\n",
       "             ('잊', 4),\n",
       "             ('못하', 27),\n",
       "             ('그', 155),\n",
       "             ('짜릿', 1),\n",
       "             ('정자', 2),\n",
       "             ('제공', 4),\n",
       "             ('자', 111),\n",
       "             ('누군데', 4),\n",
       "             ('그리고', 45),\n",
       "             ('46', 2),\n",
       "             ('세', 15),\n",
       "             ('임신', 3),\n",
       "             ('출산', 4),\n",
       "             ('가능', 61),\n",
       "             (';;', 11),\n",
       "             ('ㄷ', 135),\n",
       "             ('/', 71),\n",
       "             ('쥬얼', 1),\n",
       "             ('정도', 81),\n",
       "             ('데리', 4),\n",
       "             ('살', 125),\n",
       "             (';', 8),\n",
       "             ('김제동', 3),\n",
       "             ('분야', 6),\n",
       "             ('최고', 10),\n",
       "             ('권위자', 1),\n",
       "             ('아닌가', 16),\n",
       "             ('ㅋㅋㅋ', 1182),\n",
       "             ('자기', 41),\n",
       "             ('밥줄', 2),\n",
       "             ('니', 232),\n",
       "             ('공부', 63),\n",
       "             ('모든', 19),\n",
       "             ('기본', 7),\n",
       "             ('다를', 6),\n",
       "             ('이해', 35),\n",
       "             ('사람', 156),\n",
       "             ('누구', 19),\n",
       "             ('전제', 1),\n",
       "             ('얘기', 21),\n",
       "             ('필요', 21),\n",
       "             ('없이', 25),\n",
       "             ('뭐', 139),\n",
       "             ('어쩌', 16),\n",
       "             ('라고', 164),\n",
       "             ('하찮', 4),\n",
       "             ('축생', 2),\n",
       "             ('줄', 67),\n",
       "             ('감성', 3),\n",
       "             ('따위', 5),\n",
       "             ('타', 37),\n",
       "             ('서', 182),\n",
       "             ('훈수', 2),\n",
       "             ('이거', 41),\n",
       "             ('잖', 9),\n",
       "             ('이따위', 1),\n",
       "             ('유튜브', 11),\n",
       "             ('홍보', 5),\n",
       "             ('싶', 67),\n",
       "             ('놈', 90),\n",
       "             ('댓글', 32),\n",
       "             ('하나', 54),\n",
       "             ('이렇게', 27),\n",
       "             ('일일이', 2),\n",
       "             ('캡쳐', 7),\n",
       "             ('올리', 23),\n",
       "             ('광고', 8),\n",
       "             ('려는', 13),\n",
       "             ('아닌', 24),\n",
       "             ('이상', 54),\n",
       "             ('리뷰', 4),\n",
       "             ('관리', 8),\n",
       "             ('꼼꼼', 1),\n",
       "             ('곳', 42),\n",
       "             ('더라', 85),\n",
       "             ('좆', 93),\n",
       "             ('꼴리', 9),\n",
       "             ('무법천지', 1),\n",
       "             ('씨', 45),\n",
       "             ('성씨', 2),\n",
       "             ('유부', 1),\n",
       "             ('우동', 2),\n",
       "             ('달', 38),\n",
       "             ('창악', 1),\n",
       "             ('플', 9),\n",
       "             ('러', 31),\n",
       "             ('연령', 2),\n",
       "             ('딱', 58),\n",
       "             ('40', 13),\n",
       "             ('그래서', 36),\n",
       "             ('그걸', 23),\n",
       "             ('다고', 112),\n",
       "             ('법', 20),\n",
       "             ('처벌', 6),\n",
       "             ('않', 105),\n",
       "             ('지요', 8),\n",
       "             ('남', 79),\n",
       "             ('신경', 9),\n",
       "             ('쓸', 16),\n",
       "             ('4', 33),\n",
       "             ('5', 45),\n",
       "             ('원', 61),\n",
       "             ('내고', 1),\n",
       "             ('좌석', 2),\n",
       "             ('두', 41),\n",
       "             ('개', 150),\n",
       "             ('전부', 14),\n",
       "             ('버려라', 2),\n",
       "             ('이런', 78),\n",
       "             ('환경', 5),\n",
       "             ('랑', 80),\n",
       "             ('비슷', 21),\n",
       "             ('위치', 6),\n",
       "             ('특징', 10),\n",
       "             ('동쪽', 1),\n",
       "             ('부터', 70),\n",
       "             ('개발', 8),\n",
       "             ('편서풍', 2),\n",
       "             ('부니', 1),\n",
       "             ('까', 35),\n",
       "             ('...', 53),\n",
       "             ('란', 29),\n",
       "             ('결국', 16),\n",
       "             ('공장', 14),\n",
       "             ('짓', 39),\n",
       "             ('건데', 28),\n",
       "             ('지으면', 3),\n",
       "             ('전기', 6),\n",
       "             ('대량', 2),\n",
       "             ('발전소', 1),\n",
       "             ('어야제', 1),\n",
       "             ('원자력', 2),\n",
       "             ('현재', 6),\n",
       "             ('문', 28),\n",
       "             ('씹', 71),\n",
       "             ('쌔', 6),\n",
       "             ('같', 254),\n",
       "             ('머저리', 1),\n",
       "             ('새끼', 279),\n",
       "             ('대통령', 14),\n",
       "             ('과거', 12),\n",
       "             ('그런', 66),\n",
       "             ('어딨', 6),\n",
       "             ('무적', 5),\n",
       "             ('권', 38),\n",
       "             ('화력', 2),\n",
       "             ('매연', 1),\n",
       "             ('오지', 12),\n",
       "             ('미세먼지', 1),\n",
       "             ('부', 23),\n",
       "             ('서쪽', 1),\n",
       "             ('??', 24),\n",
       "             ('자살', 20),\n",
       "             ('단', 18),\n",
       "             ('소리', 45),\n",
       "             ('소문', 3),\n",
       "             ('으니', 25),\n",
       "             ('뿐', 21),\n",
       "             ('이하', 6),\n",
       "             ('습니다', 28),\n",
       "             ('베', 87),\n",
       "             ('댓', 6),\n",
       "             ('장문', 1),\n",
       "             ('ㅋㅋ', 610),\n",
       "             ('동물', 6),\n",
       "             ('직감', 2),\n",
       "             ('용접', 67),\n",
       "             ('잖아', 32),\n",
       "             ('꿀', 10),\n",
       "             ('직장', 9),\n",
       "             ('해로운', 1),\n",
       "             ('노동', 2),\n",
       "             ('직', 7),\n",
       "             ('굳이', 10),\n",
       "             ('한자', 2),\n",
       "             ('섞', 1),\n",
       "             ('어서', 88),\n",
       "             ('머', 23),\n",
       "             ('조선족', 12),\n",
       "             ('칼', 18),\n",
       "             ('찌르', 4),\n",
       "             ('시켜', 19),\n",
       "             ('집', 74),\n",
       "             ('팔', 32),\n",
       "             ('그걸로', 4),\n",
       "             ('놀', 10),\n",
       "             ('다가', 58),\n",
       "             ('일손', 1),\n",
       "             ('부족', 5),\n",
       "             ('부서', 3),\n",
       "             ('공무원', 19),\n",
       "             ('채용', 1),\n",
       "             ('문제', 38),\n",
       "             ('천', 31),\n",
       "             ('한텐', 1),\n",
       "             ('무지', 2),\n",
       "             ('인갑', 2),\n",
       "             ('구글', 6),\n",
       "             ('죽여야', 2),\n",
       "             ('살인', 2),\n",
       "             ('뎅이', 1),\n",
       "             ('병균', 1),\n",
       "             ('쳐', 42),\n",
       "             ('넣', 23),\n",
       "             ('돌아다니', 4),\n",
       "             ('세균전', 1),\n",
       "             ('라', 190),\n",
       "             ('든다', 4),\n",
       "             ('자정', 2),\n",
       "             ('작용', 2),\n",
       "             ('보람', 2),\n",
       "             ('튜브', 2),\n",
       "             ('봤', 48),\n",
       "             ('만족', 4),\n",
       "             ('퀄리티', 2),\n",
       "             ('더', 116),\n",
       "             ('줘서', 5),\n",
       "             ('뽑', 14),\n",
       "             ('걱정하지마', 2),\n",
       "             ('급', 23),\n",
       "             ('편집자', 2),\n",
       "             ('써도', 3),\n",
       "             ('난이도', 2),\n",
       "             ('컨텐츠', 5),\n",
       "             ('거나', 32),\n",
       "             ('사업', 8),\n",
       "             ('충분히', 5),\n",
       "             ('가용', 1),\n",
       "             ('범위', 1),\n",
       "             ('니까', 79),\n",
       "             ('운영', 9),\n",
       "             ('미쳤', 4),\n",
       "             ('진짜', 138),\n",
       "             ('ㅁ', 94),\n",
       "             ('ㅈ', 162),\n",
       "             ('ㅎ', 114),\n",
       "             ('방생', 3),\n",
       "             ('마', 46),\n",
       "             ('링크', 54),\n",
       "             ('번', 74),\n",
       "             ('은혜', 6),\n",
       "             ('복음', 4),\n",
       "             ('기록', 7),\n",
       "             ('성경', 2),\n",
       "             ('로마서', 2),\n",
       "             ('빌레몬서', 1),\n",
       "             ('이외', 1),\n",
       "             ('쓰여진', 1),\n",
       "             ('명령', 4),\n",
       "             ('교회', 11),\n",
       "             ('시대', 23),\n",
       "             ('지켜야', 1),\n",
       "             ('구절', 3),\n",
       "             ('암튼', 8),\n",
       "             ('미련', 3),\n",
       "             ('다면', 16),\n",
       "             ('너', 80),\n",
       "             ('와이프', 6),\n",
       "             ('중', 70),\n",
       "             ('어학연수', 1),\n",
       "             ('라도', 48),\n",
       "             ('면서', 106),\n",
       "             ('직접', 7),\n",
       "             ('경험', 5),\n",
       "             ('길', 50),\n",
       "             ('추천', 14),\n",
       "             ('중고', 7),\n",
       "             ('신입', 1),\n",
       "             ('경우', 23),\n",
       "             ('34', 1),\n",
       "             ('35', 3),\n",
       "             ('겐', 2),\n",
       "             ('뻬이', 1),\n",
       "             ('만세', 7),\n",
       "             ('새벽', 5),\n",
       "             ('고소장', 1),\n",
       "             ('어디', 50),\n",
       "             ('씨발', 75),\n",
       "             ('무섭', 7),\n",
       "             ('유관순', 1),\n",
       "             ('열사', 1),\n",
       "             ('ㅠㅠ', 22),\n",
       "             ('mechanism', 1),\n",
       "             ('스트레스', 8),\n",
       "             ('취하', 2),\n",
       "             ('마시', 7),\n",
       "             ('술', 16),\n",
       "             ('빡', 18),\n",
       "             ('대가', 2),\n",
       "             ('리야', 1),\n",
       "             ('헌', 2),\n",
       "             ('법조문', 1),\n",
       "             ('다시', 28),\n",
       "             ('상기', 1),\n",
       "             ('파보', 1),\n",
       "             ('막', 20),\n",
       "             ('비례', 6),\n",
       "             ('출신', 20),\n",
       "             ('마스크', 10),\n",
       "             ('장점', 1),\n",
       "             ('빨', 50),\n",
       "             ('땀', 2),\n",
       "             ('훨씬', 15),\n",
       "             ('덜', 7),\n",
       "             ('차', 59),\n",
       "             ('외피', 2),\n",
       "             ('알콜', 1),\n",
       "             ('소독', 3),\n",
       "             ('점', 20),\n",
       "             ('비싸', 13),\n",
       "             ('어', 287),\n",
       "             ('쩌', 2),\n",
       "             ('투덜대', 1),\n",
       "             ('너도나도', 1),\n",
       "             ('재끼', 1),\n",
       "             ('죠', 27),\n",
       "             ('결혼', 37),\n",
       "             ('지원금', 2),\n",
       "             ('짜리', 9),\n",
       "             ('하다못해', 1),\n",
       "             ('저게', 14),\n",
       "             ('식', 14),\n",
       "             ('약', 6),\n",
       "             ('처', 15),\n",
       "             ('공식', 4),\n",
       "             ('마스코트', 1),\n",
       "             ('된', 62),\n",
       "             ('일회용', 1),\n",
       "             ('홍보물', 1),\n",
       "             ('대놓', 2),\n",
       "             ('도라에몽', 3),\n",
       "             ('쓰인', 1),\n",
       "             ('그게', 37),\n",
       "             ('상품화', 1),\n",
       "             ('주장', 7),\n",
       "             ('병신', 121),\n",
       "             ('잼', 13),\n",
       "             ('블', 8),\n",
       "             ('박', 17),\n",
       "             ('=', 37),\n",
       "             ('본인', 20),\n",
       "             ('블루', 1),\n",
       "             ('컬러', 2),\n",
       "             ('직업', 38),\n",
       "             ('이러', 12),\n",
       "             ('ㅉㅉ', 23),\n",
       "             ('프랑스', 4),\n",
       "             ('모두', 11),\n",
       "             ('셧다운', 1),\n",
       "             ('봄', 20),\n",
       "             ('너무', 49),\n",
       "             ('오버', 2),\n",
       "             ('냐고', 15),\n",
       "             (':', 34),\n",
       "             ('깜', 8),\n",
       "             ('방', 6),\n",
       "             ('처음', 14),\n",
       "             ('바', 22),\n",
       "             ('pe', 1),\n",
       "             ('석사', 1),\n",
       "             ('한다고', 29),\n",
       "             ('몇', 54),\n",
       "             ('나중', 16),\n",
       "             ('북한', 17),\n",
       "             ('여자', 80),\n",
       "             ('야겠다', 2),\n",
       "             ('이쁜', 6),\n",
       "             ('골라서', 2),\n",
       "             ('육', 3),\n",
       "             ('변기', 2),\n",
       "             ('풀', 14),\n",
       "             ('갇', 1),\n",
       "             ('데', 87),\n",
       "             ('노무', 8),\n",
       "             ('유명', 7),\n",
       "             ('곱창', 5),\n",
       "             ('손질', 1),\n",
       "             ('인건비', 1),\n",
       "             ('70', 12),\n",
       "             ('%', 45),\n",
       "             ('됨', 55),\n",
       "             ('제대로', 22),\n",
       "             ('손', 21),\n",
       "             ('이랑', 50),\n",
       "             ('숙성', 1),\n",
       "             ('삼', 14),\n",
       "             ('걸린다', 1),\n",
       "             ('ㅇ줄', 1),\n",
       "             ('아이구', 1),\n",
       "             ('얘', 15),\n",
       "             ('며칠', 2),\n",
       "             ('전', 65),\n",
       "             ('히', 3),\n",
       "             ('심', 7),\n",
       "             ('여유', 1),\n",
       "             ('그나마', 8),\n",
       "             ('정확', 3),\n",
       "             ('해석', 9),\n",
       "             ('해줄께', 1),\n",
       "             ('는군요', 4),\n",
       "             ('해요', 2),\n",
       "             ('지원', 7),\n",
       "             ('끼리', 21),\n",
       "             ('나눈', 1),\n",
       "             ('상류층', 1),\n",
       "             ('개보', 6),\n",
       "             ('지랑', 2),\n",
       "             ('하층민', 3),\n",
       "             ('처녀', 3),\n",
       "             ('비견', 2),\n",
       "             ('자체', 33),\n",
       "             ('가치', 13),\n",
       "             ('매우', 1),\n",
       "             ('크', 13),\n",
       "             ('방증', 2),\n",
       "             ('제발', 25),\n",
       "             ('잡', 57),\n",
       "             ('가두', 1),\n",
       "             ('~~', 32),\n",
       "             ('뚜', 3),\n",
       "             ('렛', 4),\n",
       "             ('이후', 15),\n",
       "             ('믿', 29),\n",
       "             ('쯤', 7),\n",
       "             ('남대', 1),\n",
       "             ('전북', 1),\n",
       "             ('상황', 22),\n",
       "             ('궁금', 6),\n",
       "             ('지네', 6),\n",
       "             ('최대', 4),\n",
       "             ('수준', 29),\n",
       "             ('최소', 10),\n",
       "             ('은데', 49),\n",
       "             ('콩밥', 1),\n",
       "             ('자지', 4),\n",
       "             ('썰', 7),\n",
       "             ('어야', 19),\n",
       "             ('횡보', 1),\n",
       "             ('9', 18),\n",
       "             ('후', 19),\n",
       "             ('여파', 1),\n",
       "             ('갈', 22),\n",
       "             ('사실', 35),\n",
       "             ('쓸데없이', 1),\n",
       "             ('켜', 3),\n",
       "             ('강력', 2),\n",
       "             ('유감', 1),\n",
       "             ('뜻', 9),\n",
       "             ('표한다', 1),\n",
       "             ('쪽', 27),\n",
       "             ('바리', 6),\n",
       "             ('만화', 6),\n",
       "             ('느라', 2),\n",
       "             ('애썼', 2),\n",
       "             ('나베', 8),\n",
       "             ('지도자', 4),\n",
       "             ('서울', 19),\n",
       "             ('7', 39),\n",
       "             ('불', 24),\n",
       "             ('체자', 2),\n",
       "             ('불법', 9),\n",
       "             ('입국', 6),\n",
       "             ('등등', 18),\n",
       "             ('많', 103),\n",
       "             ('밖', 18),\n",
       "             ('ㅇ', 44),\n",
       "             ('ㅔ없음', 1),\n",
       "             ('바닷가', 1),\n",
       "             ('가깝', 3),\n",
       "             ('집값', 5),\n",
       "             ('쌈', 3),\n",
       "             ('속해', 1),\n",
       "             ('상', 23),\n",
       "             ('인천', 1),\n",
       "             ('경기도', 2),\n",
       "             ('가까운', 3),\n",
       "             ('동네', 28),\n",
       "             ('그러', 35),\n",
       "             ('불체', 1),\n",
       "             ('외노', 9),\n",
       "             ('짱', 17),\n",
       "             ('온갖', 3),\n",
       "             ('인간', 42),\n",
       "             ('들락날락', 1),\n",
       "             ('첫', 9),\n",
       "             ('스타트', 2),\n",
       "             ('끊어졌', 1),\n",
       "             ('예지', 8),\n",
       "             ('걸레', 9),\n",
       "             ('함부로', 3),\n",
       "             ('아라', 8),\n",
       "             ('앜', 2),\n",
       "             ('비록', 2),\n",
       "             ('을지', 4),\n",
       "             ('모르', 53),\n",
       "             ('목표', 5),\n",
       "             ('잃', 2),\n",
       "             ('는다면', 1),\n",
       "             ('영웅본색', 1),\n",
       "             ('라서', 19),\n",
       "             ('뭘', 26),\n",
       "             ('본데', 1),\n",
       "             ('머리', 32),\n",
       "             ('여군', 2),\n",
       "             ('빡빡이', 1),\n",
       "             ('군인', 4),\n",
       "             ('간', 20),\n",
       "             ('세상만사', 1),\n",
       "             ('복잡', 1),\n",
       "             ('쌩까', 2),\n",
       "             ('절대', 19),\n",
       "             ('연락', 7),\n",
       "             ('먼저', 10),\n",
       "             ('올', 7),\n",
       "             ('욕', 35),\n",
       "             ('꿈틀대', 1),\n",
       "             ('쥬', 6),\n",
       "             ('거지', 11),\n",
       "             ('논하', 1),\n",
       "             ('자칭', 4),\n",
       "             ('미래', 5),\n",
       "             ('꽤', 7),\n",
       "             ('봐', 21),\n",
       "             ('얼마나', 17),\n",
       "             ('는지', 35),\n",
       "             ('인증', 13),\n",
       "             ('ㄱ', 26),\n",
       "             ('지옥', 7),\n",
       "             ('자식', 13),\n",
       "             ('가정', 4),\n",
       "             ('교육', 9),\n",
       "             ('시킨', 9),\n",
       "             ('뒤진', 8),\n",
       "             ('증조할아버지', 1),\n",
       "             ('시작', 17),\n",
       "             ('싸그리', 1),\n",
       "             ('모가지', 1),\n",
       "             ('잘라', 1),\n",
       "             ('버린다', 2),\n",
       "             ('서비스', 8),\n",
       "             ('애미', 20),\n",
       "             ('변기통', 1),\n",
       "             ('쳐넣', 1),\n",
       "             ('밟', 4),\n",
       "             ('목', 4),\n",
       "             ('꺾', 5),\n",
       "             ('죽여', 7),\n",
       "             ('드림', 1),\n",
       "             ('k', 9),\n",
       "             ('흑', 4),\n",
       "             ('열', 14),\n",
       "             ('전차', 2),\n",
       "             ('실제로', 8),\n",
       "             ('위압감', 1),\n",
       "             ('자주포', 1),\n",
       "             ('ㅆㅅㅌㅊ', 8),\n",
       "             ('밀', 8),\n",
       "             ('덕', 10),\n",
       "             ('씩', 18),\n",
       "             ('히로뽕', 2),\n",
       "             ('다경', 1),\n",
       "             ('교대', 5),\n",
       "             ('뽕', 18),\n",
       "             ('쟁이', 4),\n",
       "             ('재소자', 1),\n",
       "             ('의무실', 1),\n",
       "             ('계호', 1),\n",
       "             ('보고', 9),\n",
       "             ('단순', 4),\n",
       "             ('기분', 11),\n",
       "             ('목적', 6),\n",
       "             ('ㅅ', 74),\n",
       "             ('라던', 3),\n",
       "             ('데비', 1),\n",
       "             ('아그라', 1),\n",
       "             ('효과', 4),\n",
       "             ('단다', 2),\n",
       "             ('좌', 15),\n",
       "             ('만선', 3),\n",
       "             ('에요', 2),\n",
       "             ('누나', 7),\n",
       "             ('즤', 1),\n",
       "             ('겅', 1),\n",
       "             ('들어도', 1),\n",
       "             ('피', 13),\n",
       "             ('거꾸로', 1),\n",
       "             ('솟', 1),\n",
       "             ('는다', 17),\n",
       "             ('평화', 1),\n",
       "             ('소린데', 1),\n",
       "             ('미친', 35),\n",
       "             ('빠', 25),\n",
       "             ('고려', 3),\n",
       "             ('괜찮', 17),\n",
       "             ('중세', 4),\n",
       "             ('였', 63),\n",
       "             ('태종', 1),\n",
       "             ('편', 14),\n",
       "             ('국', 35),\n",
       "             ('물', 35),\n",
       "             ('세종', 2),\n",
       "             ('망조', 1),\n",
       "             ('양반', 5),\n",
       "             ('귀족', 1),\n",
       "             ('에게', 35),\n",
       "             ('성군', 1),\n",
       "             ('몰라도', 5),\n",
       "             ('당시', 5),\n",
       "             ('쳐도', 4),\n",
       "             ('500', 7),\n",
       "             ('후퇴', 1),\n",
       "             ('개막', 2),\n",
       "             ('장', 22),\n",
       "             ('한글', 1),\n",
       "             ('원래', 24),\n",
       "             ('독음', 1),\n",
       "             ('표기법', 1),\n",
       "             ('몽', 4),\n",
       "             ('아예', 5),\n",
       "             ('짱깨', 24),\n",
       "             ('노예', 15),\n",
       "             ('국가', 25),\n",
       "             ('전락', 3),\n",
       "             ('장본인', 1),\n",
       "             ('며', 26),\n",
       "             ('백성', 1),\n",
       "             ('노비', 6),\n",
       "             ('만들', 40),\n",
       "             ('한반도', 2),\n",
       "             ('근대', 1),\n",
       "             ('화', 26),\n",
       "             ('초석', 1),\n",
       "             ('틀어막', 1),\n",
       "             ('재앙', 17),\n",
       "             ('천하', 1),\n",
       "             ('개새끼', 21),\n",
       "             ('조건', 9),\n",
       "             ('죄', 12),\n",
       "             ('형벌', 1),\n",
       "             ('깨달', 2),\n",
       "             ('ㄴ', 58),\n",
       "             ('외모', 12),\n",
       "             ('다음', 34),\n",
       "             ('날', 29),\n",
       "             ('아침', 6),\n",
       "             ('놀란', 1),\n",
       "             ('기억', 10),\n",
       "             ('부탁', 4),\n",
       "             ('가만히', 6),\n",
       "             ('ㅡㅡ', 8),\n",
       "             ('총선', 7),\n",
       "             ('뮨', 2),\n",
       "             ('재인', 2),\n",
       "             ('독재', 10),\n",
       "             ('으려면', 2),\n",
       "             ('황', 2),\n",
       "             ('해야함', 3),\n",
       "             ('방향', 4),\n",
       "             ('뻔히', 1),\n",
       "             ('못함', 11),\n",
       "             ('티', 8),\n",
       "             ('케이', 2),\n",
       "             ('퍼', 19),\n",
       "             ('물갈이', 1),\n",
       "             ('과반', 1),\n",
       "             ('넘', 33),\n",
       "             ('민주당', 13),\n",
       "             ('자리', 16),\n",
       "             ('잇', 14),\n",
       "             ('우김', 1),\n",
       "             ('불출마', 3),\n",
       "             ('이미', 23),\n",
       "             ('오래', 7),\n",
       "             ('김무성', 6),\n",
       "             ('욕함', 6),\n",
       "             ('진', 28),\n",
       "             ('도배', 2),\n",
       "             ('ㅇㅇ', 33),\n",
       "             ('연애', 4),\n",
       "             ('할땐', 3),\n",
       "             ('았', 91),\n",
       "             ('???', 7),\n",
       "             ('하하하하하하', 2),\n",
       "             ('휴', 5),\n",
       "             ...])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:16.646065Z",
     "start_time": "2021-04-30T00:27:16.633099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 10434\n",
      "등장 빈도가 1번 이하인 희귀 단어의 수: 5995\n",
      "단어 집합에서 희귀 단어의 비율: 57.45639256277554\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 10.188300873525712\n"
     ]
    }
   ],
   "source": [
    "threshold = 2\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:16.661024Z",
     "start_time": "2021-04-30T00:27:16.648059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 4441\n"
     ]
    }
   ],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:16.810624Z",
     "start_time": "2021-04-30T00:27:16.663022Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:17.124784Z",
     "start_time": "2021-04-30T00:27:16.812619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 384\n",
      "리뷰의 평균 길이 : 13.471153846153847\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:17.139767Z",
     "start_time": "2021-04-30T00:27:17.126305Z"
    }
   },
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:17.154728Z",
     "start_time": "2021-04-30T00:27:17.141761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 300 이하인 샘플의 비율: 99.93131868131869\n"
     ]
    }
   ],
   "source": [
    "max_len = 300\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:27:17.215564Z",
     "start_time": "2021-04-30T00:27:17.161709Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T06:46:12.396775Z",
     "start_time": "2021-04-30T06:46:12.373351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    9,  308,  238,    1,\n",
       "          1,    1,    2, 1735,    1,  127,  127, 2139,   16,    2,  115,\n",
       "        431,   12,    2])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:28:44.794052Z",
     "start_time": "2021-04-30T00:28:44.776917Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional, Dropout,BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN():\n",
    "    # 모델 구조 정의하기\n",
    "    model = models.Sequential()\n",
    "    model.add(Embedding(vocab_size, 100,input_length = max_len))\n",
    "    model.add(layers.Dense(128, activation='relu')) \n",
    "    model.add(layers.Dense(128, activation='relu')) #ReLU 활성화함수 채택\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train, y_train, epochs=30, callbacks=[es, mc], batch_size=32, validation_split=0.2)\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    score = loaded_model.evaluate(X_test, y_test)[1]\n",
    "    print(\"테스트 정확도: %.4f\" % (score))\n",
    "    test_result.append(('DNN',score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:28:45.651457Z",
     "start_time": "2021-04-30T00:28:45.641485Z"
    }
   },
   "outputs": [],
   "source": [
    "def DNN():\n",
    "    # 모델 구조 정의하기\n",
    "    model = models.Sequential()\n",
    "    model.add(Embedding(vocab_size, 100,input_length = max_len))\n",
    "    model.add(layers.Dense(128, activation='relu')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(layers.Dense(128, activation='relu')) #ReLU 활성화함수 채택\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train, y_train, epochs=30, callbacks=[es, mc], batch_size=32, validation_split=0.2)\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    score = loaded_model.evaluate(X_test, y_test)[1]\n",
    "    print(\"테스트 정확도: %.4f\" % (score))\n",
    "    test_result.append(('DNN',score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:28:47.486216Z",
     "start_time": "2021-04-30T00:28:47.472679Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100,input_length = max_len))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train, y_train, epochs=30, callbacks=[es, mc], batch_size=32, validation_split=0.2)\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    score = loaded_model.evaluate(X_test, y_test)[1]\n",
    "    print(\"테스트 정확도: %.4f\" % (score))\n",
    "    test_result.append(('LSTM',score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:56:40.529652Z",
     "start_time": "2021-04-30T04:56:40.512697Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_2_layer():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100,input_length = max_len))\n",
    "    model.add(LSTM(128, return_sequences=True,activation='softmax'))\n",
    "    model.add(LSTM(128, activation='softmax'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train, y_train, epochs=100, callbacks=[es, mc], batch_size=32, validation_split=0.2)\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    score = loaded_model.evaluate(X_test, y_test)[1]\n",
    "    print(\"테스트 정확도: %.4f\" % (score))\n",
    "#     test_result.append(('LSTM_2layer',score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T06:01:05.893232Z",
     "start_time": "2021-04-30T04:56:41.216031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6836 - acc: 0.6471\n",
      "Epoch 00001: val_acc improved from -inf to 0.67048, saving model to best_model.h5\n",
      "110/110 [==============================] - 58s 531ms/step - loss: 0.6836 - acc: 0.6471 - val_loss: 0.6721 - val_acc: 0.6705\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6688 - acc: 0.6471\n",
      "Epoch 00002: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 63s 569ms/step - loss: 0.6688 - acc: 0.6471 - val_loss: 0.6572 - val_acc: 0.6705\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6595 - acc: 0.6471\n",
      "Epoch 00003: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 71s 649ms/step - loss: 0.6595 - acc: 0.6471 - val_loss: 0.6477 - val_acc: 0.6705\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6542 - acc: 0.6471\n",
      "Epoch 00004: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 71s 641ms/step - loss: 0.6542 - acc: 0.6471 - val_loss: 0.6422 - val_acc: 0.6705\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6514 - acc: 0.6471\n",
      "Epoch 00005: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 66s 600ms/step - loss: 0.6514 - acc: 0.6471 - val_loss: 0.6390 - val_acc: 0.6705\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6497 - acc: 0.6471\n",
      "Epoch 00006: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 72s 657ms/step - loss: 0.6497 - acc: 0.6471 - val_loss: 0.6366 - val_acc: 0.6705\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6485 - acc: 0.6471\n",
      "Epoch 00007: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 77s 703ms/step - loss: 0.6485 - acc: 0.6471 - val_loss: 0.6353 - val_acc: 0.6705\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6475 - acc: 0.6471\n",
      "Epoch 00008: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 73s 664ms/step - loss: 0.6475 - acc: 0.6471 - val_loss: 0.6348 - val_acc: 0.6705\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6465 - acc: 0.6471\n",
      "Epoch 00009: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 78s 710ms/step - loss: 0.6465 - acc: 0.6471 - val_loss: 0.6341 - val_acc: 0.6705\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6455 - acc: 0.6471\n",
      "Epoch 00010: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 74s 670ms/step - loss: 0.6455 - acc: 0.6471 - val_loss: 0.6336 - val_acc: 0.6705\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6446 - acc: 0.6471\n",
      "Epoch 00011: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 70s 633ms/step - loss: 0.6446 - acc: 0.6471 - val_loss: 0.6330 - val_acc: 0.6705\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6435 - acc: 0.6471\n",
      "Epoch 00012: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 69s 627ms/step - loss: 0.6435 - acc: 0.6471 - val_loss: 0.6328 - val_acc: 0.6705\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6424 - acc: 0.6471\n",
      "Epoch 00013: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 63s 575ms/step - loss: 0.6424 - acc: 0.6471 - val_loss: 0.6323 - val_acc: 0.6705\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6413 - acc: 0.6471\n",
      "Epoch 00014: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 64s 577ms/step - loss: 0.6413 - acc: 0.6471 - val_loss: 0.6319 - val_acc: 0.6705\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6401 - acc: 0.6471\n",
      "Epoch 00015: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 70s 636ms/step - loss: 0.6401 - acc: 0.6471 - val_loss: 0.6315 - val_acc: 0.6705\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6388 - acc: 0.6471\n",
      "Epoch 00016: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 63s 573ms/step - loss: 0.6388 - acc: 0.6471 - val_loss: 0.6310 - val_acc: 0.6705\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6374 - acc: 0.6471\n",
      "Epoch 00017: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 96s 869ms/step - loss: 0.6374 - acc: 0.6471 - val_loss: 0.6305 - val_acc: 0.6705\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6357 - acc: 0.6471\n",
      "Epoch 00018: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 163s 1s/step - loss: 0.6357 - acc: 0.6471 - val_loss: 0.6299 - val_acc: 0.6705\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6298 - acc: 0.6471\n",
      "Epoch 00019: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 113s 1s/step - loss: 0.6298 - acc: 0.6471 - val_loss: 0.6140 - val_acc: 0.6705\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6014 - acc: 0.6471\n",
      "Epoch 00020: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 71s 648ms/step - loss: 0.6014 - acc: 0.6471 - val_loss: 0.5958 - val_acc: 0.6705\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5696 - acc: 0.6471\n",
      "Epoch 00021: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 150s 1s/step - loss: 0.5696 - acc: 0.6471 - val_loss: 0.5740 - val_acc: 0.6705\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5308 - acc: 0.6471\n",
      "Epoch 00022: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 158s 1s/step - loss: 0.5308 - acc: 0.6471 - val_loss: 0.5439 - val_acc: 0.6705\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4879 - acc: 0.6471\n",
      "Epoch 00023: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 67s 610ms/step - loss: 0.4879 - acc: 0.6471 - val_loss: 0.5231 - val_acc: 0.6705\n",
      "Epoch 24/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4503 - acc: 0.7078\n",
      "Epoch 00024: val_acc improved from 0.67048 to 0.75057, saving model to best_model.h5\n",
      "110/110 [==============================] - 73s 663ms/step - loss: 0.4503 - acc: 0.7078 - val_loss: 0.5129 - val_acc: 0.7506\n",
      "Epoch 25/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4181 - acc: 0.8231\n",
      "Epoch 00025: val_acc improved from 0.75057 to 0.76659, saving model to best_model.h5\n",
      "110/110 [==============================] - 69s 629ms/step - loss: 0.4181 - acc: 0.8231 - val_loss: 0.4967 - val_acc: 0.7666\n",
      "Epoch 26/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3910 - acc: 0.8600\n",
      "Epoch 00026: val_acc improved from 0.76659 to 0.77918, saving model to best_model.h5\n",
      "110/110 [==============================] - 75s 684ms/step - loss: 0.3910 - acc: 0.8600 - val_loss: 0.4867 - val_acc: 0.7792\n",
      "Epoch 27/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3677 - acc: 0.8792\n",
      "Epoch 00027: val_acc improved from 0.77918 to 0.79176, saving model to best_model.h5\n",
      "110/110 [==============================] - 66s 600ms/step - loss: 0.3677 - acc: 0.8792 - val_loss: 0.4790 - val_acc: 0.7918\n",
      "Epoch 28/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3447 - acc: 0.8964\n",
      "Epoch 00028: val_acc improved from 0.79176 to 0.79634, saving model to best_model.h5\n",
      "110/110 [==============================] - 61s 553ms/step - loss: 0.3447 - acc: 0.8964 - val_loss: 0.4731 - val_acc: 0.7963\n",
      "Epoch 29/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3250 - acc: 0.9081\n",
      "Epoch 00029: val_acc did not improve from 0.79634\n",
      "110/110 [==============================] - 61s 552ms/step - loss: 0.3250 - acc: 0.9081 - val_loss: 0.4720 - val_acc: 0.7952\n",
      "Epoch 30/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3072 - acc: 0.9164\n",
      "Epoch 00030: val_acc improved from 0.79634 to 0.81121, saving model to best_model.h5\n",
      "110/110 [==============================] - 69s 624ms/step - loss: 0.3072 - acc: 0.9164 - val_loss: 0.4591 - val_acc: 0.8112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.2896 - acc: 0.9233\n",
      "Epoch 00031: val_acc did not improve from 0.81121\n",
      "110/110 [==============================] - 60s 549ms/step - loss: 0.2896 - acc: 0.9233 - val_loss: 0.4575 - val_acc: 0.8112\n",
      "Epoch 32/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.2731 - acc: 0.9293\n",
      "Epoch 00032: val_acc improved from 0.81121 to 0.81693, saving model to best_model.h5\n",
      "110/110 [==============================] - 64s 584ms/step - loss: 0.2731 - acc: 0.9293 - val_loss: 0.4525 - val_acc: 0.8169\n",
      "Epoch 33/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.2376 - acc: 0.9390\n",
      "Epoch 00033: val_acc improved from 0.81693 to 0.82380, saving model to best_model.h5\n",
      "110/110 [==============================] - 76s 692ms/step - loss: 0.2376 - acc: 0.9390 - val_loss: 0.4687 - val_acc: 0.8238\n",
      "Epoch 34/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.2151 - acc: 0.9405\n",
      "Epoch 00034: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 63s 570ms/step - loss: 0.2151 - acc: 0.9405 - val_loss: 0.4983 - val_acc: 0.8089\n",
      "Epoch 35/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.2109 - acc: 0.9413\n",
      "Epoch 00035: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 60s 541ms/step - loss: 0.2109 - acc: 0.9413 - val_loss: 0.5009 - val_acc: 0.8089\n",
      "Epoch 36/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1990 - acc: 0.9433\n",
      "Epoch 00036: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 72s 656ms/step - loss: 0.1990 - acc: 0.9433 - val_loss: 0.4902 - val_acc: 0.8204\n",
      "Epoch 37/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1860 - acc: 0.9473\n",
      "Epoch 00037: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 63s 575ms/step - loss: 0.1860 - acc: 0.9473 - val_loss: 0.5070 - val_acc: 0.8112\n",
      "Epoch 38/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1773 - acc: 0.9511\n",
      "Epoch 00038: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 67s 608ms/step - loss: 0.1773 - acc: 0.9511 - val_loss: 0.4956 - val_acc: 0.8192\n",
      "Epoch 39/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1711 - acc: 0.9565\n",
      "Epoch 00039: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 73s 662ms/step - loss: 0.1711 - acc: 0.9565 - val_loss: 0.5065 - val_acc: 0.8146\n",
      "Epoch 40/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1644 - acc: 0.9568\n",
      "Epoch 00040: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 74s 676ms/step - loss: 0.1644 - acc: 0.9568 - val_loss: 0.4968 - val_acc: 0.8204\n",
      "Epoch 41/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1580 - acc: 0.9599\n",
      "Epoch 00041: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 75s 679ms/step - loss: 0.1580 - acc: 0.9599 - val_loss: 0.5079 - val_acc: 0.8181\n",
      "Epoch 42/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1533 - acc: 0.9625\n",
      "Epoch 00042: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 66s 598ms/step - loss: 0.1533 - acc: 0.9625 - val_loss: 0.5231 - val_acc: 0.8204\n",
      "Epoch 43/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1504 - acc: 0.9625\n",
      "Epoch 00043: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 67s 607ms/step - loss: 0.1504 - acc: 0.9625 - val_loss: 0.5149 - val_acc: 0.8204\n",
      "Epoch 44/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1460 - acc: 0.9628\n",
      "Epoch 00044: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 60s 547ms/step - loss: 0.1460 - acc: 0.9628 - val_loss: 0.5328 - val_acc: 0.8169\n",
      "Epoch 45/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1428 - acc: 0.9648\n",
      "Epoch 00045: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 64s 586ms/step - loss: 0.1428 - acc: 0.9648 - val_loss: 0.5395 - val_acc: 0.8135\n",
      "Epoch 46/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1385 - acc: 0.9648\n",
      "Epoch 00046: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 61s 555ms/step - loss: 0.1385 - acc: 0.9648 - val_loss: 0.5541 - val_acc: 0.8124\n",
      "Epoch 47/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1356 - acc: 0.9677\n",
      "Epoch 00047: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 59s 534ms/step - loss: 0.1356 - acc: 0.9677 - val_loss: 0.5557 - val_acc: 0.8101\n",
      "Epoch 48/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1322 - acc: 0.9671\n",
      "Epoch 00048: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 59s 535ms/step - loss: 0.1322 - acc: 0.9671 - val_loss: 0.5496 - val_acc: 0.8066\n",
      "Epoch 49/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1296 - acc: 0.9688\n",
      "Epoch 00049: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 65s 587ms/step - loss: 0.1296 - acc: 0.9688 - val_loss: 0.5704 - val_acc: 0.8146\n",
      "Epoch 50/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1284 - acc: 0.9677\n",
      "Epoch 00050: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 60s 543ms/step - loss: 0.1284 - acc: 0.9677 - val_loss: 0.5660 - val_acc: 0.8055\n",
      "Epoch 51/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1245 - acc: 0.9691\n",
      "Epoch 00051: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 63s 571ms/step - loss: 0.1245 - acc: 0.9691 - val_loss: 0.5652 - val_acc: 0.8055\n",
      "Epoch 52/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1234 - acc: 0.9682\n",
      "Epoch 00052: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 59s 537ms/step - loss: 0.1234 - acc: 0.9682 - val_loss: 0.5889 - val_acc: 0.8009\n",
      "Epoch 00052: early stopping\n",
      "46/46 [==============================] - 9s 195ms/step - loss: 0.4834 - acc: 0.8166 4s \n",
      "테스트 정확도: 0.8166\n"
     ]
    }
   ],
   "source": [
    "lstm_2_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:45:02.556587Z",
     "start_time": "2021-04-30T00:45:02.537727Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_2_layer():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100,input_length = max_len))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(64, return_sequences=True,activation='softmax'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(64, return_sequences=True,activation='softmax'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(64, return_sequences=True,activation='softmax'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(64, return_sequences=True,activation='softmax'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(64, activation='softmax'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train, y_train, epochs=30, callbacks=[es, mc], batch_size=32, validation_split=0.2)\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    score = loaded_model.evaluate(X_test, y_test)[1]\n",
    "    print(\"테스트 정확도: %.4f\" % (score))\n",
    "    test_result.append(('LSTM_2layer',score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:54:06.913051Z",
     "start_time": "2021-04-30T00:45:02.991437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6603 - acc: 0.6465\n",
      "Epoch 00001: val_acc improved from -inf to 0.67048, saving model to best_model.h5\n",
      "110/110 [==============================] - 103s 934ms/step - loss: 0.6603 - acc: 0.6465 - val_loss: 0.6358 - val_acc: 0.6705\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5556 - acc: 0.7221\n",
      "Epoch 00002: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 104s 947ms/step - loss: 0.5556 - acc: 0.7221 - val_loss: 0.6339 - val_acc: 0.6705\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3468 - acc: 0.8646\n",
      "Epoch 00003: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 105s 956ms/step - loss: 0.3468 - acc: 0.8646 - val_loss: 0.6350 - val_acc: 0.6705\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.2412 - acc: 0.9144\n",
      "Epoch 00004: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 109s 987ms/step - loss: 0.2412 - acc: 0.9144 - val_loss: 0.6372 - val_acc: 0.6705\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1622 - acc: 0.9465"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-1af1c965da76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlstm_2_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-1508aea2a597>\u001b[0m in \u001b[0;36mlstm_2_layer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mmc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1121\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1123\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1124\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_2_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### relu에서 softmax로 변환시, overfitting이 일어난다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T01:03:26.150085Z",
     "start_time": "2021-04-30T01:03:26.117611Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_2_layer():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100,input_length = max_len))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64, return_sequences=True,activation='softmax'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64, return_sequences=True,activation='softmax'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64, return_sequences=True,activation='softmax'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64, return_sequences=True,activation='softmax'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64, activation='softmax'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train, y_train, epochs=30, callbacks=[es, mc], batch_size=128, validation_split=0.2)\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    score = loaded_model.evaluate(X_test, y_test)[1]\n",
    "    print(\"테스트 정확도: %.4f\" % (score))\n",
    "    test_result.append(('LSTM_2layer',score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T01:10:05.909202Z",
     "start_time": "2021-04-30T01:03:26.866268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6769 - acc: 0.6420\n",
      "Epoch 00001: val_acc improved from -inf to 0.67048, saving model to best_model.h5\n",
      "28/28 [==============================] - 176s 6s/step - loss: 0.6769 - acc: 0.6420 - val_loss: 0.6550 - val_acc: 0.6705\n",
      "Epoch 2/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6547 - acc: 0.6471\n",
      "Epoch 00002: val_acc did not improve from 0.67048\n",
      "28/28 [==============================] - 185s 7s/step - loss: 0.6547 - acc: 0.6471 - val_loss: 0.6372 - val_acc: 0.6705\n",
      "Epoch 3/30\n",
      " 2/28 [=>............................] - ETA: 1:04 - loss: 0.6411 - acc: 0.6680"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-1af1c965da76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlstm_2_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-d7a5b8b023c3>\u001b[0m in \u001b[0;36mlstm_2_layer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mmc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_2_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T01:11:09.040951Z",
     "start_time": "2021-04-30T01:11:09.026990Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_2_layer():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100,input_length = max_len))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train, y_train, epochs=30, callbacks=[es, mc], batch_size=128, validation_split=0.2)\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    score = loaded_model.evaluate(X_test, y_test)[1]\n",
    "    print(\"테스트 정확도: %.4f\" % (score))\n",
    "    test_result.append(('LSTM_2layer',score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T01:53:11.583405Z",
     "start_time": "2021-04-30T01:11:13.327075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.9429 - acc: 0.5306\n",
      "Epoch 00001: val_acc improved from -inf to 0.67048, saving model to best_model.h5\n",
      "28/28 [==============================] - 183s 7s/step - loss: 0.9429 - acc: 0.5306 - val_loss: 0.6507 - val_acc: 0.6705\n",
      "Epoch 2/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.8535 - acc: 0.5581\n",
      "Epoch 00002: val_acc did not improve from 0.67048\n",
      "28/28 [==============================] - 204s 7s/step - loss: 0.8535 - acc: 0.5581 - val_loss: 0.6366 - val_acc: 0.6705\n",
      "Epoch 3/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7948 - acc: 0.5701\n",
      "Epoch 00003: val_acc did not improve from 0.67048\n",
      "28/28 [==============================] - 175s 6s/step - loss: 0.7948 - acc: 0.5701 - val_loss: 0.6291 - val_acc: 0.6705\n",
      "Epoch 4/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7437 - acc: 0.5999\n",
      "Epoch 00004: val_acc did not improve from 0.67048\n",
      "28/28 [==============================] - 173s 6s/step - loss: 0.7437 - acc: 0.5999 - val_loss: 0.6331 - val_acc: 0.6705\n",
      "Epoch 5/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7449 - acc: 0.5956\n",
      "Epoch 00005: val_acc did not improve from 0.67048\n",
      "28/28 [==============================] - 150s 5s/step - loss: 0.7449 - acc: 0.5956 - val_loss: 0.6371 - val_acc: 0.6705\n",
      "Epoch 6/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7275 - acc: 0.5982\n",
      "Epoch 00006: val_acc did not improve from 0.67048\n",
      "28/28 [==============================] - 174s 6s/step - loss: 0.7275 - acc: 0.5982 - val_loss: 0.6999 - val_acc: 0.3295\n",
      "Epoch 7/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7250 - acc: 0.5913\n",
      "Epoch 00007: val_acc did not improve from 0.67048\n",
      "28/28 [==============================] - 156s 6s/step - loss: 0.7250 - acc: 0.5913 - val_loss: 0.6598 - val_acc: 0.6682\n",
      "Epoch 8/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7024 - acc: 0.6022\n",
      "Epoch 00008: val_acc did not improve from 0.67048\n",
      "28/28 [==============================] - 179s 6s/step - loss: 0.7024 - acc: 0.6022 - val_loss: 0.6471 - val_acc: 0.6705\n",
      "Epoch 9/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7010 - acc: 0.6002\n",
      "Epoch 00009: val_acc did not improve from 0.67048\n",
      "28/28 [==============================] - 181s 6s/step - loss: 0.7010 - acc: 0.6002 - val_loss: 0.7323 - val_acc: 0.3295\n",
      "Epoch 10/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6870 - acc: 0.6168\n",
      "Epoch 00010: val_acc did not improve from 0.67048\n",
      "28/28 [==============================] - 227s 8s/step - loss: 0.6870 - acc: 0.6168 - val_loss: 0.7150 - val_acc: 0.4462\n",
      "Epoch 11/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6724 - acc: 0.6285\n",
      "Epoch 00011: val_acc did not improve from 0.67048\n",
      "28/28 [==============================] - 212s 8s/step - loss: 0.6724 - acc: 0.6285 - val_loss: 0.7286 - val_acc: 0.3295\n",
      "Epoch 12/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6583 - acc: 0.6191\n",
      "Epoch 00012: val_acc did not improve from 0.67048\n",
      "28/28 [==============================] - 218s 8s/step - loss: 0.6583 - acc: 0.6191 - val_loss: 0.8105 - val_acc: 0.3295\n",
      "Epoch 13/30\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6519 - acc: 0.6428\n",
      "Epoch 00013: val_acc did not improve from 0.67048\n",
      "28/28 [==============================] - 178s 6s/step - loss: 0.6519 - acc: 0.6428 - val_loss: 0.7153 - val_acc: 0.3295\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1224 test_function  *\n        return step_function(self, iterator)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1215 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1208 run_step  **\n        outputs = model.test_step(data)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1174 test_step\n        y_pred = self(x, training=False)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_8 is incompatible with the layer: expected axis -1 of input shape to have value 100 but received input with shape [None, 300]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-1af1c965da76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlstm_2_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-59a6d1d5bda1>\u001b[0m in \u001b[0;36mlstm_2_layer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"테스트 정확도: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mtest_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LSTM_2layer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1224 test_function  *\n        return step_function(self, iterator)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1215 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1208 run_step  **\n        outputs = model.test_step(data)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1174 test_step\n        y_pred = self(x, training=False)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_8 is incompatible with the layer: expected axis -1 of input shape to have value 100 but received input with shape [None, 300]\n"
     ]
    }
   ],
   "source": [
    "lstm_2_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T15:16:04.732017Z",
     "start_time": "2021-04-29T15:16:04.720049Z"
    }
   },
   "outputs": [],
   "source": [
    "def bidirectional_lstm():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100,input_length = max_len))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Bidirectional(LSTM(128,activation='relu')))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train, y_train, epochs=30, callbacks=[es, mc], batch_size=32, validation_split=0.2)\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    score = loaded_model.evaluate(X_test, y_test)[1]\n",
    "    print(\"테스트 정확도: %.4f\" % (score))\n",
    "    test_result.append(('Bi-LSTM',score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T15:16:13.328629Z",
     "start_time": "2021-04-29T15:16:13.312672Z"
    }
   },
   "outputs": [],
   "source": [
    "def bidirectional_lstm_2():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100,input_length = max_len))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True,activation='relu')))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Bidirectional(LSTM(128,activation='relu')))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train, y_train, epochs=30, callbacks=[es, mc], batch_size=32, validation_split=0.2)\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    score = loaded_model.evaluate(X_test, y_test)[1]\n",
    "    print(\"테스트 정확도: %.4f\" % (score))\n",
    "    test_result.append(('Bi-LSTM-2',score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T15:16:38.056435Z",
     "start_time": "2021-04-29T15:16:38.049453Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "def cnn_1D():\n",
    "    model = Sequential()    \n",
    "    model.add(Embedding(vocab_size, 100,input_length = max_len))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(256, 3, padding='valid', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train, y_train, epochs=30, callbacks=[es, mc], batch_size=32, validation_split=0.2)\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    score = loaded_model.evaluate(X_test, y_test)[1]\n",
    "    print(\"테스트 정확도: %.4f\" % (score))\n",
    "    test_result.append(('1D-CNN',score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T16:07:44.340252Z",
     "start_time": "2021-04-29T15:16:39.998935Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DNN 모델 진행합니다.\n",
      "Epoch 1/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6649 - acc: 0.6440\n",
      "Epoch 00001: val_acc improved from -inf to 0.67048, saving model to best_model.h5\n",
      "110/110 [==============================] - 7s 68ms/step - loss: 0.6649 - acc: 0.6440 - val_loss: 0.6359 - val_acc: 0.6705\n",
      "Epoch 2/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6482 - acc: 0.6495\n",
      "Epoch 00002: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 7s 68ms/step - loss: 0.6483 - acc: 0.6493 - val_loss: 0.6345 - val_acc: 0.6705\n",
      "Epoch 3/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6468 - acc: 0.6507\n",
      "Epoch 00003: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 8s 69ms/step - loss: 0.6470 - acc: 0.6504 - val_loss: 0.6444 - val_acc: 0.6705\n",
      "Epoch 4/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6464 - acc: 0.6506\n",
      "Epoch 00004: val_acc improved from 0.67048 to 0.67175, saving model to best_model.h5\n",
      "110/110 [==============================] - 8s 73ms/step - loss: 0.6462 - acc: 0.6509 - val_loss: 0.6367 - val_acc: 0.6718\n",
      "Epoch 5/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6459 - acc: 0.650 - ETA: 0s - loss: 0.6458 - acc: 0.6510\n",
      "Epoch 00005: val_acc improved from 0.67175 to 0.67201, saving model to best_model.h5\n",
      "110/110 [==============================] - 9s 82ms/step - loss: 0.6460 - acc: 0.6508 - val_loss: 0.6338 - val_acc: 0.6720\n",
      "Epoch 6/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6463 - acc: 0.6508\n",
      "Epoch 00006: val_acc did not improve from 0.67201\n",
      "110/110 [==============================] - 9s 78ms/step - loss: 0.6461 - acc: 0.6511 - val_loss: 0.6469 - val_acc: 0.6717\n",
      "Epoch 7/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6455 - acc: 0.6510\n",
      "Epoch 00007: val_acc improved from 0.67201 to 0.67210, saving model to best_model.h5\n",
      "110/110 [==============================] - 9s 81ms/step - loss: 0.6453 - acc: 0.6513 - val_loss: 0.6671 - val_acc: 0.6721\n",
      "Epoch 8/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6456 - acc: 0.6511\n",
      "Epoch 00008: val_acc did not improve from 0.67210\n",
      "110/110 [==============================] - 9s 78ms/step - loss: 0.6454 - acc: 0.6514 - val_loss: 0.8465 - val_acc: 0.6720\n",
      "Epoch 9/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6455 - acc: 0.6515\n",
      "Epoch 00009: val_acc improved from 0.67210 to 0.67213, saving model to best_model.h5\n",
      "110/110 [==============================] - 9s 82ms/step - loss: 0.6455 - acc: 0.6515 - val_loss: 0.6345 - val_acc: 0.6721\n",
      "Epoch 10/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6449 - acc: 0.6520\n",
      "Epoch 00010: val_acc did not improve from 0.67213\n",
      "110/110 [==============================] - 9s 85ms/step - loss: 0.6452 - acc: 0.6515 - val_loss: 0.6387 - val_acc: 0.6719\n",
      "Epoch 11/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6451 - acc: 0.6520\n",
      "Epoch 00011: val_acc did not improve from 0.67213\n",
      "110/110 [==============================] - 8s 75ms/step - loss: 0.6453 - acc: 0.6517 - val_loss: 0.6432 - val_acc: 0.6720\n",
      "Epoch 12/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6450 - acc: 0.6522\n",
      "Epoch 00012: val_acc did not improve from 0.67213\n",
      "110/110 [==============================] - 9s 79ms/step - loss: 0.6450 - acc: 0.6522 - val_loss: 0.6639 - val_acc: 0.6718\n",
      "Epoch 13/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6444 - acc: 0.6520\n",
      "Epoch 00013: val_acc did not improve from 0.67213\n",
      "110/110 [==============================] - 11s 101ms/step - loss: 0.6448 - acc: 0.6515 - val_loss: 0.7490 - val_acc: 0.3327\n",
      "Epoch 14/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6453 - acc: 0.6518\n",
      "Epoch 00014: val_acc did not improve from 0.67213\n",
      "110/110 [==============================] - 11s 100ms/step - loss: 0.6453 - acc: 0.6518 - val_loss: 0.7691 - val_acc: 0.6709\n",
      "Epoch 15/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6453 - acc: 0.6512\n",
      "Epoch 00015: val_acc did not improve from 0.67213\n",
      "110/110 [==============================] - 8s 74ms/step - loss: 0.6452 - acc: 0.6512 - val_loss: 0.6415 - val_acc: 0.6720\n",
      "Epoch 00015: early stopping\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.6555 - acc: 0.6425\n",
      "테스트 정확도: 0.6425\n",
      "\n",
      "LSTM 모델 진행합니다.\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5844 - acc: 0.7046\n",
      "Epoch 00001: val_acc improved from -inf to 0.67048, saving model to best_model.h5\n",
      "110/110 [==============================] - 36s 325ms/step - loss: 0.5844 - acc: 0.7046 - val_loss: 0.6266 - val_acc: 0.6705\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.2252 - acc: 0.9164\n",
      "Epoch 00002: val_acc improved from 0.67048 to 0.68307, saving model to best_model.h5\n",
      "110/110 [==============================] - 36s 327ms/step - loss: 0.2252 - acc: 0.9164 - val_loss: 0.5836 - val_acc: 0.6831\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1108 - acc: 0.9645\n",
      "Epoch 00003: val_acc improved from 0.68307 to 0.74943, saving model to best_model.h5\n",
      "110/110 [==============================] - 34s 306ms/step - loss: 0.1108 - acc: 0.9645 - val_loss: 0.5139 - val_acc: 0.7494\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0655 - acc: 0.9808\n",
      "Epoch 00004: val_acc improved from 0.74943 to 0.77803, saving model to best_model.h5\n",
      "110/110 [==============================] - 31s 285ms/step - loss: 0.0655 - acc: 0.9808 - val_loss: 0.5396 - val_acc: 0.7780\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0460 - acc: 0.9883\n",
      "Epoch 00005: val_acc improved from 0.77803 to 0.79291, saving model to best_model.h5\n",
      "110/110 [==============================] - 32s 292ms/step - loss: 0.0460 - acc: 0.9883 - val_loss: 0.6662 - val_acc: 0.7929\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0234 - acc: 0.9948\n",
      "Epoch 00006: val_acc improved from 0.79291 to 0.80320, saving model to best_model.h5\n",
      "110/110 [==============================] - 36s 328ms/step - loss: 0.0234 - acc: 0.9948 - val_loss: 0.7872 - val_acc: 0.8032\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0222 - acc: 0.9937\n",
      "Epoch 00007: val_acc did not improve from 0.80320\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.0222 - acc: 0.9937 - val_loss: 0.9790 - val_acc: 0.7563\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0152 - acc: 0.9954- ETA: 5s - lo\n",
      "Epoch 00008: val_acc did not improve from 0.80320\n",
      "110/110 [==============================] - 29s 262ms/step - loss: 0.0152 - acc: 0.9954 - val_loss: 0.9872 - val_acc: 0.7826\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0124 - acc: 0.9963\n",
      "Epoch 00009: val_acc did not improve from 0.80320\n",
      "110/110 [==============================] - 35s 315ms/step - loss: 0.0124 - acc: 0.9963 - val_loss: 1.0238 - val_acc: 0.7849\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0182 - acc: 0.9963\n",
      "Epoch 00010: val_acc did not improve from 0.80320\n",
      "110/110 [==============================] - 31s 283ms/step - loss: 0.0182 - acc: 0.9963 - val_loss: 1.0338 - val_acc: 0.7941\n",
      "Epoch 11/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0356 - acc: 0.9897\n",
      "Epoch 00011: val_acc did not improve from 0.80320\n",
      "110/110 [==============================] - 31s 282ms/step - loss: 0.0356 - acc: 0.9897 - val_loss: 1.1242 - val_acc: 0.7506\n",
      "Epoch 12/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0288 - acc: 0.9917\n",
      "Epoch 00012: val_acc did not improve from 0.80320\n",
      "110/110 [==============================] - 34s 308ms/step - loss: 0.0288 - acc: 0.9917 - val_loss: 1.0076 - val_acc: 0.7872\n",
      "Epoch 13/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0227 - acc: 0.9943\n",
      "Epoch 00013: val_acc did not improve from 0.80320\n",
      "110/110 [==============================] - 35s 319ms/step - loss: 0.0227 - acc: 0.9943 - val_loss: 1.0847 - val_acc: 0.7895\n",
      "Epoch 00013: early stopping\n",
      "46/46 [==============================] - 3s 59ms/step - loss: 0.8182 - acc: 0.8104\n",
      "테스트 정확도: 0.8104\n",
      "\n",
      "LSTM_2layer 모델 진행합니다.\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6442\n",
      "Epoch 00001: val_acc improved from -inf to 0.67048, saving model to best_model.h5\n",
      "110/110 [==============================] - 64s 580ms/step - loss: nan - acc: 0.6442 - val_loss: nan - val_acc: 0.6705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1664: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00002: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 63s 577ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00003: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 65s 590ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00004: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 60s 543ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00005: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 59s 533ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00006: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 61s 558ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00007: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 61s 553ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00008: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 64s 579ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00009: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 62s 563ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00010: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 67s 607ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 00010: early stopping\n",
      "46/46 [==============================] - 6s 138ms/step - loss: nan - acc: 0.6408\n",
      "테스트 정확도: 0.6408\n",
      "\n",
      "Bi-LSTM 모델 진행합니다.\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6368\n",
      "Epoch 00001: val_acc improved from -inf to 0.67048, saving model to best_model.h5\n",
      "110/110 [==============================] - 44s 400ms/step - loss: nan - acc: 0.6368 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00002: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 45s 405ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00003: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 40s 365ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00004: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 43s 388ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00005: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 42s 386ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00006: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 41s 374ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00007: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 43s 395ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00008: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 46s 420ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00009: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 46s 417ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00010: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 47s 425ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 00010: early stopping\n",
      "46/46 [==============================] - 3s 64ms/step - loss: nan - acc: 0.6408\n",
      "테스트 정확도: 0.6408\n",
      "\n",
      "Bi-LSTM 2층 모델 진행합니다.\n",
      "Epoch 1/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6457\n",
      "Epoch 00001: val_acc improved from -inf to 0.67048, saving model to best_model.h5\n",
      "110/110 [==============================] - 115s 1s/step - loss: nan - acc: 0.6457 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00002: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 115s 1s/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00003: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 114s 1s/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00004: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 120s 1s/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00005: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 112s 1s/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00006: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 110s 998ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00007: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 111s 1s/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00008: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 109s 991ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00009: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 112s 1s/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - ETA: 0s - loss: nan - acc: 0.6471\n",
      "Epoch 00010: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 110s 998ms/step - loss: nan - acc: 0.6471 - val_loss: nan - val_acc: 0.6705\n",
      "Epoch 00010: early stopping\n",
      "46/46 [==============================] - 10s 218ms/step - loss: nan - acc: 0.6408\n",
      "테스트 정확도: 0.6408\n",
      "\u0001D-CNN 모델 진행합니다.\n",
      "Epoch 1/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.6654 - acc: 0.6700\n",
      "Epoch 00001: val_acc improved from -inf to 0.67048, saving model to best_model.h5\n",
      "110/110 [==============================] - 13s 120ms/step - loss: 0.6656 - acc: 0.6697 - val_loss: 0.7547 - val_acc: 0.6705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.3190 - acc: 0.8721\n",
      "Epoch 00002: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 14s 126ms/step - loss: 0.3188 - acc: 0.8724 - val_loss: 1.3427 - val_acc: 0.6705\n",
      "Epoch 3/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9579\n",
      "Epoch 00003: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 13s 118ms/step - loss: 0.1289 - acc: 0.9579 - val_loss: 1.5718 - val_acc: 0.6705\n",
      "Epoch 4/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9802\n",
      "Epoch 00004: val_acc did not improve from 0.67048\n",
      "110/110 [==============================] - 13s 119ms/step - loss: 0.0717 - acc: 0.9803 - val_loss: 1.3387 - val_acc: 0.6705\n",
      "Epoch 5/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9882\n",
      "Epoch 00005: val_acc improved from 0.67048 to 0.69336, saving model to best_model.h5\n",
      "110/110 [==============================] - 14s 129ms/step - loss: 0.0443 - acc: 0.9880 - val_loss: 0.9912 - val_acc: 0.6934\n",
      "Epoch 6/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9908\n",
      "Epoch 00006: val_acc improved from 0.69336 to 0.79176, saving model to best_model.h5\n",
      "110/110 [==============================] - 14s 128ms/step - loss: 0.0410 - acc: 0.9903 - val_loss: 0.5471 - val_acc: 0.7918\n",
      "Epoch 7/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9871\n",
      "Epoch 00007: val_acc improved from 0.79176 to 0.81007, saving model to best_model.h5\n",
      "110/110 [==============================] - 13s 122ms/step - loss: 0.0454 - acc: 0.9871 - val_loss: 0.4927 - val_acc: 0.8101\n",
      "Epoch 8/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9908\n",
      "Epoch 00008: val_acc improved from 0.81007 to 0.81350, saving model to best_model.h5\n",
      "110/110 [==============================] - 13s 119ms/step - loss: 0.0352 - acc: 0.9908 - val_loss: 0.5811 - val_acc: 0.8135\n",
      "Epoch 9/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9908\n",
      "Epoch 00009: val_acc improved from 0.81350 to 0.82380, saving model to best_model.h5\n",
      "110/110 [==============================] - 13s 122ms/step - loss: 0.0345 - acc: 0.9908 - val_loss: 0.7143 - val_acc: 0.8238\n",
      "Epoch 10/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9857\n",
      "Epoch 00010: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 17s 156ms/step - loss: 0.0423 - acc: 0.9857 - val_loss: 0.7101 - val_acc: 0.7609\n",
      "Epoch 11/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9868\n",
      "Epoch 00011: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 15s 134ms/step - loss: 0.0396 - acc: 0.9868 - val_loss: 0.6426 - val_acc: 0.7975\n",
      "Epoch 12/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9857\n",
      "Epoch 00012: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 13s 120ms/step - loss: 0.0380 - acc: 0.9854 - val_loss: 0.6925 - val_acc: 0.8204\n",
      "Epoch 13/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9848\n",
      "Epoch 00013: val_acc did not improve from 0.82380\n",
      "110/110 [==============================] - 13s 122ms/step - loss: 0.0493 - acc: 0.9848 - val_loss: 0.6750 - val_acc: 0.8204\n",
      "Epoch 14/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9900\n",
      "Epoch 00014: val_acc improved from 0.82380 to 0.83753, saving model to best_model.h5\n",
      "110/110 [==============================] - 13s 120ms/step - loss: 0.0304 - acc: 0.9897 - val_loss: 0.6969 - val_acc: 0.8375\n",
      "Epoch 15/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9914\n",
      "Epoch 00015: val_acc improved from 0.83753 to 0.84096, saving model to best_model.h5\n",
      "110/110 [==============================] - 14s 124ms/step - loss: 0.0292 - acc: 0.9914 - val_loss: 0.7765 - val_acc: 0.8410\n",
      "Epoch 16/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9900\n",
      "Epoch 00016: val_acc did not improve from 0.84096\n",
      "110/110 [==============================] - 14s 128ms/step - loss: 0.0278 - acc: 0.9900 - val_loss: 0.7176 - val_acc: 0.8341\n",
      "Epoch 17/30\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9874\n",
      "Epoch 00017: val_acc did not improve from 0.84096\n",
      "110/110 [==============================] - 14s 126ms/step - loss: 0.0366 - acc: 0.9868 - val_loss: 0.7858 - val_acc: 0.8249\n",
      "Epoch 00017: early stopping\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 0.8983 - acc: 0.8063: 0s - loss: 0.8889 - acc: 0.80\n",
      "테스트 정확도: 0.8063\n"
     ]
    }
   ],
   "source": [
    "test_result = []    \n",
    "print(\"\\nDNN 모델 진행합니다.\")\n",
    "DNN()\n",
    "print(\"\\nLSTM 모델 진행합니다.\")\n",
    "lstm()\n",
    "print(\"\\nLSTM_2layer 모델 진행합니다.\")\n",
    "lstm_2_layer()\n",
    "print(\"\\nBi-LSTM 모델 진행합니다.\")\n",
    "bidirectional_lstm()\n",
    "print(\"\\nBi-LSTM 2층 모델 진행합니다.\")\n",
    "bidirectional_lstm_2()\n",
    "print(\"\\1D-CNN 모델 진행합니다.\")\n",
    "cnn_1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T01:59:46.891027Z",
     "start_time": "2021-04-23T01:59:46.885042Z"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T01:59:47.146361Z",
     "start_time": "2021-04-23T01:59:47.127411Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T02:11:24.302109Z",
     "start_time": "2021-04-23T02:09:09.912797Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6294 - acc: 0.6471\n",
      "Epoch 00001: val_acc improved from -inf to 0.66934, saving model to best_model.h5\n",
      "14/14 [==============================] - 12s 833ms/step - loss: 0.6294 - acc: 0.6471 - val_loss: 0.6119 - val_acc: 0.6693\n",
      "Epoch 2/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6061 - acc: 0.6540\n",
      "Epoch 00002: val_acc improved from 0.66934 to 0.67506, saving model to best_model.h5\n",
      "14/14 [==============================] - 9s 661ms/step - loss: 0.6061 - acc: 0.6540 - val_loss: 0.5906 - val_acc: 0.6751\n",
      "Epoch 3/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5835 - acc: 0.6883\n",
      "Epoch 00003: val_acc improved from 0.67506 to 0.72654, saving model to best_model.h5\n",
      "14/14 [==============================] - 7s 480ms/step - loss: 0.5835 - acc: 0.6883 - val_loss: 0.5950 - val_acc: 0.7265\n",
      "Epoch 4/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5517 - acc: 0.7410\n",
      "Epoch 00004: val_acc did not improve from 0.72654\n",
      "14/14 [==============================] - 9s 661ms/step - loss: 0.5517 - acc: 0.7410 - val_loss: 0.5623 - val_acc: 0.7162\n",
      "Epoch 5/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5137 - acc: 0.7748\n",
      "Epoch 00005: val_acc improved from 0.72654 to 0.74600, saving model to best_model.h5\n",
      "14/14 [==============================] - 7s 532ms/step - loss: 0.5137 - acc: 0.7748 - val_loss: 0.5425 - val_acc: 0.7460\n",
      "Epoch 6/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4703 - acc: 0.8071\n",
      "Epoch 00006: val_acc did not improve from 0.74600\n",
      "14/14 [==============================] - 11s 754ms/step - loss: 0.4703 - acc: 0.8071 - val_loss: 0.5632 - val_acc: 0.7254\n",
      "Epoch 7/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4373 - acc: 0.8286\n",
      "Epoch 00007: val_acc improved from 0.74600 to 0.75973, saving model to best_model.h5\n",
      "14/14 [==============================] - 9s 660ms/step - loss: 0.4373 - acc: 0.8286 - val_loss: 0.5048 - val_acc: 0.7597\n",
      "Epoch 8/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3997 - acc: 0.8537\n",
      "Epoch 00008: val_acc improved from 0.75973 to 0.76430, saving model to best_model.h5\n",
      "14/14 [==============================] - 8s 540ms/step - loss: 0.3997 - acc: 0.8537 - val_loss: 0.4988 - val_acc: 0.7643\n",
      "Epoch 9/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3744 - acc: 0.8678\n",
      "Epoch 00009: val_acc improved from 0.76430 to 0.77002, saving model to best_model.h5\n",
      "14/14 [==============================] - 10s 738ms/step - loss: 0.3744 - acc: 0.8678 - val_loss: 0.4843 - val_acc: 0.7700\n",
      "Epoch 10/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3683 - acc: 0.8532\n",
      "Epoch 00010: val_acc did not improve from 0.77002\n",
      "14/14 [==============================] - 9s 636ms/step - loss: 0.3683 - acc: 0.8532 - val_loss: 0.7507 - val_acc: 0.4359\n",
      "Epoch 11/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5283 - acc: 0.7032\n",
      "Epoch 00011: val_acc did not improve from 0.77002\n",
      "14/14 [==============================] - 12s 828ms/step - loss: 0.5283 - acc: 0.7032 - val_loss: 0.6287 - val_acc: 0.6087\n",
      "Epoch 12/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4531 - acc: 0.8371\n",
      "Epoch 00012: val_acc did not improve from 0.77002\n",
      "14/14 [==============================] - 8s 603ms/step - loss: 0.4531 - acc: 0.8371 - val_loss: 0.5854 - val_acc: 0.7288\n",
      "Epoch 13/15\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4053 - acc: 0.8892\n",
      "Epoch 00013: val_acc did not improve from 0.77002\n",
      "14/14 [==============================] - 12s 827ms/step - loss: 0.4053 - acc: 0.8892 - val_loss: 0.5524 - val_acc: 0.7574\n",
      "Epoch 00013: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T03:29:51.189772Z",
     "start_time": "2021-04-23T03:29:51.178803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>모델명</th>\n",
       "      <th>test 정확도</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>0.642614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.829670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM_2layer</td>\n",
       "      <td>0.640797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bi-LSTM</td>\n",
       "      <td>0.640797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bi-LSTM-2</td>\n",
       "      <td>0.640797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1D-CNN</td>\n",
       "      <td>0.843407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           모델명  test 정확도\n",
       "0          DNN  0.642614\n",
       "1         LSTM  0.829670\n",
       "2  LSTM_2layer  0.640797\n",
       "3      Bi-LSTM  0.640797\n",
       "4    Bi-LSTM-2  0.640797\n",
       "5       1D-CNN  0.843407"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(test_result,columns=['모델명','test 정확도'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T18:03:00.200020Z",
     "start_time": "2021-04-29T18:03:00.182070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>모델명</th>\n",
       "      <th>test 정확도</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN</td>\n",
       "      <td>0.642516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.810440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM_2layer</td>\n",
       "      <td>0.640797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bi-LSTM</td>\n",
       "      <td>0.640797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bi-LSTM-2</td>\n",
       "      <td>0.640797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1D-CNN</td>\n",
       "      <td>0.806319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           모델명  test 정확도\n",
       "0          DNN  0.642516\n",
       "1         LSTM  0.810440\n",
       "2  LSTM_2layer  0.640797\n",
       "3      Bi-LSTM  0.640797\n",
       "4    Bi-LSTM-2  0.640797\n",
       "5       1D-CNN  0.806319"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(test_result,columns=['모델명','test 정확도'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T16:17:19.280259Z",
     "start_time": "2021-03-31T16:17:15.924231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 9ms/step - loss: 0.4627 - acc: 0.8098\n",
      "테스트 정확도: 0.8098\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T16:18:19.626552Z",
     "start_time": "2021-03-31T16:18:19.607608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  124,   54,    8],\n",
       "       [   0,    0,    0, ...,    0,    0,  394],\n",
       "       [   0,    0,    0, ...,   60,   60,   60],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 3999,  714, 1609],\n",
       "       [   0,    0,    0, ..., 1177,  298,  459],\n",
       "       [   0,    0,    0, ...,    0,  294,  101]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T16:18:27.302028Z",
     "start_time": "2021-03-31T16:18:25.635484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22834557],\n",
       "       [0.15960008],\n",
       "       [0.05404943],\n",
       "       ...,\n",
       "       [0.43997997],\n",
       "       [0.09528944],\n",
       "       [0.3143381 ]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = loaded_model.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T16:18:43.744856Z",
     "start_time": "2021-03-31T16:18:43.695961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86       933\n",
      "           1       0.81      0.62      0.70       523\n",
      "\n",
      "    accuracy                           0.81      1456\n",
      "   macro avg       0.81      0.77      0.78      1456\n",
      "weighted avg       0.81      0.81      0.80      1456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, np.round(preds,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T16:19:00.382019Z",
     "start_time": "2021-03-31T16:19:00.329160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8097527472527473\n",
      "Precision:  0.8075\n",
      "Recall:  0.6175908221797323\n",
      "Specificity:  0.917470525187567\n",
      "F1-Score:  0.6998916576381364\n",
      "F2-Score:  0.6480738362760834\n",
      "auc score:  0.7675306736836496\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def model_evaluation(label, predict):\n",
    "    cf_matrix = confusion_matrix(label, predict)\n",
    "    Accuracy = (cf_matrix[0][0] + cf_matrix[1][1]) / sum(sum(cf_matrix))\n",
    "    Precision = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[0][1])\n",
    "    Recall = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[1][0])\n",
    "    Specificity = cf_matrix[0][0] / (cf_matrix[0][0] + cf_matrix[0][1])\n",
    "    F1_Score = (2 * Recall * Precision) / (Recall + Precision)\n",
    "    F2_Score = (5 * Recall * Precision) / (Recall + 4*Precision)\n",
    "    \n",
    "    print(\"Accuracy: \", Accuracy)\n",
    "    print(\"Precision: \", Precision)\n",
    "    print(\"Recall: \", Recall)\n",
    "    print(\"Specificity: \", Specificity)\n",
    "    print(\"F1-Score: \", F1_Score)\n",
    "    print(\"F2-Score: \", F2_Score)\n",
    "    print(\"auc score: \" , roc_auc_score(label, np.round(predict,0)))\n",
    "model_evaluation(y_test, np.round(preds,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 리뷰 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "  new_sentence = mecab.morphs(new_sentence) # 토큰화\n",
    "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "  score = float(loaded_model.predict(pad_new)) # 예측\n",
    "  if(score > 0.5):\n",
    "    print(\"{:.2f}% 확률로 욕설에 가깝습니다.\".format(score * 100))\n",
    "  else:\n",
    "    print(\"{:.2f}% 확률로 욕설이 아닙니다.\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.09% 확률로 욕설이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('노잼 ..완전 재미 없음 ㅉㅉ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.34% 확률로 욕설에 가깝습니다.\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('병신ㅉㅉ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
