{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:26.599393Z",
     "start_time": "2021-04-14T23:41:25.491339Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train= pd.read_csv('korean-hate-speech/labeled/train.tsv' ,sep='\\t')\n",
    "dev= pd.read_csv('korean-hate-speech/labeled/dev.tsv' ,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:14:06.703671Z",
     "start_time": "2021-04-09T04:14:06.674751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>contain_gender_bias</th>\n",
       "      <th>bias</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(현재 호텔주인 심정) 아18 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속...</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...못된 넘들...남의 고통을 즐겼던 넘들..이젠 마땅한 처벌을 받아야지..,그래...</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1. 사람 얼굴 손톱으로 긁은것은 인격살해이고2. 동영상이 몰카냐? 메걸리안들 생각...</td>\n",
       "      <td>True</td>\n",
       "      <td>gender</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>힘내세요~ 응원합니다!!</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7892</th>\n",
       "      <td>힘내세요~~삼가 고인의 명복을 빕니다..</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7893</th>\n",
       "      <td>힘내세용 ^^ 항상 응원합니닷 ^^ !</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7894</th>\n",
       "      <td>힘내소...연기로 답해요.나도 53살 인데 이런일 저런일 다 있더라구요.인격을 믿습...</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7895</th>\n",
       "      <td>힘들면 관뒀어야지 그게 현명한거다</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7896 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comments  contain_gender_bias  \\\n",
       "0     (현재 호텔주인 심정) 아18 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속...                False   \n",
       "1     ....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...                False   \n",
       "2     ...못된 넘들...남의 고통을 즐겼던 넘들..이젠 마땅한 처벌을 받아야지..,그래...                False   \n",
       "3                    1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데                False   \n",
       "4     1. 사람 얼굴 손톱으로 긁은것은 인격살해이고2. 동영상이 몰카냐? 메걸리안들 생각...                 True   \n",
       "...                                                 ...                  ...   \n",
       "7891                                      힘내세요~ 응원합니다!!                False   \n",
       "7892                             힘내세요~~삼가 고인의 명복을 빕니다..                False   \n",
       "7893                              힘내세용 ^^ 항상 응원합니닷 ^^ !                False   \n",
       "7894  힘내소...연기로 답해요.나도 53살 인데 이런일 저런일 다 있더라구요.인격을 믿습...                False   \n",
       "7895                                 힘들면 관뒀어야지 그게 현명한거다                False   \n",
       "\n",
       "        bias  hate  \n",
       "0     others  hate  \n",
       "1       none  none  \n",
       "2       none  hate  \n",
       "3       none  none  \n",
       "4     gender  hate  \n",
       "...      ...   ...  \n",
       "7891    none  none  \n",
       "7892    none  none  \n",
       "7893    none  none  \n",
       "7894    none  none  \n",
       "7895    none  none  \n",
       "\n",
       "[7896 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 독립변수 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:27.696536Z",
     "start_time": "2021-04-14T23:41:27.669044Z"
    }
   },
   "outputs": [],
   "source": [
    "train = train[['comments','hate']]\n",
    "train['hate'] = train['hate'].map({'none':0,'offensive':1,'hate':2})\n",
    "dev = dev[['comments','hate']]\n",
    "dev['hate'] = dev['hate'].map({'none':0,'offensive':1,'hate':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:36.436272Z",
     "start_time": "2021-04-14T23:41:28.085504Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:36.451224Z",
     "start_time": "2021-04-14T23:41:36.438257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 리뷰의 개수 : 7896\n",
      "테스트용 리뷰의 개수 : 471\n"
     ]
    }
   ],
   "source": [
    "print('훈련용 리뷰의 개수 :', len(train))\n",
    "print('테스트용 리뷰의 개수 :', len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:36.645776Z",
     "start_time": "2021-04-14T23:41:36.453219Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b643129220>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['hate'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불용어 제거 후 토큰화 작업\n",
    "\n",
    "- ( , ) 도 추가\n",
    "- ....은 의미 있을 수도 있어 제거 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:36.660736Z",
     "start_time": "2021-04-14T23:41:36.647771Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = ['(',')','도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '만', '음', '면']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:37.842607Z",
     "start_time": "2021-04-14T23:41:36.662733Z"
    }
   },
   "outputs": [],
   "source": [
    "mecab = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "\n",
    "train['tokenized'] = train['comments'].apply(mecab.morphs)\n",
    "train['tokenized'] = train['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "dev['tokenized'] = dev['comments'].apply(mecab.morphs)\n",
    "dev['tokenized'] = dev['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 및 길이 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:37.902571Z",
     "start_time": "2021-04-14T23:41:37.843573Z"
    }
   },
   "outputs": [],
   "source": [
    "none_words = np.hstack(train[train.hate == 0]['tokenized'].values)\n",
    "offensive_words = np.hstack(train[train.hate == 1]['tokenized'].values)\n",
    "hate_words = np.hstack(train[train.hate == 2]['tokenized'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:37.932350Z",
     "start_time": "2021-04-14T23:41:37.904410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 2634), ('?', 561), ('거', 431), ('..', 421), ('보', 400), ('있', 391), ('잘', 387), ('좋', 376), ('!', 358), ('는데', 352), ('나', 332), ('안', 320), ('같', 257), ('아', 254), ('너무', 252), ('없', 247), ('겠', 247), ('사람', 237), ('했', 235), ('~', 235)]\n"
     ]
    }
   ],
   "source": [
    "none_words_count = Counter(none_words)\n",
    "print(none_words_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:37.977489Z",
     "start_time": "2021-04-14T23:41:37.934331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 2080), ('?', 830), ('거', 426), ('..', 351), ('나', 344), ('안', 342), ('있', 287), ('냐', 282), ('보', 275), ('없', 274), ('ㅋㅋ', 259), ('아', 232), ('어', 220), ('는데', 211), ('ㅋㅋㅋ', 211), ('사람', 205), ('여자', 204), ('로', 200), ('왜', 194), ('니', 192)]\n"
     ]
    }
   ],
   "source": [
    "offensive_words_count = Counter(offensive_words)\n",
    "print(offensive_words_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:38.007136Z",
     "start_time": "2021-04-14T23:41:37.980209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 1456), ('?', 607), ('거', 325), ('ㅋㅋ', 319), ('여자', 298), ('ㅋㅋㅋ', 288), ('냐', 272), ('나', 264), ('안', 244), ('있', 240), ('..', 227), ('보', 223), ('아', 221), ('니', 190), (',', 186), ('어', 176), ('남자', 176), ('없', 174), ('겠', 154), ('같', 153)]\n"
     ]
    }
   ],
   "source": [
    "hate_words_count = Counter(hate_words)\n",
    "print(hate_words_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:38.037087Z",
     "start_time": "2021-04-14T23:41:38.008135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8       14\n",
       "9       22\n",
       "13      13\n",
       "15      20\n",
       "23       9\n",
       "        ..\n",
       "7859    53\n",
       "7862     5\n",
       "7872     6\n",
       "7877    13\n",
       "7882    12\n",
       "Name: tokenized, Length: 2499, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['hate']==1]['tokenized'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:38.396094Z",
     "start_time": "2021-04-14T23:41:38.038085Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none 리뷰의 평균 길이 : 13.952094090648307\n",
      "offensive 리뷰의 평균 길이 : 15.990396158463385\n",
      "hate 리뷰의 평균 길이 : 18.14965986394558\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(10,5))\n",
    "text_len = train[train['hate']==0]['tokenized'].map(lambda x: len(x))\n",
    "ax1.hist(text_len, color='blue')\n",
    "ax1.set_title('none label Reviews')\n",
    "ax1.set_xlabel('length of samples')\n",
    "ax1.set_ylabel('number of samples')\n",
    "print('none 리뷰의 평균 길이 :', np.mean(text_len))\n",
    "\n",
    "text_len = train[train['hate']==1]['tokenized'].map(lambda x: len(x))\n",
    "ax2.hist(text_len, color='orange')\n",
    "ax2.set_title('offensive label Reviews')\n",
    "fig.suptitle('Words in texts')\n",
    "ax2.set_xlabel('length of samples')\n",
    "ax2.set_ylabel('number of samples')\n",
    "print('offensive 리뷰의 평균 길이 :', np.mean(text_len))\n",
    "\n",
    "text_len = train[train['hate']==2]['tokenized'].map(lambda x: len(x))\n",
    "ax3.hist(text_len, color='red')\n",
    "ax3.set_title('hate label Reviews')\n",
    "fig.suptitle('Words in texts')\n",
    "ax3.set_xlabel('length of samples')\n",
    "ax3.set_ylabel('number of samples')\n",
    "print('hate 리뷰의 평균 길이 :', np.mean(text_len))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:38.411055Z",
     "start_time": "2021-04-14T23:41:38.397093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 리뷰의 개수 : 5922\n",
      "테스트용 리뷰의 개수 : 1974\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(train, test_size = 0.25, random_state = 42)\n",
    "print('훈련용 리뷰의 개수 :', len(train_data))\n",
    "print('테스트용 리뷰의 개수 :', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:38.440974Z",
     "start_time": "2021-04-14T23:41:38.413055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2635\n",
       "1    1859\n",
       "2    1428\n",
       "Name: hate, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.hate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:38.470897Z",
     "start_time": "2021-04-14T23:41:38.442970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    851\n",
       "1    640\n",
       "2    483\n",
       "Name: hate, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.hate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:38.485887Z",
     "start_time": "2021-04-14T23:41:38.472891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "dataset_y = to_categorical(train_data['hate'])\n",
    "dataset_y = np.array(dataset_y, dtype=np.int32)\n",
    "print(dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:38.515777Z",
     "start_time": "2021-04-14T23:41:38.487850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " ...\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "test_y = to_categorical(test_data['hate'])\n",
    "test_y = np.array(test_y, dtype=np.int32)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:38.530735Z",
     "start_time": "2021-04-14T23:41:38.517784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5922,), (5922, 3), (1974,), (1974, 3))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_data['tokenized'].values\n",
    "y_train = dataset_y\n",
    "X_test= test_data['tokenized'].values\n",
    "y_test = test_y\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:41.602510Z",
     "start_time": "2021-04-14T23:41:40.513136Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "loaded_model = gensim.models.Word2Vec.load(\"aihub_review_6.model\") # 모델 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:42.022104Z",
     "start_time": "2021-04-14T23:41:41.960271Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-cfb6e38cd13b>:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  loaded_model.similar_by_word(\"ㅆㅂ\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('시바', 0.8410485982894897),\n",
       " ('시발', 0.8202505111694336),\n",
       " ('ㅆ발', 0.7903104424476624),\n",
       " ('ㅆㅍ', 0.7833770513534546),\n",
       " ('아오', 0.782518208026886),\n",
       " ('슈바', 0.7626177072525024),\n",
       " ('시밤', 0.7533204555511475),\n",
       " ('젠장', 0.7490739226341248),\n",
       " ('어휴', 0.7402772903442383),\n",
       " ('ㅆㅃ', 0.7374851703643799)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.similar_by_word(\"ㅆㅂ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:44:15.931304Z",
     "start_time": "2021-04-14T23:44:15.926319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어의 수 :  87507\n"
     ]
    }
   ],
   "source": [
    "print(\"단어의 수 : \", len(loaded_model.wv.vocab))\n",
    "vocab_len = len(loaded_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:43.385491Z",
     "start_time": "2021-04-14T23:41:43.380472Z"
    }
   },
   "outputs": [],
   "source": [
    "def embedding(word):\n",
    "    if word in loaded_model.wv.vocab:\n",
    "        return loaded_model.wv[word]\n",
    "    else:\n",
    "        return np.random.normal(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:44.644353Z",
     "start_time": "2021-04-14T23:41:44.349911Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = [[embedding(word) for word in sentence] for sentence in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:45.394119Z",
     "start_time": "2021-04-14T23:41:45.380156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3205704  -0.29223534  0.28166977  0.19160533  0.25994918 -0.16052312\n",
      "  0.05649608 -0.24007043  0.5995519  -0.3636819  -0.3441387  -0.02996193\n",
      "  0.06250454  0.25497457  0.1653577  -0.18234226 -0.03239172 -0.0298142\n",
      "  0.23375997  0.59823406 -0.0016627   0.2772874  -0.23550123 -0.31420958\n",
      "  0.15515238 -0.77207947  0.09017662  0.28241655 -0.5808407   0.52307254\n",
      " -0.11540357 -0.09884845  0.34062558 -0.35937607 -0.16666664  0.29611918\n",
      " -0.20421973  0.23769344  0.02505805  0.30397066  0.21490121  0.13577458\n",
      " -0.23035082 -0.04642522 -0.16268203  0.41368815  0.24912463 -0.02056008\n",
      "  0.03675332 -0.24290226  0.42826453 -0.01529319  0.34062698  0.5212221\n",
      " -0.07689518  0.05357447  0.21594955  0.31536388 -0.18403806 -0.4769869\n",
      " -0.1996679   0.11920177 -0.10625325  0.50578874  0.02689878  0.08991355\n",
      "  0.411692    0.4209339  -0.50788397 -0.29566303 -0.01066168  0.21273248\n",
      " -0.18241528 -0.28116482  0.14358966 -0.04431742 -0.2796953  -0.22792146\n",
      " -0.25466937  0.4047509   0.16238016  0.20823391  0.24615094  0.25442454\n",
      " -0.20839918 -0.15073174  0.14402248  0.02000539 -0.18602116  0.0486574\n",
      " -0.2901863   0.31851366  0.0297474  -0.01084981  0.11762465  0.14998645\n",
      "  0.28770804  0.05415045  0.5345626  -0.16391584]\n",
      "[ 2.95080513e-01  5.50271422e-02 -3.56495082e-01 -1.34640947e-01\n",
      "  1.00204714e-01  1.79345906e-01 -6.45820349e-02 -7.25331187e-01\n",
      "  1.24817207e-01  4.45756644e-01  2.99675375e-01  3.58261257e-01\n",
      "  6.12041564e-04  1.28356129e-01  5.74806146e-02  2.86601722e-01\n",
      "  4.43542115e-02  4.61867034e-01 -3.24208736e-01 -1.04817569e-01\n",
      " -8.81366804e-02 -3.22201937e-01  3.27860713e-02 -1.47209525e-01\n",
      "  7.67986029e-02 -2.96544045e-01  4.82487231e-01  1.29405797e-01\n",
      "  1.53904483e-01  7.61559248e-01 -3.94392490e-01  3.09569240e-01\n",
      " -8.19751322e-02  1.08336553e-01  1.71656102e-01  4.52243418e-01\n",
      "  5.31943589e-02  3.29722375e-01 -1.85438707e-01 -2.08240718e-01\n",
      "  4.61531103e-01 -7.23922700e-02 -3.47793281e-01 -4.11697216e-02\n",
      "  5.84477305e-01  3.13591033e-01  1.50376499e-01  7.66072273e-01\n",
      " -4.88373011e-01  5.99976718e-01  1.32905632e-01 -5.01711786e-01\n",
      "  1.78103521e-01  7.02865198e-02  1.47708878e-01 -4.94118594e-02\n",
      "  2.47600511e-01  3.29588681e-01 -4.32207853e-01  2.88767904e-01\n",
      " -8.22587758e-02 -2.19414756e-01 -3.41848254e-01 -2.19609380e-01\n",
      "  2.00001746e-01  1.03371013e-02  1.63698912e-01  2.00183585e-01\n",
      "  2.59349775e-02  3.33481282e-02  5.48489429e-02 -5.29412553e-02\n",
      " -1.25842625e-02 -2.11371318e-01  2.07813993e-01 -3.47707838e-01\n",
      " -1.83960706e-01  3.62632543e-01  2.47085124e-01  5.00319451e-02\n",
      "  1.84841320e-01 -7.72268325e-02  6.36260509e-01 -5.38789630e-01\n",
      " -1.43685505e-01 -3.34200829e-01  2.77999550e-01 -9.08716470e-02\n",
      " -3.52274895e-01 -2.37103581e-01  5.22732101e-02 -1.05883732e-01\n",
      "  5.23610413e-02  2.90478077e-02  8.40739906e-02  1.57380342e-01\n",
      " -1.19069733e-01  2.39723682e-01  2.68305689e-02  1.85495261e-02]\n",
      "[-0.06975114 -0.02333212 -0.34242913 -0.65749645  0.44075182  0.38853037\n",
      " -0.24899201 -0.46536884  0.1578069   0.11884633 -0.14838232  0.401875\n",
      "  0.14021231  0.48074675 -0.35667738  0.234587   -0.40051636 -0.09643216\n",
      " -0.21059585 -0.5750946  -0.24882084 -0.5100791   0.73976314 -0.16107625\n",
      " -0.33262125 -0.06069643 -0.04932199  0.6116862  -0.18082425 -0.10272083\n",
      "  0.06100629 -0.31039166 -0.13246778 -0.2850177   0.67445856  0.02501617\n",
      "  0.5674535   0.6405443  -0.06226732 -0.39049715  0.32749626  0.24600294\n",
      " -0.34257054  0.08529352  0.6731009   0.2939127  -0.02276891  0.8297695\n",
      " -0.19117288  0.17173076  0.23881623 -0.03085208 -0.2527439   0.3834512\n",
      "  0.34629187 -0.10783894  0.3027891   0.386872   -0.28045714  0.75121284\n",
      " -0.17567544 -0.4100942  -0.4388937   0.30771267  0.44359002  0.04559035\n",
      " -0.25032303  0.17951797  0.08083501  0.03916267 -0.19332339  0.85996556\n",
      "  0.07159767 -0.18458518 -0.20131573  0.08652804  0.4197471   0.2678363\n",
      "  0.4345556  -0.24867684  0.59843636  0.4673022   0.05671656 -0.40612578\n",
      "  0.26558214 -0.36390546  0.54256415 -0.02074202  0.05019477 -0.18944143\n",
      " -0.01779123 -0.16821223 -0.52720606  0.39411542  0.17747054 -0.30381167\n",
      " -0.09343852 -0.6421261   0.4253305  -0.6278417 ]\n",
      "[ 0.34076023  0.28454542 -0.24458328 -0.3308674   0.04257682 -0.26397926\n",
      "  0.77578205 -0.74444383  0.49653918  0.28004402  0.00468957 -0.16223004\n",
      "  0.21321252 -0.50047076  0.16395567 -0.34992248 -0.24375688 -0.28597143\n",
      "  0.6898008   0.63852394  0.61869675  0.14896943 -0.15149356  0.35265476\n",
      "  0.2685176  -0.21388122  0.21953782  0.7498848  -0.02124586 -0.45772946\n",
      " -0.46654102  0.19651654 -0.33697227 -0.28606176 -0.1579786   0.2915489\n",
      " -0.15318143 -0.02882055  0.42040703  0.2547651  -0.32148278 -0.47465643\n",
      " -0.236467   -0.19821408  0.01254477  0.1892417   0.3023074   0.4347105\n",
      "  0.43673685 -0.15485607  0.2594133   0.07297409  0.20776153 -0.04904722\n",
      " -0.23354156  0.9240823   0.6400026   0.17492685 -0.00427334 -0.09075772\n",
      "  0.00208637  0.5981335  -0.05439617 -0.71517444 -0.02473894  0.08925829\n",
      "  0.25319442  0.01749724  0.02202251  0.12641512 -0.58804744  0.40654144\n",
      " -0.49336404  0.0881357  -0.15349522  0.07643446 -0.18118793  0.26524192\n",
      "  0.20900205 -0.48905024  0.5035591   0.6447514   0.19398557  0.0871266\n",
      "  0.2323167  -0.0292143  -0.26056436 -0.54832166 -0.21862236 -0.35587555\n",
      " -0.08152308  0.28545067  0.407971    0.17158638  0.40880862 -0.27292502\n",
      "  0.09206123 -0.6610888   0.2305466  -0.19563332]\n",
      "[-0.07302114  0.12613712 -0.13552083 -0.14180985  0.33983058 -0.3333636\n",
      " -0.19143605 -0.15088712 -0.05215897 -0.25686395  0.24432398 -0.1025637\n",
      " -0.01585292 -0.15610759  0.28032142  0.06454528 -0.12464628 -0.40653223\n",
      "  0.11441899  0.04344893 -0.16181417 -0.36239323 -0.4717277   0.03582774\n",
      "  0.14739457 -0.58583105  0.5413388   0.38118473 -0.0407617  -0.18054797\n",
      "  0.04758251  0.29295814 -0.30152154 -0.02129846 -0.14282043  0.26212874\n",
      "  0.10594476  0.19048737  0.47802827  0.37265047 -0.17499985  0.25905004\n",
      " -0.4874473   0.2568231   0.25097618  0.15111169  0.16268155  0.03562067\n",
      "  0.44559965  0.44989255 -0.01304498  0.42104578  0.32236922 -0.09929257\n",
      " -0.1985537   0.48442364 -0.02766601  0.17025253  0.14063007 -0.16295196\n",
      " -0.10728904 -0.283854    0.3606923   0.11755357  0.3569834   0.09445324\n",
      " -0.18099318  0.60162723 -0.3596928  -0.37861168  0.13056198  0.71449673\n",
      "  0.26518258  0.25300878 -0.17421646  0.2658667   0.38090017 -0.02874666\n",
      " -0.29783908 -0.2586791   0.74163306  0.5076941  -0.00392546 -0.25512257\n",
      " -0.02335305  0.01132108 -0.06209369 -0.18591924 -0.3714192  -0.41488495\n",
      " -0.28950855  0.47816452  0.42511547 -0.10862856  0.31675807  0.30719495\n",
      "  0.09835151 -0.33629707  0.190333   -0.00955087]\n",
      "[-4.87262942e-02 -1.00703418e-01  4.49291989e-03  2.86899447e-01\n",
      " -2.14356616e-01 -4.90565412e-02 -2.07406148e-01 -1.25060454e-01\n",
      "  1.04885474e-01  3.26902777e-01 -5.17850637e-01 -2.84478396e-01\n",
      " -2.34957650e-01  2.00505361e-01  3.22914064e-01  1.14291631e-01\n",
      "  2.38666952e-01  2.05876514e-01  2.08635211e-01  4.17071551e-01\n",
      " -8.19443837e-02 -3.02205950e-01 -3.21823061e-01  4.44751978e-01\n",
      " -6.97689429e-02 -4.90267873e-02 -4.30703871e-02  1.57519668e-01\n",
      " -5.23194313e-01  3.92210722e-01  1.69775158e-01  8.78866091e-02\n",
      " -2.56021500e-01 -1.80369154e-01 -2.15321943e-01  4.13629323e-01\n",
      "  4.87086698e-02  9.81743485e-02 -1.46599650e-01  4.76609528e-01\n",
      " -1.19674705e-01 -2.63634861e-01 -4.18523252e-01 -4.04319137e-01\n",
      "  1.15322359e-01  4.80825484e-01 -5.78089356e-01  3.25164932e-04\n",
      "  2.29120061e-01 -3.03711832e-01  1.80041507e-01 -4.31295596e-02\n",
      " -2.99804538e-01  1.30362049e-01 -5.47390580e-01  2.76132017e-01\n",
      "  4.79191273e-01  6.44455850e-02 -3.17774087e-01 -1.45612717e-01\n",
      "  1.09427169e-01 -1.97271675e-01 -2.68278211e-01  1.25525137e-02\n",
      "  7.86074400e-02 -3.12764108e-01 -3.40233147e-01  6.05459101e-02\n",
      " -8.11648145e-02  1.45764217e-01 -1.81886151e-01  2.69143492e-01\n",
      " -1.28174216e-01  4.10370499e-01 -7.47978464e-02  3.81485760e-01\n",
      "  3.28708321e-01  2.54453182e-01 -3.46397638e-01 -7.63757750e-02\n",
      "  5.09319715e-02  2.72777766e-01  1.42811373e-01 -1.26783803e-01\n",
      " -2.84357339e-01  2.66385108e-01  4.42805469e-01  4.18648235e-02\n",
      "  1.61723339e-03 -3.84632409e-01 -3.26663673e-01  1.53490305e-01\n",
      " -1.54574737e-01 -5.53541839e-01  4.03364748e-01 -3.99538390e-02\n",
      " -5.42005241e-01 -6.34753585e-01  2.63279825e-01  1.06822886e-01]\n"
     ]
    }
   ],
   "source": [
    "for i in X_train[0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:46.264357Z",
     "start_time": "2021-04-14T23:41:46.258372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.27485409e-01,  8.23980290e-03, -1.32144257e-01, -1.31051645e-01,\n",
       "        1.61492750e-01, -3.98410372e-02,  1.99769754e-02, -4.08526987e-01,\n",
       "        2.38573626e-01,  9.18339863e-02, -7.69471228e-02,  3.01503632e-02,\n",
       "        2.76218038e-02,  6.80007413e-02,  1.05558686e-01,  2.79601458e-02,\n",
       "       -8.63816813e-02, -2.51677409e-02,  1.18635066e-01,  1.69561043e-01,\n",
       "        6.05299696e-03, -1.78437233e-01, -6.79993853e-02,  3.51231880e-02,\n",
       "        4.09121662e-02, -3.29676509e-01,  2.06857994e-01,  3.85349631e-01,\n",
       "       -1.98827043e-01,  1.55974030e-01, -1.16328858e-01,  7.96150640e-02,\n",
       "       -1.28055438e-01, -1.70631111e-01,  2.72211749e-02,  2.90114284e-01,\n",
       "        6.96500242e-02,  2.44633555e-01,  8.81979465e-02,  1.34876311e-01,\n",
       "        6.46285340e-02, -2.83093359e-02, -3.43858689e-01, -5.80019243e-02,\n",
       "        2.45623246e-01,  3.07061791e-01,  4.39386368e-02,  3.40989679e-01,\n",
       "        7.81106651e-02,  8.66883099e-02,  2.04399362e-01, -1.61611233e-02,\n",
       "        8.27188045e-02,  1.59497008e-01, -9.37300548e-02,  2.63493598e-01,\n",
       "        3.09644520e-01,  2.40241602e-01, -1.79686740e-01,  2.72785723e-02,\n",
       "       -7.55629465e-02, -6.55498952e-02, -1.41496226e-01,  1.47061411e-03,\n",
       "        1.80223718e-01,  2.79807043e-03,  9.50599741e-03,  2.46717647e-01,\n",
       "       -1.36658192e-01, -5.49307652e-02, -1.31417960e-01,  4.01656419e-01,\n",
       "       -7.99595937e-02,  1.23989535e-02, -4.20702696e-02,  6.97149485e-02,\n",
       "        8.07519481e-02,  1.48915976e-01, -1.37721992e-03, -1.02999859e-01,\n",
       "        3.73630315e-01,  3.37255448e-01,  2.11999938e-01, -1.64211765e-01,\n",
       "       -2.69827042e-02, -1.00057699e-01,  1.80788934e-01, -1.30664065e-01,\n",
       "       -1.79420948e-01, -2.55546778e-01, -1.58899933e-01,  1.60253868e-01,\n",
       "        3.89023498e-02, -1.30451024e-02,  2.51350105e-01, -3.54798511e-04,\n",
       "       -4.60654497e-02, -3.30065221e-01,  2.78480500e-01, -1.45261541e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X_train[0]) / len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:47.401315Z",
     "start_time": "2021-04-14T23:41:47.306579Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_mean = [sum(words) / len(words) for words in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:26:48.245695Z",
     "start_time": "2021-04-15T02:26:48.222723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5922"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:27:51.869259Z",
     "start_time": "2021-04-15T02:27:51.812408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.60169819, -1.35790627, -2.70309756,  0.62644018, -0.21505753,\n",
       "        0.37314544,  1.13398024, -2.76249174,  1.39814319, -0.20613727,\n",
       "        0.21927151,  0.90779403, -0.14288599, -0.86072382,  0.44710582,\n",
       "       -0.32885719,  0.00382314, -0.54044941,  0.88373813,  0.76845042,\n",
       "        1.58945724, -1.1435182 ,  0.16304481, -0.28993426, -0.61137473,\n",
       "       -0.31594875,  0.11641879,  0.69396984, -0.51375132,  0.0170406 ,\n",
       "       -0.54840373,  0.59054524,  0.30753307, -0.63050704,  1.00998233,\n",
       "        0.90062594,  1.28733428,  0.31652857,  0.64758138, -0.61745448,\n",
       "        1.46075494,  0.17881359, -1.99898311, -0.02995837,  1.45511196,\n",
       "        1.47557797, -0.33405549,  1.32668226,  0.52339134,  0.19178768,\n",
       "        0.20866723,  0.03756713,  0.51475053,  0.75970625, -0.59429541,\n",
       "        1.339726  ,  0.79274892,  1.65310402, -0.57647777,  0.61775777,\n",
       "       -0.97694022,  0.48223161, -0.45398593, -0.62310321,  1.57022712,\n",
       "       -0.56163683, -0.34074321,  2.73926899,  0.42436562, -0.8285744 ,\n",
       "       -0.46062418,  1.9163659 , -0.43156049,  0.07264653, -0.232212  ,\n",
       "       -0.00334765,  0.90480696,  0.14116367,  0.3875135 , -1.0906053 ,\n",
       "        1.66484499,  2.84049498,  0.50324924, -2.26606966, -0.05377517,\n",
       "       -1.65624517, -0.32274231, -0.69001281,  0.17078401, -1.13157885,\n",
       "       -0.37141817,  1.19223765, -0.86872754, -0.27223917,  0.36762969,\n",
       "       -0.69185426, -0.30869145, -1.4414248 ,  0.40681091, -0.08417537])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X_scaled = preprocessing.scale(X_train_mean)\n",
    "X_scaled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:51.076868Z",
     "start_time": "2021-04-14T23:41:50.981125Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = [[embedding(word) for word in sentence] for sentence in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:41:53.656284Z",
     "start_time": "2021-04-14T23:41:53.617851Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_mean = [sum(words) / len(words) for words in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T23:42:18.308155Z",
     "start_time": "2021-04-14T23:42:18.120147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 100\n",
      "리뷰의 평균 길이 : 100.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(l) for l in X_train_mean))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train_mean))/len(X_train_mean))\n",
    "plt.hist([len(s) for s in X_train_mean], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:47:52.095782Z",
     "start_time": "2021-04-15T02:47:52.058881Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional, Dropout,BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(64,input_shape=(len(X_train_mean), 100),return_sequences=True)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(3, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:48:04.412515Z",
     "start_time": "2021-04-15T02:48:04.383591Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[0;32m   2349\u001b[0m     \"\"\"\n\u001b[0;32m   2350\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2351\u001b[1;33m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[0;32m   2352\u001b[0m                        \u001b[1;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2353\u001b[0m                        \u001b[1;34m'`fit()` with some data, or specify '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:49:35.135304Z",
     "start_time": "2021-04-15T02:49:34.850107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_42 (LSTM)               (None, 8)                 3488      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 3,539\n",
      "Trainable params: 3,539\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(8, input_shape=(len(X_train_mean), 100)),\n",
    "    tf.keras.layers.Dense(4),\n",
    "    tf.keras.layers.Dense(3)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "# Sequeatial Model \n",
    "model.add(LSTM(20, input_shape=(12, 1))) # (timestep, feature) \n",
    "model.add(Dense(1)) \n",
    "# output = 1\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T03:16:34.411841Z",
     "start_time": "2021-04-15T03:16:34.030864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, None, 64)          5600448   \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, None, 32)          12416     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 5,613,443\n",
      "Trainable params: 5,613,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, GlobalMaxPool1D, Dropout\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_len, output_dim=64))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T03:29:41.008590Z",
     "start_time": "2021-04-15T03:17:06.590164Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0979 - acc: 0.4125\n",
      "Epoch 00001: val_acc improved from -inf to 0.45000, saving model to best_model.h5\n",
      "2/2 [==============================] - 37s 18s/step - loss: 1.0979 - acc: 0.4125 - val_loss: 1.0974 - val_acc: 0.4500\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0976 - acc: 0.4250\n",
      "Epoch 00002: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0976 - acc: 0.4250 - val_loss: 1.0960 - val_acc: 0.4500\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0970 - acc: 0.4000\n",
      "Epoch 00003: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0970 - acc: 0.4000 - val_loss: 1.0950 - val_acc: 0.4500\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0963 - acc: 0.4000\n",
      "Epoch 00004: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0963 - acc: 0.4000 - val_loss: 1.0942 - val_acc: 0.4500\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0965 - acc: 0.4000\n",
      "Epoch 00005: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0965 - acc: 0.4000 - val_loss: 1.0934 - val_acc: 0.4500\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0942 - acc: 0.4000\n",
      "Epoch 00006: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0942 - acc: 0.4000 - val_loss: 1.0925 - val_acc: 0.4500\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0953 - acc: 0.4000\n",
      "Epoch 00007: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0953 - acc: 0.4000 - val_loss: 1.0915 - val_acc: 0.4500\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0937 - acc: 0.4000\n",
      "Epoch 00008: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0937 - acc: 0.4000 - val_loss: 1.0905 - val_acc: 0.4500\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0949 - acc: 0.4000\n",
      "Epoch 00009: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0949 - acc: 0.4000 - val_loss: 1.0897 - val_acc: 0.4500\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0935 - acc: 0.4000\n",
      "Epoch 00010: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0935 - acc: 0.4000 - val_loss: 1.0888 - val_acc: 0.4500\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0931 - acc: 0.4000\n",
      "Epoch 00011: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0931 - acc: 0.4000 - val_loss: 1.0881 - val_acc: 0.4500\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0940 - acc: 0.4000\n",
      "Epoch 00012: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0940 - acc: 0.4000 - val_loss: 1.0874 - val_acc: 0.4500\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0979 - acc: 0.4000\n",
      "Epoch 00013: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0979 - acc: 0.4000 - val_loss: 1.0869 - val_acc: 0.4500\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0917 - acc: 0.4000\n",
      "Epoch 00014: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0917 - acc: 0.4000 - val_loss: 1.0865 - val_acc: 0.4500\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0931 - acc: 0.4000\n",
      "Epoch 00015: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0931 - acc: 0.4000 - val_loss: 1.0860 - val_acc: 0.4500\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0928 - acc: 0.4000\n",
      "Epoch 00016: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0928 - acc: 0.4000 - val_loss: 1.0856 - val_acc: 0.4500\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0922 - acc: 0.4000\n",
      "Epoch 00017: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0922 - acc: 0.4000 - val_loss: 1.0851 - val_acc: 0.4500\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0905 - acc: 0.4000\n",
      "Epoch 00018: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0905 - acc: 0.4000 - val_loss: 1.0846 - val_acc: 0.4500\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0928 - acc: 0.4000\n",
      "Epoch 00019: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0928 - acc: 0.4000 - val_loss: 1.0840 - val_acc: 0.4500\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0914 - acc: 0.4000\n",
      "Epoch 00020: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0914 - acc: 0.4000 - val_loss: 1.0835 - val_acc: 0.4500\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0908 - acc: 0.4000\n",
      "Epoch 00021: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0908 - acc: 0.4000 - val_loss: 1.0830 - val_acc: 0.4500\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0922 - acc: 0.4000\n",
      "Epoch 00022: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0922 - acc: 0.4000 - val_loss: 1.0824 - val_acc: 0.4500\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0899 - acc: 0.4000\n",
      "Epoch 00023: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0899 - acc: 0.4000 - val_loss: 1.0818 - val_acc: 0.4500\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0906 - acc: 0.4000\n",
      "Epoch 00024: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0906 - acc: 0.4000 - val_loss: 1.0813 - val_acc: 0.4500\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0907 - acc: 0.4000\n",
      "Epoch 00025: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0907 - acc: 0.4000 - val_loss: 1.0808 - val_acc: 0.4500\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0880 - acc: 0.4000\n",
      "Epoch 00026: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0880 - acc: 0.4000 - val_loss: 1.0802 - val_acc: 0.4500\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0895 - acc: 0.4000\n",
      "Epoch 00027: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 4s/step - loss: 1.0895 - acc: 0.4000 - val_loss: 1.0797 - val_acc: 0.4500\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0893 - acc: 0.4000\n",
      "Epoch 00028: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 8s 4s/step - loss: 1.0893 - acc: 0.4000 - val_loss: 1.0793 - val_acc: 0.4500\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0869 - acc: 0.4000\n",
      "Epoch 00029: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0869 - acc: 0.4000 - val_loss: 1.0790 - val_acc: 0.4500\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0881 - acc: 0.4000\n",
      "Epoch 00030: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0881 - acc: 0.4000 - val_loss: 1.0785 - val_acc: 0.4500\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0900 - acc: 0.4000\n",
      "Epoch 00031: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0900 - acc: 0.4000 - val_loss: 1.0780 - val_acc: 0.4500\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0915 - acc: 0.4000\n",
      "Epoch 00032: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0915 - acc: 0.4000 - val_loss: 1.0776 - val_acc: 0.4500\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 1.0893 - acc: 0.4000\n",
      "Epoch 00033: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0893 - acc: 0.4000 - val_loss: 1.0773 - val_acc: 0.4500\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0905 - acc: 0.4000\n",
      "Epoch 00034: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0905 - acc: 0.4000 - val_loss: 1.0770 - val_acc: 0.4500\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0889 - acc: 0.4000\n",
      "Epoch 00035: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0889 - acc: 0.4000 - val_loss: 1.0766 - val_acc: 0.4500\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0882 - acc: 0.4000\n",
      "Epoch 00036: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0882 - acc: 0.4000 - val_loss: 1.0761 - val_acc: 0.4500\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0886 - acc: 0.4000\n",
      "Epoch 00037: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0886 - acc: 0.4000 - val_loss: 1.0755 - val_acc: 0.4500\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0921 - acc: 0.4000\n",
      "Epoch 00038: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0921 - acc: 0.4000 - val_loss: 1.0749 - val_acc: 0.4500\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0963 - acc: 0.4000\n",
      "Epoch 00039: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0963 - acc: 0.4000 - val_loss: 1.0744 - val_acc: 0.4500\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0851 - acc: 0.4000\n",
      "Epoch 00040: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0851 - acc: 0.4000 - val_loss: 1.0738 - val_acc: 0.4500\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0821 - acc: 0.4000\n",
      "Epoch 00041: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0821 - acc: 0.4000 - val_loss: 1.0731 - val_acc: 0.4500\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0867 - acc: 0.4000\n",
      "Epoch 00042: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0867 - acc: 0.4000 - val_loss: 1.0727 - val_acc: 0.4500\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0897 - acc: 0.4000\n",
      "Epoch 00043: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0897 - acc: 0.4000 - val_loss: 1.0726 - val_acc: 0.4500\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0929 - acc: 0.4000\n",
      "Epoch 00044: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0929 - acc: 0.4000 - val_loss: 1.0726 - val_acc: 0.4500\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0911 - acc: 0.4000\n",
      "Epoch 00045: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0911 - acc: 0.4000 - val_loss: 1.0727 - val_acc: 0.4500\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0847 - acc: 0.4000\n",
      "Epoch 00046: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0847 - acc: 0.4000 - val_loss: 1.0727 - val_acc: 0.4500\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0870 - acc: 0.4000\n",
      "Epoch 00047: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0870 - acc: 0.4000 - val_loss: 1.0728 - val_acc: 0.4500\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0938 - acc: 0.4000\n",
      "Epoch 00048: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0938 - acc: 0.4000 - val_loss: 1.0729 - val_acc: 0.4500\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0919 - acc: 0.4000\n",
      "Epoch 00049: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0919 - acc: 0.4000 - val_loss: 1.0728 - val_acc: 0.4500\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0872 - acc: 0.4000\n",
      "Epoch 00050: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0872 - acc: 0.4000 - val_loss: 1.0728 - val_acc: 0.4500\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0835 - acc: 0.4000\n",
      "Epoch 00051: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0835 - acc: 0.4000 - val_loss: 1.0727 - val_acc: 0.4500\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0985 - acc: 0.4000\n",
      "Epoch 00052: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0985 - acc: 0.4000 - val_loss: 1.0727 - val_acc: 0.4500\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0884 - acc: 0.4000\n",
      "Epoch 00053: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0884 - acc: 0.4000 - val_loss: 1.0727 - val_acc: 0.4500\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0841 - acc: 0.4000\n",
      "Epoch 00054: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0841 - acc: 0.4000 - val_loss: 1.0728 - val_acc: 0.4500\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0876 - acc: 0.4000\n",
      "Epoch 00055: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0876 - acc: 0.4000 - val_loss: 1.0729 - val_acc: 0.4500\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0881 - acc: 0.4000\n",
      "Epoch 00056: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0881 - acc: 0.4000 - val_loss: 1.0730 - val_acc: 0.4500\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0934 - acc: 0.4000\n",
      "Epoch 00057: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0934 - acc: 0.4000 - val_loss: 1.0730 - val_acc: 0.4500\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0862 - acc: 0.4000\n",
      "Epoch 00058: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0862 - acc: 0.4000 - val_loss: 1.0730 - val_acc: 0.4500\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0919 - acc: 0.4000\n",
      "Epoch 00059: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0919 - acc: 0.4000 - val_loss: 1.0732 - val_acc: 0.4500\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0924 - acc: 0.4000\n",
      "Epoch 00060: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0924 - acc: 0.4000 - val_loss: 1.0732 - val_acc: 0.4500\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0917 - acc: 0.4000\n",
      "Epoch 00061: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0917 - acc: 0.4000 - val_loss: 1.0733 - val_acc: 0.4500\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0875 - acc: 0.4000\n",
      "Epoch 00062: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0875 - acc: 0.4000 - val_loss: 1.0734 - val_acc: 0.4500\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0872 - acc: 0.4000\n",
      "Epoch 00063: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0872 - acc: 0.4000 - val_loss: 1.0735 - val_acc: 0.4500\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0884 - acc: 0.4000\n",
      "Epoch 00064: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0884 - acc: 0.4000 - val_loss: 1.0738 - val_acc: 0.4500\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 1.0961 - acc: 0.4000\n",
      "Epoch 00065: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0961 - acc: 0.4000 - val_loss: 1.0742 - val_acc: 0.4500\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0865 - acc: 0.4000\n",
      "Epoch 00066: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 4s/step - loss: 1.0865 - acc: 0.4000 - val_loss: 1.0746 - val_acc: 0.4500\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0888 - acc: 0.4000\n",
      "Epoch 00067: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0888 - acc: 0.4000 - val_loss: 1.0752 - val_acc: 0.4500\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0918 - acc: 0.4000\n",
      "Epoch 00068: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0918 - acc: 0.4000 - val_loss: 1.0757 - val_acc: 0.4500\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0839 - acc: 0.4000\n",
      "Epoch 00069: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0839 - acc: 0.4000 - val_loss: 1.0762 - val_acc: 0.4500\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0855 - acc: 0.4000\n",
      "Epoch 00070: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0855 - acc: 0.4000 - val_loss: 1.0763 - val_acc: 0.4500\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0869 - acc: 0.4000\n",
      "Epoch 00071: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0869 - acc: 0.4000 - val_loss: 1.0764 - val_acc: 0.4500\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0942 - acc: 0.4000\n",
      "Epoch 00072: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0942 - acc: 0.4000 - val_loss: 1.0765 - val_acc: 0.4500\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0930 - acc: 0.4000\n",
      "Epoch 00073: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0930 - acc: 0.4000 - val_loss: 1.0766 - val_acc: 0.4500\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0860 - acc: 0.4000\n",
      "Epoch 00074: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0860 - acc: 0.4000 - val_loss: 1.0766 - val_acc: 0.4500\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0857 - acc: 0.4000\n",
      "Epoch 00075: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0857 - acc: 0.4000 - val_loss: 1.0765 - val_acc: 0.4500\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0880 - acc: 0.4000\n",
      "Epoch 00076: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0880 - acc: 0.4000 - val_loss: 1.0764 - val_acc: 0.4500\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0918 - acc: 0.4000\n",
      "Epoch 00077: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0918 - acc: 0.4000 - val_loss: 1.0762 - val_acc: 0.4500\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0858 - acc: 0.4000\n",
      "Epoch 00078: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0858 - acc: 0.4000 - val_loss: 1.0762 - val_acc: 0.4500\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0903 - acc: 0.4000\n",
      "Epoch 00079: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0903 - acc: 0.4000 - val_loss: 1.0761 - val_acc: 0.4500\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0883 - acc: 0.4000\n",
      "Epoch 00080: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0883 - acc: 0.4000 - val_loss: 1.0761 - val_acc: 0.4500\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0882 - acc: 0.4000\n",
      "Epoch 00081: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0882 - acc: 0.4000 - val_loss: 1.0760 - val_acc: 0.4500\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0895 - acc: 0.4000\n",
      "Epoch 00082: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0895 - acc: 0.4000 - val_loss: 1.0759 - val_acc: 0.4500\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0904 - acc: 0.4000\n",
      "Epoch 00083: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0904 - acc: 0.4000 - val_loss: 1.0759 - val_acc: 0.4500\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0888 - acc: 0.4000\n",
      "Epoch 00084: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0888 - acc: 0.4000 - val_loss: 1.0760 - val_acc: 0.4500\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0895 - acc: 0.4000\n",
      "Epoch 00085: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0895 - acc: 0.4000 - val_loss: 1.0761 - val_acc: 0.4500\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0881 - acc: 0.4000\n",
      "Epoch 00086: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0881 - acc: 0.4000 - val_loss: 1.0762 - val_acc: 0.4500\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0886 - acc: 0.4000\n",
      "Epoch 00087: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0886 - acc: 0.4000 - val_loss: 1.0763 - val_acc: 0.4500\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0857 - acc: 0.4000\n",
      "Epoch 00088: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0857 - acc: 0.4000 - val_loss: 1.0763 - val_acc: 0.4500\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0951 - acc: 0.4000\n",
      "Epoch 00089: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0951 - acc: 0.4000 - val_loss: 1.0764 - val_acc: 0.4500\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0873 - acc: 0.4000\n",
      "Epoch 00090: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0873 - acc: 0.4000 - val_loss: 1.0765 - val_acc: 0.4500\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0853 - acc: 0.4000\n",
      "Epoch 00091: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0853 - acc: 0.4000 - val_loss: 1.0766 - val_acc: 0.4500\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0946 - acc: 0.4000\n",
      "Epoch 00092: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0946 - acc: 0.4000 - val_loss: 1.0767 - val_acc: 0.4500\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0907 - acc: 0.4000\n",
      "Epoch 00093: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0907 - acc: 0.4000 - val_loss: 1.0768 - val_acc: 0.4500\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0827 - acc: 0.4000\n",
      "Epoch 00094: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0827 - acc: 0.4000 - val_loss: 1.0769 - val_acc: 0.4500\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0931 - acc: 0.4000\n",
      "Epoch 00095: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0931 - acc: 0.4000 - val_loss: 1.0771 - val_acc: 0.4500\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0880 - acc: 0.4000\n",
      "Epoch 00096: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0880 - acc: 0.4000 - val_loss: 1.0774 - val_acc: 0.4500\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 1.0911 - acc: 0.4000\n",
      "Epoch 00097: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0911 - acc: 0.4000 - val_loss: 1.0775 - val_acc: 0.4500\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0905 - acc: 0.4000\n",
      "Epoch 00098: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0905 - acc: 0.4000 - val_loss: 1.0775 - val_acc: 0.4500\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0885 - acc: 0.4000\n",
      "Epoch 00099: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0885 - acc: 0.4000 - val_loss: 1.0774 - val_acc: 0.4500\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0922 - acc: 0.4000\n",
      "Epoch 00100: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0922 - acc: 0.4000 - val_loss: 1.0774 - val_acc: 0.4500\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-c40fdff1707b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"테스트 정확도: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train_mean, y_train, epochs=100, callbacks=[mc], batch_size=64, validation_split=0.2)\n",
    "\n",
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T00:02:02.278966Z",
     "start_time": "2021-04-15T00:02:02.184221Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(100, 256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(256, 3, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T00:02:02.730758Z",
     "start_time": "2021-04-15T00:02:02.720785Z"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T00:02:31.371045Z",
     "start_time": "2021-04-15T00:02:03.084811Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:372 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:385 call\n        return self._run_internal_graph(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py:247 call\n        outputs = self._convolution_op(inputs, self.kernel)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1010 convolution_v2\n        return convolution_internal(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1140 convolution_internal\n        return op(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:574 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:574 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1880 conv1d\n        result = gen_nn_ops.conv2d(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:975 conv2d\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:742 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:591 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3477 _create_op_internal\n        ret = Operation(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1974 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Negative dimension size caused by subtracting 3 from 1 for '{{node sequential_2/conv1d_1/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_2/conv1d_1/conv1d/ExpandDims, sequential_2/conv1d_1/conv1d/ExpandDims_1)' with input shapes: [?,1,1,256], [1,3,256,256].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-61daf753b9c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:372 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:385 call\n        return self._run_internal_graph(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py:247 call\n        outputs = self._convolution_op(inputs, self.kernel)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1010 convolution_v2\n        return convolution_internal(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1140 convolution_internal\n        return op(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:574 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:574 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1880 conv1d\n        result = gen_nn_ops.conv2d(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:975 conv2d\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:742 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:591 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3477 _create_op_internal\n        ret = Operation(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1974 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Negative dimension size caused by subtracting 3 from 1 for '{{node sequential_2/conv1d_1/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_2/conv1d_1/conv1d/ExpandDims, sequential_2/conv1d_1/conv1d/ExpandDims_1)' with input shapes: [?,1,1,256], [1,3,256,256].\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train_mean, y_train, epochs=100, callbacks=[mc], batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T05:01:02.345302Z",
     "start_time": "2021-04-09T05:01:01.086668Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100\n  y sizes: 1974\nPlease provide data which shares the same first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-448417521361>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"테스트 정확도: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1342\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m         \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1344\u001b[1;33m         data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1345\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[0;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100\n  y sizes: 1974\nPlease provide data which shares the same first dimension."
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test_mean, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:13:21.881003Z",
     "start_time": "2021-03-31T17:13:17.218412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39774117, 0.4180528 , 0.18420602],\n",
       "       [0.93720895, 0.05255724, 0.01023376],\n",
       "       [0.53255975, 0.3266842 , 0.14075604],\n",
       "       ...,\n",
       "       [0.27636996, 0.466064  , 0.2575661 ],\n",
       "       [0.2051393 , 0.4949639 , 0.29989678],\n",
       "       [0.39454758, 0.3854387 , 0.22001368]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = loaded_model.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 평가 참고 \n",
    "\n",
    "- https://ayoteralab.tistory.com/entry/Iris-dataset-classification-with-Keras?category=873956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T16:21:22.083815Z",
     "start_time": "2021-04-01T16:21:20.936885Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a74692dd2eda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:13:31.208117Z",
     "start_time": "2021-03-31T17:13:30.879991Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-2f3d6900dd4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scatter with Sepal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scatter with Petal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(14,6))\n",
    "ax[0].scatter(dataset.data[:,0], dataset.data[:,1], c=dataset.target)\n",
    "ax[0].set_title('scatter with Sepal')\n",
    "ax[1].scatter(dataset.data[:,2], dataset.data[:,3], c=dataset.target)\n",
    "ax[1].set_title('scatter with Petal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
