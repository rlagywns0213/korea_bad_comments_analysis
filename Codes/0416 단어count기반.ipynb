{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:36:40.880428Z",
     "start_time": "2021-04-16T05:36:38.192578Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train= pd.read_csv('korean-hate-speech/labeled/train.tsv' ,sep='\\t')\n",
    "dev= pd.read_csv('korean-hate-speech/labeled/dev.tsv' ,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:36:40.989138Z",
     "start_time": "2021-04-16T05:36:40.886413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>contain_gender_bias</th>\n",
       "      <th>bias</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(현재 호텔주인 심정) 아18 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속...</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...못된 넘들...남의 고통을 즐겼던 넘들..이젠 마땅한 처벌을 받아야지..,그래...</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1. 사람 얼굴 손톱으로 긁은것은 인격살해이고2. 동영상이 몰카냐? 메걸리안들 생각...</td>\n",
       "      <td>True</td>\n",
       "      <td>gender</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>힘내세요~ 응원합니다!!</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7892</th>\n",
       "      <td>힘내세요~~삼가 고인의 명복을 빕니다..</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7893</th>\n",
       "      <td>힘내세용 ^^ 항상 응원합니닷 ^^ !</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7894</th>\n",
       "      <td>힘내소...연기로 답해요.나도 53살 인데 이런일 저런일 다 있더라구요.인격을 믿습...</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7895</th>\n",
       "      <td>힘들면 관뒀어야지 그게 현명한거다</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7896 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comments  contain_gender_bias  \\\n",
       "0     (현재 호텔주인 심정) 아18 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속...                False   \n",
       "1     ....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...                False   \n",
       "2     ...못된 넘들...남의 고통을 즐겼던 넘들..이젠 마땅한 처벌을 받아야지..,그래...                False   \n",
       "3                    1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데                False   \n",
       "4     1. 사람 얼굴 손톱으로 긁은것은 인격살해이고2. 동영상이 몰카냐? 메걸리안들 생각...                 True   \n",
       "...                                                 ...                  ...   \n",
       "7891                                      힘내세요~ 응원합니다!!                False   \n",
       "7892                             힘내세요~~삼가 고인의 명복을 빕니다..                False   \n",
       "7893                              힘내세용 ^^ 항상 응원합니닷 ^^ !                False   \n",
       "7894  힘내소...연기로 답해요.나도 53살 인데 이런일 저런일 다 있더라구요.인격을 믿습...                False   \n",
       "7895                                 힘들면 관뒀어야지 그게 현명한거다                False   \n",
       "\n",
       "        bias  hate  \n",
       "0     others  hate  \n",
       "1       none  none  \n",
       "2       none  hate  \n",
       "3       none  none  \n",
       "4     gender  hate  \n",
       "...      ...   ...  \n",
       "7891    none  none  \n",
       "7892    none  none  \n",
       "7893    none  none  \n",
       "7894    none  none  \n",
       "7895    none  none  \n",
       "\n",
       "[7896 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 독립변수 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:36:41.049975Z",
     "start_time": "2021-04-16T05:36:40.997117Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-4a9ecf198709>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['hate'] = train['hate'].map({'none':0,'offensive':1,'hate':2})\n"
     ]
    }
   ],
   "source": [
    "train = train[['comments','hate']]\n",
    "train['hate'] = train['hate'].map({'none':0,'offensive':1,'hate':2})\n",
    "dev = dev[['comments','hate']]\n",
    "dev['hate'] = dev['hate'].map({'none':0,'offensive':1,'hate':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:37:02.384581Z",
     "start_time": "2021-04-16T05:36:41.064935Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:37:02.415498Z",
     "start_time": "2021-04-16T05:37:02.391560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 리뷰의 개수 : 7896\n",
      "테스트용 리뷰의 개수 : 471\n"
     ]
    }
   ],
   "source": [
    "print('훈련용 리뷰의 개수 :', len(train))\n",
    "print('테스트용 리뷰의 개수 :', len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:37:02.914163Z",
     "start_time": "2021-04-16T05:37:02.419486Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x287022a1520>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['hate'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불용어 제거 후 토큰화 작업\n",
    "\n",
    "- ( , ) 도 추가\n",
    "- ....은 의미 있을 수도 있어 제거 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:37:02.929123Z",
     "start_time": "2021-04-16T05:37:02.918152Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = ['(',')','도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '만', '음', '면']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:37:07.402683Z",
     "start_time": "2021-04-16T05:37:02.933113Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-2317ec7bde5f>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['tokenized'] = train['comments'].apply(mecab.morphs)\n",
      "<ipython-input-8-2317ec7bde5f>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['tokenized'] = train['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n"
     ]
    }
   ],
   "source": [
    "mecab = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "\n",
    "train['tokenized'] = train['comments'].apply(mecab.morphs)\n",
    "train['tokenized'] = train['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "dev['tokenized'] = dev['comments'].apply(mecab.morphs)\n",
    "dev['tokenized'] = dev['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:37:51.434252Z",
     "start_time": "2021-04-16T05:37:24.407739Z"
    }
   },
   "outputs": [],
   "source": [
    "all_token = train['tokenized'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:37:53.443613Z",
     "start_time": "2021-04-16T05:37:51.438237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123281\n",
      "13784\n",
      "[('.', 6170), ('?', 1998), ('거', 1182), ('..', 999), ('나', 940), ('있', 918), ('안', 906), ('보', 898), ('ㅋㅋ', 779), ('아', 707), ('없', 695), ('는데', 681), ('잘', 672), ('좋', 667), ('냐', 660), ('ㅋㅋㅋ', 645), ('!', 618), ('어', 606), ('같', 591), ('겠', 585), ('여자', 574), (',', 572), ('사람', 559), ('로', 532), ('아니', 532), ('왜', 520), ('니', 510), ('되', 494), ('했', 482), ('것', 473), ('으로', 468), ('해', 458), ('기', 457), ('좀', 451), ('말', 446), ('에서', 437), ('너무', 410), ('진짜', 405), ('~', 405), ('주', 381), ('야', 373), ('라', 370), ('었', 364), ('남자', 340), ('뭐', 331), ('못', 329), ('할', 329), ('살', 325), ('애', 313), ('더', 305), ('많', 297), ('세요', 291), ('않', 287), ('서', 286), ('때', 286), ('시', 285), ('네요', 279), ('으면', 277), ('적', 267), ('일', 265), ('씨', 262), ('알', 258), ('건', 256), ('그', 255), ('았', 251), ('연기', 250), ('수', 248), ('인데', 246), ('나오', 246), ('자', 245), ('지만', 241), ('그냥', 241), ('라고', 240), ('결혼', 240), ('내', 236), ('돈', 228), ('까지', 228), ('기사', 227), ('싶', 223), ('랑', 219), ('~~', 219), ('년', 217), ('드라마', 217), ('ㅋ', 215), ('먹', 215), ('...', 213), ('님', 210), ('얼굴', 209), ('다고', 208), ('연예인', 202), ('저', 200), ('생각', 195), ('남', 192), ('받', 189), ('던', 187), ('해서', 187), ('한테', 184), ('어요', 182), ('해라', 180), ('길', 179), ('참', 179), ('면서', 177), ('넘', 174), ('플', 172), ('마', 171), ('사', 171), ('싫', 170), ('이런', 170), ('개', 169), ('대', 168), ('하나', 168), ('이제', 167), ('악', 162), ('은데', 161), ('댓글', 161), ('이나', 160), ('ㄷ', 160), ('또', 159), ('보다', 157), ('난', 153), ('방송', 153), ('줄', 153), ('분', 152), ('던데', 152), ('둘', 151), ('배우', 151), ('부터', 149), ('다는', 149), ('맞', 148), ('근데', 148), ('이쁘', 145), ('정말', 144), ('모르', 143), ('함', 140), ('나이', 140), ('걸', 140), ('무슨', 139), ('많이', 137), ('ㅠㅠ', 134), ('봤', 132), ('정도', 130), ('전', 129), ('잘못', 129), ('중', 128), ('데', 127), ('1', 124), ('였', 124), ('그만', 124), ('지금', 123), ('응원', 122), ('는지', 120), ('처럼', 119), ('이랑', 119), ('합니다', 118), ('욕', 117), ('한국', 115), ('노', 115), ('니까', 115), ('팬', 114), ('그러', 113), ('사랑', 112), ('긴', 112), ('역시', 112), ('보이', 110), ('ㅎㅎ', 109), ('엄마', 108), ('이상', 108), ('잼', 108), ('습니다', 108), ('문제', 107), ('쓰', 107), ('2', 106), ('인생', 106), ('같이', 106), ('죠', 105), ('이렇게', 105), ('요즘', 104), ('죽', 104), ('어서', 104), ('해도', 104), ('얘', 104), ('번', 103), ('요', 103), ('든', 103), ('자기', 102), ('️', 101), ('너', 101), ('만나', 100), ('다른', 100), ('아서', 99), ('^^', 98), ('놈', 98), ('두', 98), ('더라', 98), ('된', 98), ('딸', 97), ('한다', 97), ('ㅠ', 96), ('에게', 96), ('찍', 95)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = nltk.Text(all_token, name='NMSC')\n",
    "\n",
    "# 전체 토큰의 개수\n",
    "print(len(text.tokens))\n",
    "\n",
    "# 중복을 제외한 토큰의 개수\n",
    "print(len(set(text.tokens)))            \n",
    "seq_len = len(set(text.tokens))\n",
    "# 출현 빈도가 높은 상위 토큰 10개\n",
    "print(text.vocab().most_common(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:38:37.088366Z",
     "start_time": "2021-04-16T05:38:37.051466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 리뷰의 개수 : 5922\n",
      "테스트용 리뷰의 개수 : 1974\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(train, test_size = 0.25, random_state = 42)\n",
    "print('훈련용 리뷰의 개수 :', len(train_data))\n",
    "print('테스트용 리뷰의 개수 :', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:38:39.562275Z",
     "start_time": "2021-04-16T05:38:39.539814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2635\n",
       "1    1859\n",
       "2    1428\n",
       "Name: hate, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.hate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:38:41.197613Z",
     "start_time": "2021-04-16T05:38:41.177638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    851\n",
       "1    640\n",
       "2    483\n",
       "Name: hate, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.hate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:38:42.834362Z",
     "start_time": "2021-04-16T05:38:42.827380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "dataset_y = to_categorical(train_data['hate'])\n",
    "dataset_y = np.array(dataset_y, dtype=np.int32)\n",
    "print(dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:38:43.642758Z",
     "start_time": "2021-04-16T05:38:43.626802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " ...\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "test_y = to_categorical(test_data['hate'])\n",
    "test_y = np.array(test_y, dtype=np.int32)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T06:52:51.042094Z",
     "start_time": "2021-04-16T06:52:51.033117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5922,), (5922, 3), (1974,), (1974, 3))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_data['tokenized'].values\n",
    "y_train = dataset_y\n",
    "X_test= test_data['tokenized'].values\n",
    "y_test = test_y\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T06:52:53.184419Z",
     "start_time": "2021-04-16T06:52:53.175929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[105, 29, 7, 248, 918, 82]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_count(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T06:52:53.651054Z",
     "start_time": "2021-04-16T06:52:53.642079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5781                                [이렇게, 재, 밋, 수, 있, 다니]\n",
       "439     [갠, 적, 으로, 이런, 스타일, 싸, 보인다, ., ...., 스카이, 캐슬, ...\n",
       "1379                                  [나, 보, 기, 불편, 함, ?]\n",
       "847     [그래도, 너, 죄책감, 부끄러워, 할, 줄, 아, 는구나, ., 송혜교, 측, 아...\n",
       "6794    [좋, 일, 셨, 는데, 왜, 이리, 찜찜, 할까, 나, 작정, 이미지, 세탁, 느...\n",
       "                              ...                        \n",
       "5226    [왜, 윤지, 성, 뜨, 거, 야, 나, 진짜, 맨날, 투표, 했, 는데, ㅠㅠ, ...\n",
       "5390    [원작, 이랑, 스토리, 너무, 똑같, 시, 놉, 가져오, 에피소드, 좀, 바꿔도,...\n",
       "860     [그래도, 잘, 생겼으니, 조금, 쉬, 나오, 이해, 해, 주, 것, ., ., 한...\n",
       "7603    [한지민, 씨, 코디, 제발, 좀, 바꾸, 세요, ., 엄마, 찾, 으러, 다니, ...\n",
       "7270    [크러시, 정용화, ,, 임시완, 급, 아닌데, 부산, 에서, 누가, 알아본다고, ㅋㅋ]\n",
       "Name: tokenized, Length: 5922, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T06:53:21.976830Z",
     "start_time": "2021-04-16T06:52:55.210886Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_words = [f[0] for f in text.vocab().most_common(seq_len)]\n",
    "\n",
    "def term_frequency(doc):\n",
    "    return [doc.count(word) for word in selected_words]\n",
    "\n",
    "train_x = [term_frequency(token) for token in train_data['tokenized']]\n",
    "# test_x = [term_frequency(token) for token in x_valid['token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T06:53:26.764861Z",
     "start_time": "2021-04-16T06:53:21.979653Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.array(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T06:53:26.779821Z",
     "start_time": "2021-04-16T06:53:26.765858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13784"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T06:53:26.794783Z",
     "start_time": "2021-04-16T06:53:26.781818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5922"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T06:53:26.824701Z",
     "start_time": "2021-04-16T06:53:26.797775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5922, 13784)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T06:53:27.124148Z",
     "start_time": "2021-04-16T06:53:26.826696Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:386 call\n        outputs = layer(inputs, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:216 call\n        output = tf_utils.smart_cond(training,\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:64 smart_cond\n        return smart_module.smart_cond(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\smart_cond.py:54 smart_cond\n        return true_fn()\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:210 dropped_inputs\n        return nn.dropout(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:507 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:4941 dropout\n        return dropout_v2(x, rate, noise_shape=noise_shape, seed=seed, name=name)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:5026 dropout_v2\n        raise ValueError(\"x has to be a floating point tensor since it's going \"\n\n    ValueError: x has to be a floating point tensor since it's going to be scaled. Got a <dtype: 'int32'> tensor instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-f959ce2b321d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mmc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# loaded_model = load_model('best_model.h5')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:386 call\n        outputs = layer(inputs, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:216 call\n        output = tf_utils.smart_cond(training,\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:64 smart_cond\n        return smart_module.smart_cond(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\smart_cond.py:54 smart_cond\n        return true_fn()\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:210 dropped_inputs\n        return nn.dropout(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:507 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:4941 dropout\n        return dropout_v2(x, rate, noise_shape=noise_shape, seed=seed, name=name)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:5026 dropout_v2\n        raise ValueError(\"x has to be a floating point tensor since it's going \"\n\n    ValueError: x has to be a floating point tensor since it's going to be scaled. Got a <dtype: 'int32'> tensor instead.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.layers import Dense, LSTM,Bidirectional ,Embedding, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "model = models.Sequential()\n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(13784,))) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(13784,))) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.Dense(64, activation='relu')) #ReLU 활성화함수 채택\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=100, callbacks=[mc], batch_size=64, validation_split=0.2)\n",
    "\n",
    "# loaded_model = load_model('best_model.h5')\n",
    "# print(\"테스트 정확도: %.4f\" % (model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T05:30:47.107779Z",
     "start_time": "2021-04-16T05:30:38.449480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 61, 200)           1127200   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 61, 256)           336896    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 1,866,659\n",
      "Trainable params: 1,866,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM,Bidirectional ,Embedding, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 200, input_length=max_len),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    Dropout(0.25),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(3,activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-16T05:31:39.552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.9232 - acc: 0.5402\n",
      "Epoch 00001: val_acc improved from -inf to 0.55781, saving model to best_model.h5\n",
      "75/75 [==============================] - 28s 372ms/step - loss: 0.9232 - acc: 0.5402 - val_loss: 0.9225 - val_acc: 0.5578\n",
      "Epoch 2/20\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.7003 - acc: 0.6840\n",
      "Epoch 00002: val_acc improved from 0.55781 to 0.57131, saving model to best_model.h5\n",
      "75/75 [==============================] - 31s 411ms/step - loss: 0.7003 - acc: 0.6840 - val_loss: 1.0277 - val_acc: 0.5713\n",
      "Epoch 3/20\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.5238 - acc: 0.7775\n",
      "Epoch 00003: val_acc did not improve from 0.57131\n",
      "75/75 [==============================] - 29s 392ms/step - loss: 0.5238 - acc: 0.7775 - val_loss: 1.1117 - val_acc: 0.5679\n",
      "Epoch 4/20\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.3302 - acc: 0.8712\n",
      "Epoch 00004: val_acc did not improve from 0.57131\n",
      "75/75 [==============================] - 31s 416ms/step - loss: 0.3302 - acc: 0.8712 - val_loss: 1.2986 - val_acc: 0.5544\n",
      "Epoch 5/20\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.1986 - acc: 0.9324\n",
      "Epoch 00005: val_acc did not improve from 0.57131\n",
      "75/75 [==============================] - 29s 388ms/step - loss: 0.1986 - acc: 0.9324 - val_loss: 1.7328 - val_acc: 0.5443\n",
      "Epoch 6/20\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.1384 - acc: 0.9514\n",
      "Epoch 00006: val_acc did not improve from 0.57131\n",
      "75/75 [==============================] - 28s 374ms/step - loss: 0.1384 - acc: 0.9514 - val_loss: 1.7561 - val_acc: 0.5527\n",
      "Epoch 7/20\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.1535 - acc: 0.9476\n",
      "Epoch 00007: val_acc did not improve from 0.57131\n",
      "75/75 [==============================] - 31s 410ms/step - loss: 0.1535 - acc: 0.9476 - val_loss: 1.9649 - val_acc: 0.5468\n",
      "Epoch 8/20\n",
      "39/75 [==============>...............] - ETA: 14s - loss: 0.0710 - acc: 0.9748"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['acc'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=64, callbacks=[es,mc],validation_split=0.2) #512개에 한 번씩 업데이터 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T14:47:37.903173Z",
     "start_time": "2021-04-11T14:47:37.717634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 61)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 61, 64)            360704    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 61, 64)            33024     \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 426,947\n",
      "Trainable params: 426,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, BatchNormalization\n",
    "import tensorflow as tf\n",
    "# 모델 구조 정의하기\n",
    "input = tf.keras.layers.Input(shape=(max_len,))\n",
    "net = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64)(input)\n",
    "net = tf.keras.layers.LSTM(units=64, return_sequences=True, activation='relu')(net) \n",
    "net = tf.keras.layers.LSTM(units=64, activation='relu')(net) \n",
    "net = tf.keras.layers.Dense(units=3, activation='softmax')(net)\n",
    "model = tf.keras.models.Model(inputs=input, outputs=net)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T14:47:05.816649Z",
     "start_time": "2021-04-11T14:45:05.641701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 1.0499 - acc: 0.4469\n",
      "Epoch 00001: val_acc improved from -inf to 0.44051, saving model to best_model.h5\n",
      "75/75 [==============================] - 4s 52ms/step - loss: 1.0499 - acc: 0.4469 - val_loss: 0.9972 - val_acc: 0.4405\n",
      "Epoch 2/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.8849 - acc: 0.5609- ETA: 1s - loss: 0.\n",
      "Epoch 00002: val_acc improved from 0.44051 to 0.54768, saving model to best_model.h5\n",
      "75/75 [==============================] - 4s 47ms/step - loss: 0.8849 - acc: 0.5609 - val_loss: 0.9304 - val_acc: 0.5477\n",
      "Epoch 3/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.8046 - acc: 0.6294\n",
      "Epoch 00003: val_acc improved from 0.54768 to 0.55274, saving model to best_model.h5\n",
      "75/75 [==============================] - 3s 46ms/step - loss: 0.8047 - acc: 0.6293 - val_loss: 0.9398 - val_acc: 0.5527\n",
      "Epoch 4/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.6460 - acc: 0.7178\n",
      "Epoch 00004: val_acc improved from 0.55274 to 0.56371, saving model to best_model.h5\n",
      "75/75 [==============================] - 4s 47ms/step - loss: 0.6460 - acc: 0.7178 - val_loss: 1.0888 - val_acc: 0.5637\n",
      "Epoch 5/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.5384 - acc: 0.7684\n",
      "Epoch 00005: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 3s 46ms/step - loss: 0.5384 - acc: 0.7684 - val_loss: 1.1958 - val_acc: 0.5443\n",
      "Epoch 6/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.4726 - acc: 0.8096\n",
      "Epoch 00006: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 4s 47ms/step - loss: 0.4726 - acc: 0.8096 - val_loss: 1.4773 - val_acc: 0.5460\n",
      "Epoch 7/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.4492 - acc: 0.8292\n",
      "Epoch 00007: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 3s 46ms/step - loss: 0.4492 - acc: 0.8292 - val_loss: 340.6476 - val_acc: 0.5384\n",
      "Epoch 8/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.4278 - acc: 0.8442\n",
      "Epoch 00008: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 4s 51ms/step - loss: 0.4277 - acc: 0.8442 - val_loss: 1.7014 - val_acc: 0.5232\n",
      "Epoch 9/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2957 - acc: 0.8917\n",
      "Epoch 00009: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 5s 73ms/step - loss: 0.2957 - acc: 0.8917 - val_loss: 1.8362 - val_acc: 0.4996\n",
      "Epoch 10/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.8965\n",
      "Epoch 00010: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 3s 45ms/step - loss: 0.2947 - acc: 0.8966 - val_loss: 2.7282 - val_acc: 0.5426\n",
      "Epoch 11/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2291 - acc: 0.9160\n",
      "Epoch 00011: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 3s 46ms/step - loss: 0.2291 - acc: 0.9160 - val_loss: 2.6458 - val_acc: 0.5409\n",
      "Epoch 12/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.1770 - acc: 0.9405\n",
      "Epoch 00012: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 6s 76ms/step - loss: 0.1770 - acc: 0.9405 - val_loss: 2.7393 - val_acc: 0.5266\n",
      "Epoch 13/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9559\n",
      "Epoch 00013: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 4s 49ms/step - loss: 0.1383 - acc: 0.9559 - val_loss: 2.7524 - val_acc: 0.5511\n",
      "Epoch 14/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9671\n",
      "Epoch 00014: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 4s 54ms/step - loss: 0.1077 - acc: 0.9671 - val_loss: 3.4844 - val_acc: 0.5300\n",
      "Epoch 15/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9742\n",
      "Epoch 00015: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 6s 81ms/step - loss: 0.0829 - acc: 0.9742 - val_loss: 3.8183 - val_acc: 0.5266\n",
      "Epoch 16/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0870 - acc: 0.9721\n",
      "Epoch 00016: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 4s 51ms/step - loss: 0.0870 - acc: 0.9721 - val_loss: 3.0980 - val_acc: 0.5190\n",
      "Epoch 17/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9749\n",
      "Epoch 00017: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 4s 48ms/step - loss: 0.0800 - acc: 0.9749 - val_loss: 3.5417 - val_acc: 0.5325\n",
      "Epoch 18/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0588 - acc: 0.9818\n",
      "Epoch 00018: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 5s 73ms/step - loss: 0.0588 - acc: 0.9818 - val_loss: 3.7654 - val_acc: 0.5207\n",
      "Epoch 19/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9884\n",
      "Epoch 00019: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 4s 56ms/step - loss: 0.0392 - acc: 0.9884 - val_loss: 4.1584 - val_acc: 0.5300\n",
      "Epoch 20/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0284 - acc: 0.9937\n",
      "Epoch 00020: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 4s 49ms/step - loss: 0.0284 - acc: 0.9937 - val_loss: 4.6379 - val_acc: 0.5300\n",
      "Epoch 21/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0343 - acc: 0.9913- ETA: 0s - loss: 0.\n",
      "Epoch 00021: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 5s 66ms/step - loss: 0.0343 - acc: 0.9913 - val_loss: 5.1622 - val_acc: 0.5367\n",
      "Epoch 22/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0627 - acc: 0.9806\n",
      "Epoch 00022: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 4s 57ms/step - loss: 0.0627 - acc: 0.9806 - val_loss: 3.9102 - val_acc: 0.5291\n",
      "Epoch 23/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9873\n",
      "Epoch 00023: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 3s 46ms/step - loss: 0.0434 - acc: 0.9873 - val_loss: 4.4131 - val_acc: 0.5241\n",
      "Epoch 24/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0286 - acc: 0.9916\n",
      "Epoch 00024: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 4s 57ms/step - loss: 0.0286 - acc: 0.9916 - val_loss: 4.0741 - val_acc: 0.5300\n",
      "Epoch 25/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9947\n",
      "Epoch 00025: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 5s 68ms/step - loss: 0.0223 - acc: 0.9947 - val_loss: 4.6105 - val_acc: 0.5105\n",
      "Epoch 26/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9960\n",
      "Epoch 00026: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 4s 47ms/step - loss: 0.0166 - acc: 0.9960 - val_loss: 4.8899 - val_acc: 0.5274\n",
      "Epoch 27/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0119 - acc: 0.9973\n",
      "Epoch 00027: val_acc did not improve from 0.56371\n",
      "75/75 [==============================] - 4s 49ms/step - loss: 0.0119 - acc: 0.9973 - val_loss: 5.2415 - val_acc: 0.5215\n",
      "Epoch 28/100\n",
      "72/75 [===========================>..] - ETA: 0s - loss: 0.0114 - acc: 0.9976"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-696936add0bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=100, callbacks=[mc], batch_size=64, validation_split=0.2)\n",
    "\n",
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T14:54:13.402822Z",
     "start_time": "2021-04-11T14:54:13.152490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 61, 64)            360768    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 61, 64)            256       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 61, 64)            33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 61, 64)            256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 427,523\n",
      "Trainable params: 427,267\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조 정의하기\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, 64, input_length=max_len))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64,return_sequences=True, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "          \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T15:07:20.066575Z",
     "start_time": "2021-04-11T14:54:15.011763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0770 - acc: 0.4326\n",
      "Epoch 00001: val_acc improved from -inf to 0.43713, saving model to best_model.h5\n",
      "75/75 [==============================] - 5s 64ms/step - loss: 1.0769 - acc: 0.4328 - val_loss: 1.0915 - val_acc: 0.4371\n",
      "Epoch 2/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.9251 - acc: 0.5486\n",
      "Epoch 00002: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 0.9249 - acc: 0.5487 - val_loss: 1.0938 - val_acc: 0.3857\n",
      "Epoch 3/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.7200 - acc: 0.6710\n",
      "Epoch 00003: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 0.7199 - acc: 0.6711 - val_loss: 1.0892 - val_acc: 0.4059\n",
      "Epoch 4/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.7625\n",
      "Epoch 00004: val_acc improved from 0.43713 to 0.49198, saving model to best_model.h5\n",
      "75/75 [==============================] - 6s 80ms/step - loss: 0.5512 - acc: 0.7625 - val_loss: 1.0528 - val_acc: 0.4920\n",
      "Epoch 5/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.4182 - acc: 0.8361\n",
      "Epoch 00005: val_acc did not improve from 0.49198\n",
      "75/75 [==============================] - 5s 66ms/step - loss: 0.4192 - acc: 0.8360 - val_loss: 1.0555 - val_acc: 0.4624\n",
      "Epoch 6/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.3696 - acc: 0.8587\n",
      "Epoch 00006: val_acc improved from 0.49198 to 0.50802, saving model to best_model.h5\n",
      "75/75 [==============================] - 4s 56ms/step - loss: 0.3695 - acc: 0.8588 - val_loss: 0.9888 - val_acc: 0.5080\n",
      "Epoch 7/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.2592 - acc: 0.9027\n",
      "Epoch 00007: val_acc improved from 0.50802 to 0.55021, saving model to best_model.h5\n",
      "75/75 [==============================] - 7s 94ms/step - loss: 0.2591 - acc: 0.9027 - val_loss: 0.9257 - val_acc: 0.5502\n",
      "Epoch 8/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9280\n",
      "Epoch 00008: val_acc did not improve from 0.55021\n",
      "75/75 [==============================] - 4s 57ms/step - loss: 0.1911 - acc: 0.9280 - val_loss: 1.0798 - val_acc: 0.5468\n",
      "Epoch 9/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.1476 - acc: 0.9457\n",
      "Epoch 00009: val_acc did not improve from 0.55021\n",
      "75/75 [==============================] - 6s 85ms/step - loss: 0.1476 - acc: 0.9457 - val_loss: 1.3547 - val_acc: 0.5494\n",
      "Epoch 10/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9417\n",
      "Epoch 00010: val_acc did not improve from 0.55021\n",
      "75/75 [==============================] - 5s 65ms/step - loss: 0.1547 - acc: 0.9415 - val_loss: 1.5739 - val_acc: 0.5308\n",
      "Epoch 11/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.1675 - acc: 0.9400\n",
      "Epoch 00011: val_acc did not improve from 0.55021\n",
      "75/75 [==============================] - 6s 74ms/step - loss: 0.1675 - acc: 0.9400 - val_loss: 1.9563 - val_acc: 0.5477\n",
      "Epoch 12/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9652\n",
      "Epoch 00012: val_acc did not improve from 0.55021\n",
      "75/75 [==============================] - 6s 76ms/step - loss: 0.0871 - acc: 0.9652 - val_loss: 2.5364 - val_acc: 0.5435\n",
      "Epoch 13/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0834 - acc: 0.9700\n",
      "Epoch 00013: val_acc improved from 0.55021 to 0.55274, saving model to best_model.h5\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 0.0834 - acc: 0.9700 - val_loss: 2.7860 - val_acc: 0.5527\n",
      "Epoch 14/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9715\n",
      "Epoch 00014: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 7s 91ms/step - loss: 0.0705 - acc: 0.9715 - val_loss: 3.0985 - val_acc: 0.5527\n",
      "Epoch 15/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9791\n",
      "Epoch 00015: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 4s 53ms/step - loss: 0.0636 - acc: 0.9791 - val_loss: 2.8385 - val_acc: 0.5435\n",
      "Epoch 16/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0581 - acc: 0.9795\n",
      "Epoch 00016: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 5s 71ms/step - loss: 0.0581 - acc: 0.9795 - val_loss: 3.1274 - val_acc: 0.5384\n",
      "Epoch 17/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9804\n",
      "Epoch 00017: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 6s 74ms/step - loss: 0.0598 - acc: 0.9804 - val_loss: 2.9511 - val_acc: 0.5409\n",
      "Epoch 18/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9806\n",
      "Epoch 00018: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 4s 56ms/step - loss: 0.0611 - acc: 0.9806 - val_loss: 2.8332 - val_acc: 0.5418\n",
      "Epoch 19/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9837\n",
      "Epoch 00019: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 7s 92ms/step - loss: 0.0518 - acc: 0.9837 - val_loss: 3.1100 - val_acc: 0.5519\n",
      "Epoch 20/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0428 - acc: 0.9844\n",
      "Epoch 00020: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 4s 56ms/step - loss: 0.0428 - acc: 0.9844 - val_loss: 3.4200 - val_acc: 0.5494\n",
      "Epoch 21/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9846\n",
      "Epoch 00021: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 6s 86ms/step - loss: 0.0433 - acc: 0.9846 - val_loss: 3.2674 - val_acc: 0.5409\n",
      "Epoch 22/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9869\n",
      "Epoch 00022: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 0.0440 - acc: 0.9869 - val_loss: 3.4057 - val_acc: 0.5443\n",
      "Epoch 23/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9905\n",
      "Epoch 00023: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 7s 89ms/step - loss: 0.0316 - acc: 0.9903 - val_loss: 3.3262 - val_acc: 0.5392\n",
      "Epoch 24/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9310\n",
      "Epoch 00024: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 7s 88ms/step - loss: 0.2110 - acc: 0.9310 - val_loss: 2.9125 - val_acc: 0.4886\n",
      "Epoch 25/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9261\n",
      "Epoch 00025: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 8s 112ms/step - loss: 0.2257 - acc: 0.9261 - val_loss: 2.3363 - val_acc: 0.5443\n",
      "Epoch 26/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9567\n",
      "Epoch 00026: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 9s 117ms/step - loss: 0.1199 - acc: 0.9567 - val_loss: 2.6143 - val_acc: 0.5426\n",
      "Epoch 27/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0859 - acc: 0.9707\n",
      "Epoch 00027: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 8s 105ms/step - loss: 0.0859 - acc: 0.9707 - val_loss: 3.0594 - val_acc: 0.5511\n",
      "Epoch 28/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9757\n",
      "Epoch 00028: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 7s 97ms/step - loss: 0.0706 - acc: 0.9757 - val_loss: 2.9743 - val_acc: 0.5325\n",
      "Epoch 29/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9791\n",
      "Epoch 00029: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 9s 116ms/step - loss: 0.0625 - acc: 0.9791 - val_loss: 2.7534 - val_acc: 0.5527\n",
      "Epoch 30/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9850\n",
      "Epoch 00030: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 9s 117ms/step - loss: 0.0467 - acc: 0.9850 - val_loss: 3.4148 - val_acc: 0.5451\n",
      "Epoch 31/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0346 - acc: 0.9871\n",
      "Epoch 00031: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 8s 104ms/step - loss: 0.0346 - acc: 0.9871 - val_loss: 3.3095 - val_acc: 0.5367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9890\n",
      "Epoch 00032: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 7s 95ms/step - loss: 0.0330 - acc: 0.9888 - val_loss: 3.9001 - val_acc: 0.5451\n",
      "Epoch 33/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9831\n",
      "Epoch 00033: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 9s 118ms/step - loss: 0.0569 - acc: 0.9831 - val_loss: 2.8612 - val_acc: 0.5460\n",
      "Epoch 34/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9886\n",
      "Epoch 00034: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 9s 118ms/step - loss: 0.0356 - acc: 0.9886 - val_loss: 3.2739 - val_acc: 0.5468\n",
      "Epoch 35/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0285 - acc: 0.9922\n",
      "Epoch 00035: val_acc did not improve from 0.55274\n",
      "75/75 [==============================] - 8s 109ms/step - loss: 0.0285 - acc: 0.9922 - val_loss: 3.3395 - val_acc: 0.5511\n",
      "Epoch 36/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9903- ETA: 0s - loss: 0.0312 - a\n",
      "Epoch 00036: val_acc improved from 0.55274 to 0.56118, saving model to best_model.h5\n",
      "75/75 [==============================] - 7s 93ms/step - loss: 0.0290 - acc: 0.9903 - val_loss: 4.0044 - val_acc: 0.5612\n",
      "Epoch 37/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9913\n",
      "Epoch 00037: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 118ms/step - loss: 0.0281 - acc: 0.9913 - val_loss: 3.7093 - val_acc: 0.5519\n",
      "Epoch 38/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9920\n",
      "Epoch 00038: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 117ms/step - loss: 0.0226 - acc: 0.9920 - val_loss: 3.6260 - val_acc: 0.5477\n",
      "Epoch 39/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0225 - acc: 0.9911\n",
      "Epoch 00039: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 8s 112ms/step - loss: 0.0225 - acc: 0.9911 - val_loss: 3.8171 - val_acc: 0.5586\n",
      "Epoch 40/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0203 - acc: 0.9947\n",
      "Epoch 00040: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 0.0203 - acc: 0.9947 - val_loss: 3.8047 - val_acc: 0.5477\n",
      "Epoch 41/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9947\n",
      "Epoch 00041: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 0.0161 - acc: 0.9947 - val_loss: 3.9706 - val_acc: 0.5494\n",
      "Epoch 42/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9947\n",
      "Epoch 00042: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 7s 89ms/step - loss: 0.0157 - acc: 0.9947 - val_loss: 3.7513 - val_acc: 0.5367\n",
      "Epoch 43/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0172 - acc: 0.9937\n",
      "Epoch 00043: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 7s 88ms/step - loss: 0.0172 - acc: 0.9937 - val_loss: 4.1423 - val_acc: 0.5443\n",
      "Epoch 44/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9962- \n",
      "Epoch 00044: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 114ms/step - loss: 0.0110 - acc: 0.9962 - val_loss: 4.3496 - val_acc: 0.5460\n",
      "Epoch 45/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9945\n",
      "Epoch 00045: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 8s 110ms/step - loss: 0.0155 - acc: 0.9945 - val_loss: 4.0907 - val_acc: 0.5384\n",
      "Epoch 46/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9949\n",
      "Epoch 00046: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 126ms/step - loss: 0.0168 - acc: 0.9949 - val_loss: 4.1518 - val_acc: 0.5342\n",
      "Epoch 47/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9951- ETA: 2s\n",
      "Epoch 00047: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 127ms/step - loss: 0.0195 - acc: 0.9951 - val_loss: 3.9746 - val_acc: 0.5342\n",
      "Epoch 48/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0175 - acc: 0.9939\n",
      "Epoch 00048: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 117ms/step - loss: 0.0175 - acc: 0.9939 - val_loss: 4.2220 - val_acc: 0.5477\n",
      "Epoch 49/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0192 - acc: 0.9943\n",
      "Epoch 00049: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 8s 111ms/step - loss: 0.0192 - acc: 0.9943 - val_loss: 3.9895 - val_acc: 0.5451\n",
      "Epoch 50/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9821\n",
      "Epoch 00050: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 114ms/step - loss: 0.0632 - acc: 0.9821 - val_loss: 3.5221 - val_acc: 0.5502\n",
      "Epoch 51/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0303 - acc: 0.9892\n",
      "Epoch 00051: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 134ms/step - loss: 0.0303 - acc: 0.9892 - val_loss: 3.4417 - val_acc: 0.5570\n",
      "Epoch 52/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9947\n",
      "Epoch 00052: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 135ms/step - loss: 0.0170 - acc: 0.9947 - val_loss: 3.7948 - val_acc: 0.5570\n",
      "Epoch 53/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9960\n",
      "Epoch 00053: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 131ms/step - loss: 0.0163 - acc: 0.9960 - val_loss: 4.0218 - val_acc: 0.5443\n",
      "Epoch 54/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9943\n",
      "Epoch 00054: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 129ms/step - loss: 0.0157 - acc: 0.9943 - val_loss: 3.8976 - val_acc: 0.5443\n",
      "Epoch 55/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0132 - acc: 0.9973\n",
      "Epoch 00055: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 127ms/step - loss: 0.0132 - acc: 0.9973 - val_loss: 3.9495 - val_acc: 0.5553\n",
      "Epoch 56/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0107 - acc: 0.9962\n",
      "Epoch 00056: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 117ms/step - loss: 0.0107 - acc: 0.9962 - val_loss: 4.3028 - val_acc: 0.5460\n",
      "Epoch 57/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0154 - acc: 0.9943\n",
      "Epoch 00057: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 8s 110ms/step - loss: 0.0154 - acc: 0.9943 - val_loss: 4.0537 - val_acc: 0.5460\n",
      "Epoch 58/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9962- ETA: 0s - loss: 0.0085 - acc:\n",
      "Epoch 00058: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 121ms/step - loss: 0.0092 - acc: 0.9962 - val_loss: 4.1993 - val_acc: 0.5451\n",
      "Epoch 59/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9970- ETA: 0s - loss: 0.0088 - acc:\n",
      "Epoch 00059: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 124ms/step - loss: 0.0092 - acc: 0.9970 - val_loss: 4.5536 - val_acc: 0.5485\n",
      "Epoch 60/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9960- ETA: 3s\n",
      "Epoch 00060: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 127ms/step - loss: 0.0121 - acc: 0.9960 - val_loss: 4.0781 - val_acc: 0.5468\n",
      "Epoch 61/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9909\n",
      "Epoch 00061: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 127ms/step - loss: 0.0287 - acc: 0.9909 - val_loss: 3.5415 - val_acc: 0.5367\n",
      "Epoch 62/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0167 - acc: 0.9947\n",
      "Epoch 00062: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 119ms/step - loss: 0.0167 - acc: 0.9947 - val_loss: 4.1455 - val_acc: 0.5392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0243 - acc: 0.9920\n",
      "Epoch 00063: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 8s 113ms/step - loss: 0.0243 - acc: 0.9920 - val_loss: 3.5673 - val_acc: 0.5418\n",
      "Epoch 64/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9941\n",
      "Epoch 00064: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 116ms/step - loss: 0.0185 - acc: 0.9941 - val_loss: 3.9010 - val_acc: 0.5359\n",
      "Epoch 65/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0135 - acc: 0.9951\n",
      "Epoch 00065: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 135ms/step - loss: 0.0135 - acc: 0.9951 - val_loss: 3.9472 - val_acc: 0.5451\n",
      "Epoch 66/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9962- ETA: 3s - l\n",
      "Epoch 00066: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 135ms/step - loss: 0.0108 - acc: 0.9962 - val_loss: 3.9476 - val_acc: 0.5451\n",
      "Epoch 67/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9985\n",
      "Epoch 00067: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 130ms/step - loss: 0.0085 - acc: 0.9985 - val_loss: 4.0027 - val_acc: 0.5367\n",
      "Epoch 68/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0140 - acc: 0.9958\n",
      "Epoch 00068: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 131ms/step - loss: 0.0140 - acc: 0.9958 - val_loss: 4.0861 - val_acc: 0.5401\n",
      "Epoch 69/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0108 - acc: 0.9970- ETA: 1s - loss: 0.0107\n",
      "Epoch 00069: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 125ms/step - loss: 0.0108 - acc: 0.9970 - val_loss: 4.1699 - val_acc: 0.5376\n",
      "Epoch 70/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0149 - acc: 0.9964\n",
      "Epoch 00070: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 116ms/step - loss: 0.0149 - acc: 0.9964 - val_loss: 4.0452 - val_acc: 0.5359\n",
      "Epoch 71/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9962- ETA\n",
      "Epoch 00071: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 114ms/step - loss: 0.0136 - acc: 0.9962 - val_loss: 3.8865 - val_acc: 0.5502\n",
      "Epoch 72/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9973- ETA: 1s - loss: 0.0099 - a\n",
      "Epoch 00072: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 126ms/step - loss: 0.0102 - acc: 0.9973 - val_loss: 4.0762 - val_acc: 0.5460\n",
      "Epoch 73/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9970\n",
      "Epoch 00073: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 127ms/step - loss: 0.0083 - acc: 0.9970 - val_loss: 4.0646 - val_acc: 0.5342\n",
      "Epoch 74/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9964\n",
      "Epoch 00074: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 133ms/step - loss: 0.0078 - acc: 0.9964 - val_loss: 4.2388 - val_acc: 0.5367\n",
      "Epoch 75/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0056 - acc: 0.9975- ETA: 0s - loss: 0.0057 - acc: 0.9\n",
      "Epoch 00075: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 11s 141ms/step - loss: 0.0056 - acc: 0.9975 - val_loss: 4.5057 - val_acc: 0.5435\n",
      "Epoch 76/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0075 - acc: 0.9977\n",
      "Epoch 00076: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 8s 113ms/step - loss: 0.0075 - acc: 0.9977 - val_loss: 4.5663 - val_acc: 0.5409\n",
      "Epoch 77/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0098 - acc: 0.9973\n",
      "Epoch 00077: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 8s 113ms/step - loss: 0.0098 - acc: 0.9973 - val_loss: 4.5776 - val_acc: 0.5401\n",
      "Epoch 78/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9981\n",
      "Epoch 00078: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 119ms/step - loss: 0.0067 - acc: 0.9981 - val_loss: 4.8314 - val_acc: 0.5485\n",
      "Epoch 79/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9977\n",
      "Epoch 00079: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 127ms/step - loss: 0.0092 - acc: 0.9977 - val_loss: 4.4392 - val_acc: 0.5418\n",
      "Epoch 80/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9981- ETA: 3s - loss: 0.0069 - acc: 0 - ETA: 3s - los\n",
      "Epoch 00080: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 131ms/step - loss: 0.0054 - acc: 0.9981 - val_loss: 4.5013 - val_acc: 0.5367\n",
      "Epoch 81/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9985\n",
      "Epoch 00081: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 133ms/step - loss: 0.0053 - acc: 0.9985 - val_loss: 4.4923 - val_acc: 0.5392\n",
      "Epoch 82/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0052 - acc: 0.9987\n",
      "Epoch 00082: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 140ms/step - loss: 0.0052 - acc: 0.9987 - val_loss: 4.6727 - val_acc: 0.5451\n",
      "Epoch 83/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0120 - acc: 0.9964\n",
      "Epoch 00083: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 128ms/step - loss: 0.0120 - acc: 0.9964 - val_loss: 4.3491 - val_acc: 0.5384\n",
      "Epoch 84/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0079 - acc: 0.9966\n",
      "Epoch 00084: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 118ms/step - loss: 0.0079 - acc: 0.9966 - val_loss: 4.8205 - val_acc: 0.5384\n",
      "Epoch 85/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9966\n",
      "Epoch 00085: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 9s 116ms/step - loss: 0.0129 - acc: 0.9966 - val_loss: 4.4439 - val_acc: 0.5367\n",
      "Epoch 86/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9985- ETA: 8s - loss: 0.0033 - acc: 0.99 - ETA: 7s - loss: 0.0032 - acc: 0.999 - ETA: 7s - loss: 0.0031 - acc: 0\n",
      "Epoch 00086: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 131ms/step - loss: 0.0063 - acc: 0.9985 - val_loss: 4.4143 - val_acc: 0.5392\n",
      "Epoch 87/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9979\n",
      "Epoch 00087: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 134ms/step - loss: 0.0076 - acc: 0.9979 - val_loss: 4.6525 - val_acc: 0.5502\n",
      "Epoch 88/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9954\n",
      "Epoch 00088: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 130ms/step - loss: 0.0117 - acc: 0.9954 - val_loss: 4.6927 - val_acc: 0.5477\n",
      "Epoch 89/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9954- ETA: 1s - loss: 0.0168\n",
      "Epoch 00089: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 131ms/step - loss: 0.0149 - acc: 0.9954 - val_loss: 4.6926 - val_acc: 0.5435\n",
      "Epoch 90/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0045 - acc: 0.9989\n",
      "Epoch 00090: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 131ms/step - loss: 0.0045 - acc: 0.9989 - val_loss: 4.8805 - val_acc: 0.5468\n",
      "Epoch 91/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0081 - acc: 0.9981\n",
      "Epoch 00091: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 10s 134ms/step - loss: 0.0081 - acc: 0.9981 - val_loss: 4.7846 - val_acc: 0.5392\n",
      "Epoch 92/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9975\n",
      "Epoch 00092: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 7s 97ms/step - loss: 0.0096 - acc: 0.9975 - val_loss: 4.6041 - val_acc: 0.5342\n",
      "Epoch 93/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9983\n",
      "Epoch 00093: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 7s 94ms/step - loss: 0.0063 - acc: 0.9983 - val_loss: 4.7938 - val_acc: 0.5333\n",
      "Epoch 94/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9979\n",
      "Epoch 00094: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 4s 54ms/step - loss: 0.0062 - acc: 0.9979 - val_loss: 4.7175 - val_acc: 0.5274\n",
      "Epoch 95/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0075 - acc: 0.9970\n",
      "Epoch 00095: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 5s 71ms/step - loss: 0.0075 - acc: 0.9970 - val_loss: 4.7716 - val_acc: 0.5376\n",
      "Epoch 96/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9956\n",
      "Epoch 00096: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 5s 71ms/step - loss: 0.0154 - acc: 0.9956 - val_loss: 4.5990 - val_acc: 0.5502\n",
      "Epoch 97/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0084 - acc: 0.9970\n",
      "Epoch 00097: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 4s 51ms/step - loss: 0.0084 - acc: 0.9970 - val_loss: 4.5451 - val_acc: 0.5198\n",
      "Epoch 98/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9947\n",
      "Epoch 00098: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 7s 93ms/step - loss: 0.0273 - acc: 0.9947 - val_loss: 3.7962 - val_acc: 0.5181\n",
      "Epoch 99/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9964\n",
      "Epoch 00099: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 0.0107 - acc: 0.9964 - val_loss: 3.9898 - val_acc: 0.5224\n",
      "Epoch 100/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0144 - acc: 0.9960\n",
      "Epoch 00100: val_acc did not improve from 0.56118\n",
      "75/75 [==============================] - 6s 75ms/step - loss: 0.0144 - acc: 0.9960 - val_loss: 4.3624 - val_acc: 0.5392\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-696936add0bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"테스트 정확도: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=100, callbacks=[mc], batch_size=64, validation_split=0.2)\n",
    "\n",
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T00:16:54.304216Z",
     "start_time": "2021-04-09T00:16:54.212462Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(256, 3, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T00:16:54.985394Z",
     "start_time": "2021-04-09T00:16:54.978418Z"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T00:26:12.985968Z",
     "start_time": "2021-04-09T00:16:55.510987Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 1.0745 - acc: 0.4433\n",
      "Epoch 00001: val_acc improved from -inf to 0.43713, saving model to best_model.h5\n",
      "75/75 [==============================] - 5s 61ms/step - loss: 1.0745 - acc: 0.4433 - val_loss: 1.0737 - val_acc: 0.4371\n",
      "Epoch 2/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0708 - acc: 0.4470\n",
      "Epoch 00002: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 80ms/step - loss: 1.0708 - acc: 0.4469 - val_loss: 1.0788 - val_acc: 0.4371\n",
      "Epoch 3/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0684 - acc: 0.4470\n",
      "Epoch 00003: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 73ms/step - loss: 1.0683 - acc: 0.4471 - val_loss: 1.0725 - val_acc: 0.4371\n",
      "Epoch 4/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0696 - acc: 0.4468\n",
      "Epoch 00004: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 56ms/step - loss: 1.0695 - acc: 0.4469 - val_loss: 1.0705 - val_acc: 0.4371\n",
      "Epoch 5/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0725 - acc: 0.4468\n",
      "Epoch 00005: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 87ms/step - loss: 1.0724 - acc: 0.4469 - val_loss: 1.0712 - val_acc: 0.4371\n",
      "Epoch 6/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0728 - acc: 0.4468\n",
      "Epoch 00006: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 1.0728 - acc: 0.4469 - val_loss: 1.0719 - val_acc: 0.4371\n",
      "Epoch 7/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0693 - acc: 0.4470\n",
      "Epoch 00007: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 73ms/step - loss: 1.0694 - acc: 0.4469 - val_loss: 1.0717 - val_acc: 0.4371\n",
      "Epoch 8/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0711 - acc: 0.4468\n",
      "Epoch 00008: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 69ms/step - loss: 1.0710 - acc: 0.4469 - val_loss: 1.0710 - val_acc: 0.4371\n",
      "Epoch 9/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0692 - acc: 0.4468\n",
      "Epoch 00009: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 61ms/step - loss: 1.0692 - acc: 0.4469 - val_loss: 1.0704 - val_acc: 0.4371\n",
      "Epoch 10/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0707 - acc: 0.4468\n",
      "Epoch 00010: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 91ms/step - loss: 1.0707 - acc: 0.4469 - val_loss: 1.0704 - val_acc: 0.4371\n",
      "Epoch 11/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0685 - acc: 0.4470\n",
      "Epoch 00011: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 1.0685 - acc: 0.4469 - val_loss: 1.0708 - val_acc: 0.4371\n",
      "Epoch 12/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0677 - acc: 0.4470\n",
      "Epoch 00012: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 80ms/step - loss: 1.0677 - acc: 0.4469 - val_loss: 1.0722 - val_acc: 0.4371\n",
      "Epoch 13/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0687 - acc: 0.4468\n",
      "Epoch 00013: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 69ms/step - loss: 1.0686 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 14/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0663 - acc: 0.4470\n",
      "Epoch 00014: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 57ms/step - loss: 1.0663 - acc: 0.4469 - val_loss: 1.0714 - val_acc: 0.4371\n",
      "Epoch 15/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0668 - acc: 0.4470\n",
      "Epoch 00015: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 91ms/step - loss: 1.0669 - acc: 0.4469 - val_loss: 1.0699 - val_acc: 0.4371\n",
      "Epoch 16/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0681 - acc: 0.4470\n",
      "Epoch 00016: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 1.0682 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 17/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0677 - acc: 0.4468\n",
      "Epoch 00017: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 78ms/step - loss: 1.0677 - acc: 0.4469 - val_loss: 1.0704 - val_acc: 0.4371\n",
      "Epoch 18/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 1.0670 - acc: 0.4469\n",
      "Epoch 00018: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 67ms/step - loss: 1.0670 - acc: 0.4469 - val_loss: 1.0703 - val_acc: 0.4371\n",
      "Epoch 19/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0672 - acc: 0.4470\n",
      "Epoch 00019: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 62ms/step - loss: 1.0672 - acc: 0.4469 - val_loss: 1.0705 - val_acc: 0.4371\n",
      "Epoch 20/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0676 - acc: 0.4470- ETA: 1s - loss: 1.0658 - \n",
      "Epoch 00020: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 84ms/step - loss: 1.0676 - acc: 0.4469 - val_loss: 1.0704 - val_acc: 0.4371\n",
      "Epoch 21/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0679 - acc: 0.4470\n",
      "Epoch 00021: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 54ms/step - loss: 1.0679 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 22/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0674 - acc: 0.4470\n",
      "Epoch 00022: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 92ms/step - loss: 1.0674 - acc: 0.4469 - val_loss: 1.0708 - val_acc: 0.4371\n",
      "Epoch 23/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0679 - acc: 0.4468\n",
      "Epoch 00023: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 60ms/step - loss: 1.0678 - acc: 0.4469 - val_loss: 1.0703 - val_acc: 0.4371\n",
      "Epoch 24/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0674 - acc: 0.4470- ETA: 2s - loss\n",
      "Epoch 00024: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 96ms/step - loss: 1.0674 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 25/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0678 - acc: 0.4468- ETA: 0s - loss: 1.0669 - acc: 0.\n",
      "Epoch 00025: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 62ms/step - loss: 1.0677 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 26/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0674 - acc: 0.4470- ETA: 2s - loss: 1.0669 - acc: 0. - ETA: 2s - \n",
      "Epoch 00026: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 91ms/step - loss: 1.0674 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 27/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0669 - acc: 0.4468\n",
      "Epoch 00027: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 1.0668 - acc: 0.4469 - val_loss: 1.0699 - val_acc: 0.4371\n",
      "Epoch 28/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0676 - acc: 0.4470\n",
      "Epoch 00028: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 91ms/step - loss: 1.0676 - acc: 0.4469 - val_loss: 1.0699 - val_acc: 0.4371\n",
      "Epoch 29/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 1.0681 - acc: 0.4469\n",
      "Epoch 00029: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 68ms/step - loss: 1.0681 - acc: 0.4469 - val_loss: 1.0699 - val_acc: 0.4371\n",
      "Epoch 30/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0676 - acc: 0.4468\n",
      "Epoch 00030: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 88ms/step - loss: 1.0675 - acc: 0.4469 - val_loss: 1.0705 - val_acc: 0.4371\n",
      "Epoch 31/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0682 - acc: 0.4468\n",
      "Epoch 00031: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 76ms/step - loss: 1.0681 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0671 - acc: 0.4468- ETA: 1s - loss:\n",
      "Epoch 00032: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 74ms/step - loss: 1.0670 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 33/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0670 - acc: 0.4468\n",
      "Epoch 00033: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 73ms/step - loss: 1.0669 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 34/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.4470\n",
      "Epoch 00034: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 56ms/step - loss: 1.0665 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 35/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0674 - acc: 0.4470\n",
      "Epoch 00035: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 90ms/step - loss: 1.0674 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 36/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0663 - acc: 0.4468\n",
      "Epoch 00036: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 64ms/step - loss: 1.0663 - acc: 0.4469 - val_loss: 1.0718 - val_acc: 0.4371\n",
      "Epoch 37/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0679 - acc: 0.4470\n",
      "Epoch 00037: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 85ms/step - loss: 1.0679 - acc: 0.4469 - val_loss: 1.0703 - val_acc: 0.4371\n",
      "Epoch 38/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0663 - acc: 0.4470- ETA: 4s - l - ETA:\n",
      "Epoch 00038: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 69ms/step - loss: 1.0664 - acc: 0.4469 - val_loss: 1.0699 - val_acc: 0.4371\n",
      "Epoch 39/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0672 - acc: 0.4468\n",
      "Epoch 00039: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 81ms/step - loss: 1.0671 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 40/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0671 - acc: 0.4468\n",
      "Epoch 00040: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 75ms/step - loss: 1.0671 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 41/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0678 - acc: 0.4470\n",
      "Epoch 00041: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 75ms/step - loss: 1.0679 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 42/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0677 - acc: 0.4468\n",
      "Epoch 00042: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 82ms/step - loss: 1.0676 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 43/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0668 - acc: 0.4470\n",
      "Epoch 00043: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 1.0668 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 44/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0670 - acc: 0.4468\n",
      "Epoch 00044: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 89ms/step - loss: 1.0670 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 45/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0669 - acc: 0.4468\n",
      "Epoch 00045: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 57ms/step - loss: 1.0668 - acc: 0.4469 - val_loss: 1.0703 - val_acc: 0.4371\n",
      "Epoch 46/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0671 - acc: 0.4468\n",
      "Epoch 00046: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 88ms/step - loss: 1.0671 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 47/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0669 - acc: 0.4468\n",
      "Epoch 00047: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 72ms/step - loss: 1.0669 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 48/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0667 - acc: 0.4470\n",
      "Epoch 00048: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 77ms/step - loss: 1.0667 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 49/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0660 - acc: 0.4470\n",
      "Epoch 00049: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 77ms/step - loss: 1.0661 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 50/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0664 - acc: 0.4468- ETA: 3s - ETA: 1s -\n",
      "Epoch 00050: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 73ms/step - loss: 1.0663 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 51/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.4470\n",
      "Epoch 00051: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 80ms/step - loss: 1.0665 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 52/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0666 - acc: 0.4470\n",
      "Epoch 00052: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 62ms/step - loss: 1.0666 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 53/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 1.0670 - acc: 0.4469\n",
      "Epoch 00053: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 80ms/step - loss: 1.0670 - acc: 0.4469 - val_loss: 1.0699 - val_acc: 0.4371\n",
      "Epoch 54/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0671 - acc: 0.4468\n",
      "Epoch 00054: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 1.0671 - acc: 0.4469 - val_loss: 1.0699 - val_acc: 0.4371\n",
      "Epoch 55/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0672 - acc: 0.4468\n",
      "Epoch 00055: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 91ms/step - loss: 1.0671 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 56/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0667 - acc: 0.4468\n",
      "Epoch 00056: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 1.0666 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 57/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0662 - acc: 0.4468\n",
      "Epoch 00057: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 70ms/step - loss: 1.0662 - acc: 0.4469 - val_loss: 1.0704 - val_acc: 0.4371\n",
      "Epoch 58/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0662 - acc: 0.4468\n",
      "Epoch 00058: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 81ms/step - loss: 1.0662 - acc: 0.4469 - val_loss: 1.0704 - val_acc: 0.4371\n",
      "Epoch 59/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0669 - acc: 0.4470\n",
      "Epoch 00059: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 58ms/step - loss: 1.0670 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 60/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0667 - acc: 0.4468\n",
      "Epoch 00060: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 97ms/step - loss: 1.0667 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 61/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0664 - acc: 0.4470\n",
      "Epoch 00061: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 1.0665 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 62/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.4470\n",
      "Epoch 00062: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 80ms/step - loss: 1.0666 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 63/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0670 - acc: 0.4468\n",
      "Epoch 00063: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 72ms/step - loss: 1.0669 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 64/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0659 - acc: 0.4470\n",
      "Epoch 00064: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 62ms/step - loss: 1.0659 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 65/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0663 - acc: 0.4470\n",
      "Epoch 00065: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 98ms/step - loss: 1.0663 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 66/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0664 - acc: 0.4470\n",
      "Epoch 00066: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 69ms/step - loss: 1.0664 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 67/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0661 - acc: 0.4470\n",
      "Epoch 00067: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 99ms/step - loss: 1.0661 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 68/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.4470\n",
      "Epoch 00068: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 60ms/step - loss: 1.0665 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 69/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0670 - acc: 0.4468\n",
      "Epoch 00069: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 82ms/step - loss: 1.0670 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 70/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0664 - acc: 0.4468\n",
      "Epoch 00070: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 83ms/step - loss: 1.0663 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 71/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0663 - acc: 0.4468\n",
      "Epoch 00071: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 61ms/step - loss: 1.0662 - acc: 0.4469 - val_loss: 1.0703 - val_acc: 0.4371\n",
      "Epoch 72/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0673 - acc: 0.4470\n",
      "Epoch 00072: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 98ms/step - loss: 1.0673 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 73/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0669 - acc: 0.4470\n",
      "Epoch 00073: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 61ms/step - loss: 1.0669 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 74/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0669 - acc: 0.4468- ETA: 0s - loss: 1.0667 - acc:\n",
      "Epoch 00074: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 89ms/step - loss: 1.0669 - acc: 0.4469 - val_loss: 1.0699 - val_acc: 0.4371\n",
      "Epoch 75/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0664 - acc: 0.4470\n",
      "Epoch 00075: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 61ms/step - loss: 1.0664 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 76/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0674 - acc: 0.4470\n",
      "Epoch 00076: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 68ms/step - loss: 1.0674 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 77/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0674 - acc: 0.4468\n",
      "Epoch 00077: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 71ms/step - loss: 1.0674 - acc: 0.4469 - val_loss: 1.0699 - val_acc: 0.4371\n",
      "Epoch 78/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0668 - acc: 0.4468\n",
      "Epoch 00078: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 53ms/step - loss: 1.0667 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 79/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.4470- ETA: 1s - loss: 1.0665 - \n",
      "Epoch 00079: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 93ms/step - loss: 1.0665 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 80/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0668 - acc: 0.4468\n",
      "Epoch 00080: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 60ms/step - loss: 1.0668 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 81/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0663 - acc: 0.4468\n",
      "Epoch 00081: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 79ms/step - loss: 1.0663 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 82/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0668 - acc: 0.4468\n",
      "Epoch 00082: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 72ms/step - loss: 1.0667 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 83/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.4470- ETA: 3s - loss: 1\n",
      "Epoch 00083: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 1.0665 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 84/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0668 - acc: 0.4468\n",
      "Epoch 00084: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 92ms/step - loss: 1.0667 - acc: 0.4469 - val_loss: 1.0703 - val_acc: 0.4371\n",
      "Epoch 85/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0664 - acc: 0.4470\n",
      "Epoch 00085: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 63ms/step - loss: 1.0665 - acc: 0.4469 - val_loss: 1.0703 - val_acc: 0.4371\n",
      "Epoch 86/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0666 - acc: 0.4468\n",
      "Epoch 00086: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 89ms/step - loss: 1.0665 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 87/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0668 - acc: 0.4468\n",
      "Epoch 00087: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 66ms/step - loss: 1.0667 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 88/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0664 - acc: 0.4470\n",
      "Epoch 00088: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 1.0664 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 89/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.4470\n",
      "Epoch 00089: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 92ms/step - loss: 1.0665 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 90/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.4470\n",
      "Epoch 00090: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 58ms/step - loss: 1.0666 - acc: 0.4469 - val_loss: 1.0700 - val_acc: 0.4371\n",
      "Epoch 91/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0666 - acc: 0.4470\n",
      "Epoch 00091: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 87ms/step - loss: 1.0666 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 92/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0666 - acc: 0.4468-  - ETA: 1s -\n",
      "Epoch 00092: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 5s 68ms/step - loss: 1.0666 - acc: 0.4469 - val_loss: 1.0699 - val_acc: 0.4371\n",
      "Epoch 93/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0664 - acc: 0.4468\n",
      "Epoch 00093: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 58ms/step - loss: 1.0663 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/75 [============================>.] - ETA: 0s - loss: 1.0668 - acc: 0.4468\n",
      "Epoch 00094: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 95ms/step - loss: 1.0667 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 95/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.4468\n",
      "Epoch 00095: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 58ms/step - loss: 1.0665 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 96/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.4470\n",
      "Epoch 00096: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 77ms/step - loss: 1.0665 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n",
      "Epoch 97/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0663 - acc: 0.4470- ETA: 0s - loss: 1.0655 - \n",
      "Epoch 00097: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 6s 78ms/step - loss: 1.0664 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 98/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.4470\n",
      "Epoch 00098: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 1.0666 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 99/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.4468\n",
      "Epoch 00099: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 7s 93ms/step - loss: 1.0665 - acc: 0.4469 - val_loss: 1.0701 - val_acc: 0.4371\n",
      "Epoch 100/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.4468\n",
      "Epoch 00100: val_acc did not improve from 0.43713\n",
      "75/75 [==============================] - 4s 58ms/step - loss: 1.0665 - acc: 0.4469 - val_loss: 1.0702 - val_acc: 0.4371\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=100, callbacks=[mc], batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T05:34:00.622018Z",
     "start_time": "2021-04-08T05:33:56.321704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1s 13ms/step - loss: 4.7960 - acc: 0.4311\n",
      "테스트 정확도: 0.4311\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:13:21.881003Z",
     "start_time": "2021-03-31T17:13:17.218412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39774117, 0.4180528 , 0.18420602],\n",
       "       [0.93720895, 0.05255724, 0.01023376],\n",
       "       [0.53255975, 0.3266842 , 0.14075604],\n",
       "       ...,\n",
       "       [0.27636996, 0.466064  , 0.2575661 ],\n",
       "       [0.2051393 , 0.4949639 , 0.29989678],\n",
       "       [0.39454758, 0.3854387 , 0.22001368]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = loaded_model.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 평가 참고 \n",
    "\n",
    "- https://ayoteralab.tistory.com/entry/Iris-dataset-classification-with-Keras?category=873956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T16:21:22.083815Z",
     "start_time": "2021-04-01T16:21:20.936885Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a74692dd2eda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:13:31.208117Z",
     "start_time": "2021-03-31T17:13:30.879991Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-2f3d6900dd4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scatter with Sepal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scatter with Petal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAFpCAYAAABUNF3EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAT3ElEQVR4nO3dX4jl93nf8c/T3QgaJ41MtAmu/hC1yFZ0YRV7opiStEpDa0m9WAK+kBwiIgJC1Aq5tCg0ufBNc1EIxnKWxQjhm+iiEcmmKBGFkrjgqtUKbNmykdnKRNrKICkOLthQsfbTi5m205lzZs7snplZPfN6wcD+zvlq5jtfds+j95wzM9XdAQAAmOTvHPcGAAAA1k3oAAAA4wgdAABgHKEDAACMI3QAAIBxhA4AADDOvqFTVU9V1VtV9fUl91dVfbaqLlXVy1X1kfVvEwAWM6cAWGSVZ3SeTnLfHvffn+SOrbdHk/zhtW8LAFb2dMwpAHbYN3S6+0tJvrvHkrNJvtibXkhyY1V9YF0bBIC9mFMALLKO79G5Ockb264vb90GANcDcwrgBDq9hvdRC27rhQurHs3mywbyvve976N33nnnGj48AFfrpZdeeqe7zxz3Pg6ZOQXwHnUtc2odoXM5ya3brm9J8uaihd19Psn5JNnY2OiLFy+u4cMDcLWq6q+Pew9HwJwCeI+6ljm1jpeuXUjy8NZPtflYku9193fW8H4BYB3MKYATaN9ndKrqj5Lcm+Smqrqc5PeS/FiSdPe5JM8leSDJpSQ/SPLIYW0WAHYypwBYZN/Q6e6H9rm/k3xqbTsCgAMwpwBYZB0vXQMAALiuCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYZ6XQqar7qurVqrpUVU8suP+nqurPquqrVfVKVT2y/q0CwGLmFAA77Rs6VXUqyZNJ7k9yV5KHququHcs+leQb3X13knuT/LuqumHNewWAXcwpABZZ5Rmde5Jc6u7XuvvdJM8kObtjTSf5yaqqJD+R5LtJrqx1pwCwmDkFwC6rhM7NSd7Ydn1567btPpfk55O8meRrSX6nu3+0lh0CwN7MKQB2WSV0asFtveP640m+kuTvJ/lHST5XVX9v1zuqerSqLlbVxbfffvvAmwWABcwpAHZZJXQuJ7l12/Ut2fyK2HaPJHm2N11K8u0kd+58R919vrs3unvjzJkzV7tnANjOnAJgl1VC58Ukd1TV7VvfuPlgkgs71rye5FeTpKp+NsmHkry2zo0CwBLmFAC7nN5vQXdfqarHkzyf5FSSp7r7lap6bOv+c0k+k+TpqvpaNl9C8OnufucQ9w0AScwpABbbN3SSpLufS/LcjtvObfvzm0n+xXq3BgCrMacA2GmlXxgKAADwXiJ0AACAcYQOAAAwjtABAADGEToAAMA4QgcAABhH6AAAAOMIHQAAYByhAwAAjCN0AACAcYQOAAAwjtABAADGEToAAMA4QgcAABhH6AAAAOMIHQAAYByhAwAAjCN0AACAcYQOAAAwjtABAADGEToAAMA4QgcAABhH6AAAAOMIHQAAYByhAwAAjCN0AACAcYQOAAAwjtABAADGEToAAMA4QgcAABhH6AAAAOMIHQAAYByhAwAAjCN0AACAcYQOAAAwjtABAADGEToAAMA4QgcAABhH6AAAAOMIHQAAYByhAwAAjCN0AACAcYQOAAAwjtABAADGEToAAMA4QgcAABhH6AAAAOMIHQAAYByhAwAAjCN0AACAcYQOAAAwjtABAADGEToAAMA4QgcAABhH6AAAAOMIHQAAYByhAwAAjLNS6FTVfVX1alVdqqonlqy5t6q+UlWvVNVfrXebALCcOQXATqf3W1BVp5I8meSfJ7mc5MWqutDd39i25sYkn09yX3e/XlU/c1gbBoDtzCkAFlnlGZ17klzq7te6+90kzyQ5u2PNJ5M8292vJ0l3v7XebQLAUuYUALusEjo3J3lj2/Xlrdu2+2CS91fVX1bVS1X18KJ3VFWPVtXFqrr49ttvX92OAeD/Z04BsMsqoVMLbusd16eTfDTJv0zy8ST/pqo+uOs/6j7f3RvdvXHmzJkDbxYAFjCnANhl3+/RyeZXxm7ddn1LkjcXrHmnu7+f5PtV9aUkdyf51lp2CQDLmVMA7LLKMzovJrmjqm6vqhuSPJjkwo41f5rkl6vqdFX9eJJfTPLN9W4VABYypwDYZd9ndLr7SlU9nuT5JKeSPNXdr1TVY1v3n+vub1bVXyR5OcmPknyhu79+mBsHgMScAmCx6t75MuajsbGx0RcvXjyWjw3Apqp6qbs3jnsf1yNzCuD4XcucWukXhgIAALyXCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYZ6XQqar7qurVqrpUVU/sse4XquqHVfWJ9W0RAPZmTgGw076hU1WnkjyZ5P4kdyV5qKruWrLu95M8v+5NAsAy5hQAi6zyjM49SS5192vd/W6SZ5KcXbDut5P8cZK31rg/ANiPOQXALquEzs1J3th2fXnrtv+rqm5O8mtJzu31jqrq0aq6WFUX33777YPuFQAWMacA2GWV0KkFt/WO6z9I8unu/uFe76i7z3f3RndvnDlzZtU9AsBezCkAdjm9wprLSW7ddn1Lkjd3rNlI8kxVJclNSR6oqivd/Sdr2SUALGdOAbDLKqHzYpI7qur2JP8jyYNJPrl9QXff/n/+XFVPJ/kPhgcAR8ScAmCXfUOnu69U1ePZ/Ck1p5I81d2vVNVjW/fv+XpnADhM5hQAi6zyjE66+7kkz+24beHg6O7fvPZtAcDqzCkAdlrpF4YCAAC8lwgdAABgHKEDAACMI3QAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGEfoAAAA4wgdAABgHKEDAACMI3QAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGEfoAAAA4wgdAABgHKEDAACMI3QAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGEfoAAAA4wgdAABgHKEDAACMI3QAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGEfoAAAA4wgdAABgHKEDAACMI3QAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGEfoAAAA4wgdAABgHKEDAACMI3QAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGEfoAAAA4wgdAABgHKEDAACMI3QAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGGel0Kmq+6rq1aq6VFVPLLj/16vq5a23L1fV3evfKgAsZk4BsNO+oVNVp5I8meT+JHcleaiq7tqx7NtJ/ml3fzjJZ5KcX/dGAWARcwqARVZ5RueeJJe6+7XufjfJM0nObl/Q3V/u7r/dunwhyS3r3SYALGVOAbDLKqFzc5I3tl1f3rptmd9K8ufXsikAOABzCoBdTq+wphbc1gsXVv1KNgfILy25/9EkjybJbbfdtuIWAWBP5hQAu6zyjM7lJLduu74lyZs7F1XVh5N8IcnZ7v6bRe+ou89390Z3b5w5c+Zq9gsAO5lTAOyySui8mOSOqrq9qm5I8mCSC9sXVNVtSZ5N8hvd/a31bxMAljKnANhl35eudfeVqno8yfNJTiV5qrtfqarHtu4/l+R3k/x0ks9XVZJc6e6Nw9s2AGwypwBYpLoXvoz50G1sbPTFixeP5WMDsKmqXvI//IuZUwDH71rm1Eq/MBQAAOC9ROgAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOCuFTlXdV1WvVtWlqnpiwf1VVZ/duv/lqvrI+rcKAIuZUwDstG/oVNWpJE8muT/JXUkeqqq7diy7P8kdW2+PJvnDNe8TABYypwBYZJVndO5Jcqm7X+vud5M8k+TsjjVnk3yxN72Q5Maq+sCa9woAi5hTAOyySujcnOSNbdeXt2476BoAOAzmFAC7nF5hTS24ra9iTarq0Wy+ZCBJ/ldVfX2Fj38S3ZTknePexHXK2SznbJZzNst96Lg3sAbm1NHzb2o5Z7Ocs1nO2Sx31XNqldC5nOTWbde3JHnzKtaku88nOZ8kVXWxuzcOtNsTwtks52yWczbLOZvlqurice9hDcypI+ZslnM2yzmb5ZzNctcyp1Z56dqLSe6oqtur6oYkDya5sGPNhSQPb/1Um48l+V53f+dqNwUAB2BOAbDLvs/odPeVqno8yfNJTiV5qrtfqarHtu4/l+S5JA8kuZTkB0keObwtA8D/Y04BsMgqL11Ldz+XzSGx/bZz2/7cST51wI99/oDrTxJns5yzWc7ZLOdslhtxNubUkXM2yzmb5ZzNcs5muas+m9p87AcAAJhjle/RAQAAeE859NCpqvuq6tWqulRVTyy4v6rqs1v3v1xVHznsPV0vVjibX986k5er6stVdfdx7PM47Hc229b9QlX9sKo+cZT7O06rnE1V3VtVX6mqV6rqr456j8dlhX9TP1VVf1ZVX906mxPxfRpV9VRVvbXsRyWf5MfhxJzaizm1nDm1nDm1nDm13KHMqu4+tLdsflPof0/yD5LckOSrSe7aseaBJH+ezd9x8LEk//Uw93S9vK14Nv84yfu3/ny/s1m47j9l83X5nzjufV8vZ5PkxiTfSHLb1vXPHPe+r6Oz+ddJfn/rz2eSfDfJDce99yM4m3+S5CNJvr7k/hP5OHyAvzcn8nzMqWs7m23rzClz6iBncyLn1Nbnu/ZZddjP6NyT5FJ3v9bd7yZ5JsnZHWvOJvlib3ohyY1V9YFD3tf1YN+z6e4vd/ffbl2+kM3f+3ASrPL3Jkl+O8kfJ3nrKDd3zFY5m08meba7X0+S7j4p57PK2XSSn6yqSvIT2RwgV452m0evu7+Uzc91mZP6OJyYU3sxp5Yzp5Yzp5Yzp/ZwGLPqsEPn5iRvbLu+vHXbQddMdNDP+7eyWbEnwb5nU1U3J/m1JOdysqzy9+aDSd5fVX9ZVS9V1cNHtrvjtcrZfC7Jz2fzF0V+LcnvdPePjmZ717WT+jicmFN7MaeWM6eWM6eWM6euzYEfi1f68dLXoBbctvPHvK2yZqKVP++q+pVsDpBfOtQdXT9WOZs/SPLp7v7h5hc9ToxVzuZ0ko8m+dUkfzfJf6mqF7r7W4e9uWO2ytl8PMlXkvyzJP8wyX+sqv/c3f/zsDd3nTupj8OJObUXc2o5c2o5c2o5c+raHPix+LBD53KSW7dd35LNQj3omolW+ryr6sNJvpDk/u7+myPa23Fb5Ww2kjyzNTxuSvJAVV3p7j85mi0em1X/Tb3T3d9P8v2q+lKSu5NMHyCrnM0jSf5tb77Y91JVfTvJnUn+29Fs8bp1Uh+HE3NqL+bUcubUcubUcubUtTnwY/Fhv3TtxSR3VNXtVXVDkgeTXNix5kKSh7d+ksLHknyvu79zyPu6Hux7NlV1W5Jnk/zGCfgqx3b7nk13397dP9fdP5fk3yf5VydgeCSr/Zv60yS/XFWnq+rHk/xikm8e8T6Pwypn83o2v4KYqvrZJB9K8tqR7vL6dFIfhxNzai/m1HLm1HLm1HLm1LU58GPxoT6j091XqurxJM9n8ydNPNXdr1TVY1v3n8vmTyJ5IMmlJD/IZsmOt+LZ/G6Sn07y+a2vCF3p7o3j2vNRWfFsTqRVzqa7v1lVf5Hk5SQ/SvKF7l74oxonWfHvzWeSPF1VX8vmU+Cf7u53jm3TR6Sq/ijJvUluqqrLSX4vyY8lJ/txODGn9mJOLWdOLWdOLWdO7e0wZlVtPjMGAAAwx6H/wlAAAICjJnQAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGEfoAAAA4wgdAABgnP8NSAWtsCxOj8UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(14,6))\n",
    "ax[0].scatter(dataset.data[:,0], dataset.data[:,1], c=dataset.target)\n",
    "ax[0].set_title('scatter with Sepal')\n",
    "ax[1].scatter(dataset.data[:,2], dataset.data[:,3], c=dataset.target)\n",
    "ax[1].set_title('scatter with Petal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
