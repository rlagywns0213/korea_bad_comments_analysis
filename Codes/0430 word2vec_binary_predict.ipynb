{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:11:12.131833Z",
     "start_time": "2021-04-30T04:11:12.081962Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "contents = []\n",
    "\n",
    "with open('Curse-detection-data/dataset.txt', 'r',encoding='UTF8') as f:\n",
    "    reader = csv.reader(f, delimiter = '\\t')\n",
    "    for row in f:\n",
    "        contents.append(row.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:11:12.379167Z",
     "start_time": "2021-04-30T04:11:12.315337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>좌배 까는건 ㅇㅂ</td>\n",
       "      <td>1\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>집에 롱 패딩만 세 개다. 10년 더 입어야지 ㅋㅋ</td>\n",
       "      <td>0\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>개소리야 니가 빨갱이를 옹호하고 드루킹을 ㅇㅇ짓이라고 말못해서 삐진거야 빨갱아</td>\n",
       "      <td>1\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>세탁이라고 봐도 된다</td>\n",
       "      <td>0\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>애새끼가 초딩도 아니고 ㅋㅋㅋㅋ</td>\n",
       "      <td>1\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5820</th>\n",
       "      <td>좌우 헬파이어 3개씩 6개 장착에 아파치보다 약하지만 20mm 기관포 장착임</td>\n",
       "      <td>0\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5821</th>\n",
       "      <td>세금 내놓으라고 데모질 중 ㅋㅋ간첩, 도둑놈 새끼들이 대통령 해처먹으니까  나도 같...</td>\n",
       "      <td>1\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>너가 한 말 중에</td>\n",
       "      <td>0\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823</th>\n",
       "      <td>제갈대중 ㅇㅂ</td>\n",
       "      <td>0\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5824</th>\n",
       "      <td>우리나라교회는 악마들이모여 주뎅이 처벌리고</td>\n",
       "      <td>1\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5825 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               document label  none\n",
       "0                                             좌배 까는건 ㅇㅂ   1\\n  None\n",
       "1                          집에 롱 패딩만 세 개다. 10년 더 입어야지 ㅋㅋ   0\\n  None\n",
       "2           개소리야 니가 빨갱이를 옹호하고 드루킹을 ㅇㅇ짓이라고 말못해서 삐진거야 빨갱아   1\\n  None\n",
       "3                                           세탁이라고 봐도 된다   0\\n  None\n",
       "4                                    애새끼가 초딩도 아니고 ㅋㅋㅋㅋ    1\\n  None\n",
       "...                                                 ...   ...   ...\n",
       "5820         좌우 헬파이어 3개씩 6개 장착에 아파치보다 약하지만 20mm 기관포 장착임   0\\n  None\n",
       "5821  세금 내놓으라고 데모질 중 ㅋㅋ간첩, 도둑놈 새끼들이 대통령 해처먹으니까  나도 같...   1\\n  None\n",
       "5822                                          너가 한 말 중에   0\\n  None\n",
       "5823                                            제갈대중 ㅇㅂ   0\\n  None\n",
       "5824                           우리나라교회는 악마들이모여 주뎅이 처벌리고    1\\n  None\n",
       "\n",
       "[5825 rows x 3 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(contents, columns=['document','label','none'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:16.601794Z",
     "start_time": "2021-04-30T04:14:16.551929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>좌배 까는건 ㅇㅂ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>집에 롱 패딩만 세 개다. 10년 더 입어야지 ㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>개소리야 니가 빨갱이를 옹호하고 드루킹을 ㅇㅇ짓이라고 말못해서 삐진거야 빨갱아</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>세탁이라고 봐도 된다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>애새끼가 초딩도 아니고 ㅋㅋㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5804</th>\n",
       "      <td>좌우 헬파이어 3개씩 6개 장착에 아파치보다 약하지만 20mm 기관포 장착임</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5805</th>\n",
       "      <td>세금 내놓으라고 데모질 중 ㅋㅋ간첩, 도둑놈 새끼들이 대통령 해처먹으니까  나도 같...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5806</th>\n",
       "      <td>너가 한 말 중에</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5807</th>\n",
       "      <td>제갈대중 ㅇㅂ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5808</th>\n",
       "      <td>우리나라교회는 악마들이모여 주뎅이 처벌리고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5809 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               document  label\n",
       "0                                             좌배 까는건 ㅇㅂ      1\n",
       "1                          집에 롱 패딩만 세 개다. 10년 더 입어야지 ㅋㅋ      0\n",
       "2           개소리야 니가 빨갱이를 옹호하고 드루킹을 ㅇㅇ짓이라고 말못해서 삐진거야 빨갱아      1\n",
       "3                                           세탁이라고 봐도 된다      0\n",
       "4                                    애새끼가 초딩도 아니고 ㅋㅋㅋㅋ       1\n",
       "...                                                 ...    ...\n",
       "5804         좌우 헬파이어 3개씩 6개 장착에 아파치보다 약하지만 20mm 기관포 장착임      0\n",
       "5805  세금 내놓으라고 데모질 중 ㅋㅋ간첩, 도둑놈 새끼들이 대통령 해처먹으니까  나도 같...      1\n",
       "5806                                          너가 한 말 중에      0\n",
       "5807                                            제갈대중 ㅇㅂ      0\n",
       "5808                           우리나라교회는 악마들이모여 주뎅이 처벌리고       1\n",
       "\n",
       "[5809 rows x 2 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어수가 1개 이하인 악플 리뷰 삭제\n",
    "none_meaning_index = df[df.document.apply(lambda x:len(x)<2)].index\n",
    "df.drop(none_meaning_index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label 값 \\n 처리 , none 열 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:22.702487Z",
     "start_time": "2021-04-30T04:14:22.631675Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'int' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-190-c4e28b416942>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'한경'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m455\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'none'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-190-c4e28b416942>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'한경'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m455\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'none'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'int' is not iterable"
     ]
    }
   ],
   "source": [
    "df[df['label'].apply(lambda x: '한경' in x)]\n",
    "df.drop(455, axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['label'] = df['label'].apply(lambda x : int(x.replace(\"\\n\",\"\")))\n",
    "df.drop('none',axis=1,inplace=True)\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:23.220102Z",
     "start_time": "2021-04-30T04:14:22.943842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "훈련용 리뷰의 개수 : 4356\n",
      "테스트용 리뷰의 개수 : 1453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1814ea50d30>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "df['document'].nunique(),df['label'].nunique() \n",
    "print(df.isnull().values.any())\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size = 0.25, random_state = 42)\n",
    "print('훈련용 리뷰의 개수 :', len(train_data))\n",
    "print('테스트용 리뷰의 개수 :', len(test_data))\n",
    "\n",
    "train_data['label'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:23.407601Z",
     "start_time": "2021-04-30T04:14:23.390647Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '만', '게임', '겜', '되', '음', '면']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:25.674902Z",
     "start_time": "2021-04-30T04:14:23.974087Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-193-ff1de90eb13d>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['tokenized'] = train_data['document'].apply(mecab.morphs)\n",
      "<ipython-input-193-ff1de90eb13d>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
      "<ipython-input-193-ff1de90eb13d>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['tokenized'] = test_data['document'].apply(mecab.morphs)\n",
      "<ipython-input-193-ff1de90eb13d>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n"
     ]
    }
   ],
   "source": [
    "mecab = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "\n",
    "\n",
    "train_data['tokenized'] = train_data['document'].apply(mecab.morphs)\n",
    "train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "test_data['tokenized'] = test_data['document'].apply(mecab.morphs)\n",
    "test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어와 길이 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:28.239562Z",
     "start_time": "2021-04-30T04:14:28.208130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document     0\n",
       "label        0\n",
       "tokenized    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:29.424428Z",
     "start_time": "2021-04-30T04:14:29.367578Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\n",
    "negative_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:30.120564Z",
     "start_time": "2021-04-30T04:14:30.092640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ㅋㅋㅋ', 825), ('.', 640), ('거', 351), ('?', 350), ('ㅋㅋ', 327), ('새끼', 260), ('있', 246), ('안', 192), ('아', 185), ('보', 176), ('나', 174), ('냐', 170), ('없', 148), ('니', 147), ('어', 145), ('로', 141), ('같', 140), ('일', 139), ('는데', 138), ('존나', 138)]\n"
     ]
    }
   ],
   "source": [
    "negative_word_count = Counter(negative_words)\n",
    "print(negative_word_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:30.560390Z",
     "start_time": "2021-04-30T04:14:30.522490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 1037), ('?', 587), ('ㅋㅋㅋ', 391), ('있', 345), ('거', 332), ('ㅋㅋ', 272), ('냐', 243), ('안', 228), ('없', 228), ('는데', 208), ('나', 207), (',', 196), ('말', 171), ('에서', 169), ('으로', 165), ('아', 164), ('ㄷ', 156), ('아니', 150), ('어', 145), ('보', 144)]\n"
     ]
    }
   ],
   "source": [
    "positive_word_count = Counter(positive_words)\n",
    "print(positive_word_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:31.426079Z",
     "start_time": "2021-04-30T04:14:30.817701Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정 리뷰의 평균 길이 : 18.231319038336583\n",
      "긍정 리뷰의 평균 길이 : 11.133120340788073\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnIAAAFhCAYAAAD0hEc9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhlZXXv8e9PQHAABWkI0GCjEiMQRWyRRGNQE8EhgrkOkBhRiSQGI06JoEkkuRI1icMlRhQVwQEIiRKIwQFRQK4INoMyhUsjKA0IrYiAA0qz7h/7rXAoqrpOdZ9TVefU9/M85zl7v2fvd6+9qV6sPaeqkCRJ0uh5wHwHIEmSpHVjISdJkjSiLOQkSZJGlIWcJEnSiLKQkyRJGlEWcpIkSSPKQk7SWElyRJJPrcN8lyfZawghSdLQWMhJGqokhyc5fVLb1dO07T+30d2rqnapqrPWZd4kleQxg4hjkH1JGn8WcpKG7RzgqUk2AEjyK8BGwO6T2h7Tpu1bkg0HHKskjRQLOUnD9k26wm23Nv504KvAVZParqmqG5Nsm+S0JLcmWZnk1RMdtdOm/57kU0luB16RZMckZye5I8kZwJY902/Spv1hktuSfDPJ1lMFmeS6JL/Ts5yTk3yi9Xt5kuXTzDdRfH4ryZ1JXtran5/kkrbcryd5fGt/aZLvJNmsjT8nyfeTLJmqryRbJvlc6+fWJF9LYu6WBFjISRqyqvoFcD5dsUb7/hpw7qS2iSLmRGAVsC3wIuDvkzyrp8t9gX8HHg58GjgBuJCugPvfwIE90x4IPAzYHngE8KfAz/oM/QXASW05pwEfmGb9JtbhCVX10Kr61yS7A8cCf9KW+2HgtCQbV9W/AucBRyV5BPAx4I+ravVUfQFvattjCbA18FbAdytKAizkJM2Ns7m3aPstukLua5Pazk6yPfA04C1V9fOqugT4KPBHPX2dV1X/UVX30BU3Twb+uqruqqpzgP/smfaXdIXUY6pqTVVdWFW39xnzuVV1elWtAT4JPGEW6/tq4MNVdX5b7vHAXcCe7fdDgGcCZwH/WVWfW0tfvwS2AR5ZVb+sqq+VL8mW1FjISZoL5wBPS7I5sKSqrga+Dvxma9u1TbMtcGtV3dEz73eB7XrGr+8Z3hb4UVX9ZNL0Ez4JfBE4KcmNSf4hyUZ9xvz9nuGfApvM4pq8RwJvaqdDb0tyG91RwW0Bquo24N/o1vs9M/T1j8BK4EvtlOxhfcYgaRGwkJM0F86jO8V5MPB/AdqRsRtb241VdW0b3yLJpj3z7gDc0DPeezTqJmDzJA+ZND1tGb+sqr+tqp2B3wSeD7x8YGs1veuBI6vq4T2fB1fViQBJdgNeRXca+ai1dVRVd1TVm6rqUcDvAW+cdKpZ0iJmISdp6KrqZ8AK4I10p1QnnNvazmnTXU93pO6d7UaFxwMH0V0LN1W/3239/m2SByZ5Gl2xA0CSZyT59XZ37O10pynXDHr9gJuBR/WMfwT40yRPSechSZ6XZNMkmwCforvW7ZXAdkn+bLq+2k0Tj0mStg5rhrQOkkaQhZykuXI2sBVd8Tbha62t97EjBwDL6I7OnQK8varOWEu/fwA8BbgVeDvwiZ7ffoXuxojbgStbDLN+WHAfjgCOb6dRX1JVK+iuk/sA8CO6U6OvaNO+E1hVVUdX1V3Ay4B3JNlpqr6AnYAvA3fSHdn84Lo+707S+InXzEqSJI0mj8hJkiSNKAs5SZKkEWUhJ0mSNKIs5CRJkkaUhZwkSdKIspCTJEkaURZykiRJI8pCTpIkaURZyEmSJI0oCzktKEnuTPKomadcGEYtXkmzk+RDSf56Lb+/NclH5zKm9TFq8WpmvqJL8ybJWcCnqmqoSSXJXsBXgJ8CRfcOz3dV1ceHuVxJ46Xlkk9V1dI5WNZZwJ7A3cDP6d5HfEhV3TTsZWu0eEROi8WNVfVQYDPgDcBHkjx2nmOSpLV5bctbjwEeCvzTPMejBchCTiS5Lsmbk3w7yY+T/GuSTXp+f36SS5LcluTrSR7f89vuSS5OckeSf2vzvqP9tnmSzyVZneRHbXhp++1I4LeAD7TTkx9o7ZXkMUn2TPL9JBv0LOuFSb7dhh+Q5LAk1yT5YZKTk2wx07pW53TgVuDxM/WV5AtJXjtpe30rye/3xtuGN07yT0m+l+TmdkrmQe23s5P8rzb8tDbfc9v47yS5pA0/pk374yQ/SPKvs/lvKS1mLZcdnuSKlnM+PimXvTrJyiS3JjktybatPUnel+SW9m/v20l2bb8dl+QdSR4CfB7YtuWsO5Nsm+SIJJ9q086UL34tyRlt+VcleUk/61VVtwH/AezW0++UffWRO/8n3p7pv97y+7faUUeSPCPJpT3TfTnJBT3j5ybZrw2/JckN7f8DVyV5Vj/rpcGwkNOElwD7ADvSFTivgK5QA44F/gR4BPBh4LRWtDwQOAU4DtgCOBF4YU+fDwA+DjwS2AH4GfABgKp6G/A12h5nVd0n+VXVN4CfAM/saf4D4IQ2/DpgP+C3gW2BHwH/MtNKtqLtBcCWwMo++joBOKBn/p3b+vzXFN2/G/hVumT7GGA74G/ab2cDe7XhpwPfacubGD+7Df9v4EvA5sBS4J9nWidJ9/GHwN7Ao+n+Pf4VQJJnAu+ky3XbAN8FTmrzPJvu3+GvAg8HXgr8sLfTqvoJ8Bza0f32uXHSsqfNF60QPKNNs1Wb7oNJdplphZI8Avh9Ws5aW1995M7efrejy2XvoMvhbwY+k2QJcB7wmCRbJtkQ2BVYmmTTtoP6JOBr6c5svBZ4clVtSrftr5tpnTQ4FnKacFRV3VhVtwL/yb17fq8GPlxV51fVmqo6HriL7tqNPYEN27y/rKrPAv+zx1ZVP6yqz1TVT6vqDuBI7i1e+nEiLSkm2RR4bmuDrrB8W1Wtqqq7gCOAF7WEM5Vtk9xGV0yeAryxqi7uo69TgN2SPLJN+4fAZ9t0/yNJ2rZ6Q1Xd2tb374H92yRnc9/C7Z0947/NvYXcL+kS/7ZV9fOqOreP7STpXh+oqutbLjuSewurPwSOraqL2r/fw4HfSLKM7t/dpsCv0V07fuU6Xou2tnzxfOC6qvp4Vd1dVRcBnwFetJb+jkryY+AHdDuff97aZ+prbbmz18uA06vq9Kq6p6rOAFYAz62qn7fhpwPLgW8D5wJPpcv9V1fVD4E1wMbAzkk2qqrrquqa/jeZ1peFnCZ8v2f4p3TXY0BXVLypHXa/rRVD29MdudoWuKHue8fM9RMDSR6c5MNJvpvkdrqLdR/ee8h/BicAv59kY7q90Yuq6rs9cZ3SE9OVdAll62n6urGqHk53jdxR3Hdvddq+WkH2X9xbkO0PfHqK/pcADwYu7OnnC60dur3bX02yNV2R/Alg+yRbAnu0bQPwl0CAC5JcnuRVM28mST2u7xn+Ll2eon1P5A+q6k66o27bVdVX6M4W/Atwc5Jjkmw22wXPkC8eCTxlUi79Q+BX1tLl66rqYXRnSSaO0vfT19pyZ69HAi+e1M/T6I5Ywr1nEibOGpxFt+P5PzufVbUSeD3dDvAtSU6aOGWtuWEhp5lcDxxZVQ/v+Ty4qk4EbgK2a0ejJmzfM/wm4LHAU6pqM7pkAF2hAt0dpNOqqivoEu9zuP+pgeuB50yKa5OqumGGPu8C3gL8+sT1HX30dSJwQJLfAB4EfHWKrn9Ad7Rvl54+HtYuVKaqfgpcCBwKXFZVvwC+DrwRuKaqftCm+35VvbqqtqU7UvjBtGvwJPWlNwftQHeXOu174kjZxOnJRwA3AFTVUVX1JGAXulOsfzFF3/085mG6fHE9cPakPPPQqnrNTB1W1aV0pz//peXbtfY1Q+7sdT3wyUn9PKSq3tV+n1zITZxZ6D2LQFWdUFVPo9u+RXeZieaIhZxm8hHgT5M8JZ2HJHleO1x/Ht2Rq9cm2TDJvnRHlyZsSlfc3Jbu5oG3T+r7ZmCmZ7CdQHcN29OBf+tp/xBw5MQpjCRL2vJn1Iqo93Dv9Wsz9XU6XYL6O+Bfq+qeKfq8h25bvS/JVq2f7ZLs3TPZ2XTXkkwkwLMmjZPkxWk3hNBdq1d021hSfw5JsrTlnLcCEzcMnQC8Mslu7UjV3wPnV9V1SZ7cctxGdNeX/Zyp/93dDDwiycPWsvzp8sXn6I7K/1GSjdrnyUke1+d6HU93PdwL+uxrutzZ61PA7yXZO8kGSTZJsldPDvo63c74HsAFVXV5W7en0M4iJHlskme2bfpzupxvzppDFnJaq6paQXft1wfoCouVtBshWkH0+8BBwG1011t8ju4aOoD30+2R/gD4Bt2pxl7/h+5atB8lOWqaEE6k2yP8ysRRq555TwO+lOSO1v9TZrFqxwI7JPm9mfpqR/E+C/wO0+/ZQnekbyXwjXYq+ct0SXDC2XTF7TnTjAM8GTg/yZ0tpkOr6tpZrJe02J1Ad8PQd9rnHQBVdSbw13TXkt1EdzPExCnQzeh2xH5EdyTrh0zxqI+q+m+6nPSddiryfqcQp8sX7bTrs9syb6S7nOXddNeXzajl26OAv+6zr+lyZ2+f1wP70hW8q+mO0P0FrTZoN3hcBFzelg/dDvx3q+qWNr4x8C66PP99umLzrf2skwbDBwJroJKcD3yofNiupDmW5Drgj6vqy/MdizRXPCKn9ZLkt5P8Sju1eiDdRbmTj7xJkqQhmO5RDVK/HgucTHeX6zXAi9bxtn1JkjRLnlqVJEkaUZ5alSRJGlEWcpIkSSNqbK+R23LLLWvZsmXzHYakOXLhhRf+oKqWzDzlwmf+khafdc1hY1vILVu2jBUrVsx3GJLmSJKpXkE0ksxf0uKzrjlsaKdWkxyb5JYkl03x25uTVHvP5ETb4UlWJrmq92n4SZ6U5NL221GTXgclSZK0aA3zGrnjgH0mNybZHvhd4Hs9bTvTPaF6lzbPB3terH40cDCwU/vcr09JkqTFaGiFXFWdA9w6xU/vA/6S+758eF/gpKq6q72OaCWwR5JtgM2q6rzqnpPyCWC/+/UoSZK0CM3pXatJXgDcUFXfmvTTdnTveJuwqrVt14Ynt0uSJC16c3azQ5IHA2+je9Hv/X6eoq3W0j7dMg6mOw3LDjvssA5RSpIkjY65PCL3aGBH4FvtxcZLgYuS/Ardkbbte6ZdCtzY2pdO0T6lqjqmqpZX1fIlS8biKQSSJEnTmrNCrqouraqtqmpZVS2jK9J2r6rvA6cB+yfZOMmOdDc1XNDe2XlHkj3b3aovB06dq5glSZIWsmE+fuRE4DzgsUlWJTloummr6nK6F69fAXwBOKSq1rSfXwN8lO4GiGuAzw8rZkmSpFEytGvkquqAGX5fNmn8SODIKaZbAew60OAkSZLGgO9alSRJGlEWcpIkSSNqbN+1OivDfOtXTfu0FEkaCFOYtHh5RE6SJGlEWchJkiSNKAs5SZKkEWUhJ0mSNKIs5CRJkkaUhZwkSdKIspCTJEkaURZykiRJI8pCTpIkaURZyEmSJI0oCzlJkqQRZSEnSZI0oizkJEmSRpSFnCRJ0oiykJMkSRpRFnKSJEkjykJOkiRpRFnISZIkjSgLOUmaJMn2Sb6a5Moklyc5tLUfkeSGJJe0z3N75jk8ycokVyXZu6f9SUkubb8dlSTzsU6SxtOG8x2AJC1AdwNvqqqLkmwKXJjkjPbb+6rqn3onTrIzsD+wC7At8OUkv1pVa4CjgYOBbwCnA/sAn5+j9ZA05jwiJ0mTVNVNVXVRG74DuBLYbi2z7AucVFV3VdW1wEpgjyTbAJtV1XlVVcAngP2GHL6kRcRCTpLWIsky4InA+a3ptUm+neTYJJu3tu2A63tmW9XatmvDk9unWs7BSVYkWbF69eoBroGkcWYhJ0nTSPJQ4DPA66vqdrrTpI8GdgNuAt4zMekUs9da2u/fWHVMVS2vquVLlixZ79glLQ4WcpI0hSQb0RVxn66qzwJU1c1Vtaaq7gE+AuzRJl8FbN8z+1Lgxta+dIp2SRoICzlJmqTdWfox4Mqqem9P+zY9k70QuKwNnwbsn2TjJDsCOwEXVNVNwB1J9mx9vhw4dU5WQtKi4F2rknR/TwX+CLg0ySWt7a3AAUl2ozs9eh3wJwBVdXmSk4Er6O54PaTdsQrwGuA44EF0d6t6x6qkgRlaIZfkWOD5wC1VtWtr+0fg94BfANcAr6yq29pvhwMHAWuA11XVF1v7k7g3CZ4OHNru/pKkoaiqc5n6+rbT1zLPkcCRU7SvAHYdXHSSdK9hnlo9ju55Sb3OAHatqscD/w84HO73DKZ9gA8m2aDNM/EMpp3aZ3KfkiRJi9LQCrmqOge4dVLbl6rq7jb6De69CNhnMEmSJM3SfN7s8CruvVZkvZ/BJEmStNjMSyGX5G10FwR/eqJpislm9Qym1q8P1JQkSYvGnBdySQ6kuwniD3tuWhjIM5h8oKYkSVpM5rSQS7IP8BbgBVX1056ffAaTJEnSLA3z8SMnAnsBWyZZBbyd7i7VjYEzurqMb1TVn/oMJkmSpNkbWiFXVQdM0fyxtUzvM5gkSZJmwVd0SZIkjSgLOUmSpBFlISdJkjSiLOQkSZJGlIWcJEnSiLKQkyRJGlEWcpIkSSPKQk6SJGlEWchJkiSNKAs5SZKkEWUhJ0mSNKIs5CRJkkaUhZwkSdKIspCTJEkaURZykiRJI8pCTpIkaURZyEmSJI0oCzlJkqQRZSEnSZI0oizkJEmSRpSFnCRJ0oiykJMkSRpRFnKSJEkjykJOkiRpRFnISZIkjSgLOUmSpBFlISdJkjSiLOQkSZJGlIWcJEnSiBpaIZfk2CS3JLmsp22LJGckubp9b97z2+FJVia5KsnePe1PSnJp++2oJBlWzJIkSaNkmEfkjgP2mdR2GHBmVe0EnNnGSbIzsD+wS5vng0k2aPMcDRwM7NQ+k/uUpIFKsn2Srya5MsnlSQ5t7e6MSlpQhlbIVdU5wK2TmvcFjm/DxwP79bSfVFV3VdW1wEpgjyTbAJtV1XlVVcAneuaRpGG5G3hTVT0O2BM4pO1wujMqaUGZ62vktq6qmwDa91atfTvg+p7pVrW27drw5HZJGpqquqmqLmrDdwBX0uUed0YlLSgL5WaHqU411Frap+4kOTjJiiQrVq9ePbDgJC1eSZYBTwTOx51RSQvMXBdyN7c9VNr3La19FbB9z3RLgRtb+9Ip2qdUVcdU1fKqWr5kyZKBBi5p8UnyUOAzwOur6va1TTpF26x2Rt0RlbQu5rqQOw04sA0fCJza075/ko2T7Eh3HckFbY/3jiR7tguEX94zjyQNTZKN6Iq4T1fVZ1vz0HZG3RGVtC6G+fiRE4HzgMcmWZXkIOBdwO8muRr43TZOVV0OnAxcAXwBOKSq1rSuXgN8lO6ak2uAzw8rZkkCaDuOHwOurKr39vzkzqikBWXDYXVcVQdM89Ozppn+SODIKdpXALsOMDRJmslTgT8CLk1ySWt7K93O58ltx/R7wIuh2xlNMrEzejf33xk9DngQ3Y6oO6OSBmZohZwkjaqqOpepr28Dd0YlLSAL5a5VSZIkzdKMhVySFyfZtA3/VZLPJtl9+KFJ0vozh0kaZ/0ckfvrqrojydOAvekegnn0cMOSpIExh0kaW/0UchMX7D4POLqqTgUeOLyQJGmgzGGSxlY/hdwNST4MvAQ4PcnGfc4nSQuBOUzS2Oonmb0E+CKwT1XdBmwB/MVQo5KkwTGHSRpbMxZyVfVTuqeXP6013Q1cPcygJGlQzGGSxlk/d62+HXgLcHhr2gj41DCDkqRBMYdJGmf9nFp9IfAC4CcAVXUjsOkwg5KkATKHSRpb/RRyv6iqAgogyUOGG5IkDZQ5TNLY6qeQO7nd8fXwJK8Gvgx8ZLhhSdLAmMMkja0Z37VaVf+U5HeB24HHAn9TVWcMPTJJGgBzmKRxNmMhB9CSnolP0kgyh0kaV9MWcknuoF1TMvknoKpqs6FFJUnryRwmaTGYtpCrKu/qkjSyzGGSFoO+Tq0m2Z3uYZoFnFtVFw81KkkaIHOYpHHVzwOB/wY4HngEsCVwXJK/GnZgkjQI5jBJ46yfI3IHAE+sqp8DJHkXcBHwjmEGJkkDYg6TNLb6eY7cdcAmPeMbA9cMJRpJGrzrMIdJGlP9HJG7C7g8yRl015f8LnBukqMAqup1Q4xPktaXOUzS2OqnkDulfSacNZxQJGkozGGSxlY/b3Y4fi4CkaRhMIdJGmf93LX6/CQXJ7k1ye1J7khy+1wEJ0nryxwmaZz1c2r1/cDvA5dW1VRPSZekhcwcJmls9XPX6vXAZSZASSPKHCZpbPVzRO4vgdOTnE139xcAVfXeoUUlSYNjDpM0tvop5I4E7qR7DtMDhxuOJA2cOUzS2OqnkNuiqp499EgkaTjMYZLGVj/XyH05yUCTYJI3JLk8yWVJTkyySZItkpyR5Or2vXnP9IcnWZnkqiR7DzIWSWNv4DlMkhaKfgq5Q4AvJPnZIG7dT7Id8DpgeVXtCmwA7A8cBpxZVTsBZ7Zxkuzcft8F2Af4YJIN1nX5khadgeYwSVpIZizkqmrTqnpAVT2oqjZr45ut53I3BB6UZEPgwcCNwL7AxIM7jwf2a8P7AidV1V1VdS2wEthjPZcvaZEYUg6TpAWhn2vkaKc5d6LnxdNVdc66LLCqbkjyT8D3gJ8BX6qqLyXZuqpuatPclGSrNst2wDd6uljV2iSpL4PMYZK0kMxYyCX5Y+BQYClwCbAncB7wzHVZYEuo+wI7ArcB/5bkZWubZYq2KZ8HleRg4GCAHXbYYV3CkzRmBp3DJGkh6ecauUOBJwPfrapnAE8EVq/HMn8HuLaqVlfVL4HPAr8J3JxkG4D2fUubfhWwfc/8S+lOxd5PVR1TVcuravmSJUvWI0RJY2TQOUySFox+CrmfV9XPAZJsXFX/DTx2PZb5PWDPJA9OEuBZwJXAacCBbZoDgVPb8GnA/kk2TrIj3emRC9Zj+ZIWl0HnMElaMPq5Rm5VkocD/wGckeRHTHNErB9VdX6SfwcuAu4GLgaOAR4KnJzkILpi78Vt+suTnAxc0aY/pKrWrOvyJS06A81hkrSQZDavH0zy28DDgC9U1S+GFtUALF++vFasWNHfxJnqMrwB8fWO0pxIcmFVLZ9hmpHIYbPKX5jCpHHQTw6byoynVpM8OsnGE6PAMrpHhkjSgmcOkzTO+rlG7jPAmiSPAT5Gd7fpCUONSpIGZ51yWJJjk9yS5LKetiOS3JDkkvZ5bs9vU76BJsmTklzafjuqXRssSQPRTyF3T1XdDbwQeH9VvQHYZrhhSdLArGsOO47ubTKTva+qdmuf02HGN9AcTfdYpJ3aZ6o+JWmd9FPI/TLJAXR3kn6utW00vJAkaaDWKYe1Bwbf2ucypnwDTXuU0mZVdV51FyR/gnvfWiNJ662fQu6VwG8AR1bVte0RIJ8abliSNDCDzmGvTfLtdup189a2HXB9zzQTb6DZrg1PbpekgejnXatXVNXrqurENn5tVb1r+KFJ0vobcA47Gng0sBtwE/Ce1j7dG2hm9WaaJCuSrFi92ucVS+pPP0fkJElAVd1cVWuq6h7gI8Ae7afp3kCzqg1Pbp+qb99MI2nWLOQkqU8TrxFsXghM3NE65Rtoquom4I4ke7a7VV/OvW+tkaT1Nm0hl+ST7fvQuQtHkgZjfXNYkhOB84DHJlnV3jrzD+1RIt8GngG8Abo30AATb6D5Avd9A81rgI/S3QBxDfD5dV8rSbqvtb2i60lJHgm8KsknmHStR1X1ezeXJM2H9cphVXXAFM0fW8v0RwJHTtG+Ati1r4glaZbWVsh9iG7P8lHAhdw3CVZrl6SFyhwmaexNe2q1qo6qqscBx1bVo6pqx56PCVDSgmYOk7QYrO2IHABV9ZokTwB+qzWdU1XfHm5YkjQY5jBJ42zGu1aTvA74NLBV+3w6yZ8POzBJGgRzmKRxNuMROeCPgadU1U8Akryb7k6ufx5mYJI0IOYwSWOrn+fIBVjTM76GqZ9WLkkLkTlM0tjq54jcx4Hzk5zSxvdjLbfgS9ICYw6TNLb6udnhvUnOAp5Gtxf7yqq6eNiBSdIgmMMkjbN+jshRVRcBFw05FkkaCnOYpHHlu1YlSZJGlIWcJEnSiFprIZdkgyRfnqtgJGmQzGGSxt1aC7mqWgP8NMnD5igeSRoYc5ikcdfPzQ4/By5Ncgbwk4nGqnrd0KKSpMExh0kaW/0Ucv/VPpI0isxhksZWP8+ROz7Jg4AdquqqOYhJkgbGHCZpnM1412qS3wMuAb7QxndLctqwA5OkQTCHSRpn/Tx+5AhgD+A2gKq6BNhxiDFJ0iAdgTlM0pjqp5C7u6p+PKmthhGMJA2BOUzS2OqnkLssyR8AGyTZKck/A19fn4UmeXiSf0/y30muTPIbSbZIckaSq9v35j3TH55kZZKrkuy9PsuWtOgMPIdJ0kLRTyH358AuwF3AicDtwOvXc7n/B/hCVf0a8ATgSuAw4Myq2gk4s42TZGdg/xbDPsAHk2ywnsuXtHgMI4dJ0oLQz12rPwXeluTd3WjdsT4LTLIZ8HTgFa3/XwC/SLIvsFeb7HjgLOAtwL7ASVV1F3BtkpV017uctz5xSFocBp3DJGkh6eeu1ScnuRT4Nt1DNb+V5EnrscxHAauBjye5OMlHkzwE2LqqbgJo31u16bcDru+Zf1Vrk6QZDSGHSdKC0c+p1Y8Bf1ZVy6pqGXAI8PH1WOaGwO7A0VX1RLonrR+2lukzRduUFyonOTjJiiQrVq9evR4hShojg85hkrRg9FPI3VFVX5sYqapzgfU5NbEKWFVV57fxf6cr7G5Osg1A+76lZ/rte+ZfCtw4VcdVdUxVLa+q5UuWLFmPECWNkUHnMElaMKa9Ri7J7m3wgiQfprtIuICX0l2/tk6q6vtJrk/y2PaU9WcBV7TPgcC72vepbZbTgBOSvBfYFtgJuGBdly9pcRhWDpOkhWRtNzu8Z9L423uG1/cZTH8OfDrJA4HvAK+kOzp4cpKDgO8BLwaoqsuTnExX6N0NHFJVa9Zz+ZLG3zBzmCQtCNMWclX1jGEttD1ZffkUPz1rmumPBI4cVjySxs8wc5gkLQRAY+UAABI2SURBVBQzPn4kycOBlwPLeqevqtcNLyxJGgxzmKRxNmMhB5wOfAO4FLhnuOFI0sCZwySNrX4KuU2q6o1Dj0SShsMcJmls9fP4kU8meXWSbdr7ULdIssXQI5OkwTCHSRpb/RyR+wXwj8DbuPdOr6J7Q4MkLXTmMEljq59C7o3AY6rqB8MORpKGwBwmaWz1c2r1cuCnww5EkobEHCZpbPVzRG4NcEmSrwJ3TTR6676kEWEOkzS2+ink/qN9JGkUmcMkja0ZC7mqOn4uApGkYTCHSRpnM14jl+TaJN+Z/JmL4CRpfa1rDktybJJbklzW07ZFkjOSXN2+N+/57fAkK5NclWTvnvYnJbm0/XZUkgx+LSUtVv2cWu19J+omdC+z9xlMkkbFuuaw44APAJ/oaTsMOLOq3pXksDb+liQ7A/sDuwDbAl9O8qtVtQY4GjiY7u0SpwP7AJ9frzWSpGbGI3JV9cOezw1V9X7gmXMQmyStt3XNYVV1DnDrpOZ9gYlTtccD+/W0n1RVd1XVtcBKYI8k2wCbVdV5VVV0ReF+SNKAzHhELsnuPaMPoNu73XRoEUnSAA04h21dVTcBVNVNSbZq7dvRHXGbsKq1/bINT26XpIHo59Tqe3qG7wauA14ylGgkafDmIodNdd1braX9/h0kB9OdgmWHHXYYXGSSxlo/d60+Yy4CkaRhGHAOuznJNu1o3DbALa19FbB9z3RLgRtb+9Ip2qeK8xjgGIDly5dPWexJ0mT9nFrdGPhfwLLe6avq74YXliQNxoBz2GnAgcC72vepPe0nJHkv3c0OOwEXVNWaJHck2RM4H3g58M/ruCqSdD/9nFo9FfgxcCE9T0WXpBGxTjksyYnAXsCWSVYBb6cr4E5OchDwPbo7YKmqy5OcDFxBd/r2kHbHKsBr6O6AfRDd3aresSppYPop5JZW1T5Dj0SShmOdclhVHTDNT8+aZvojgSOnaF8B7Drb5UtSP2Z8/Ajw9SS/PvRIJGk4zGGSxlY/R+SeBrwiybV0pyUCVFU9fqiRSdJgmMMkja1+CrnnDD0KSRoec5iksdXP40e+OxeBSNIwmMMkjbN+rpGTJEnSAmQhJ0mSNKIs5CRJkkaUhZwkSdKIspCTJEkaURZykiRJI2reCrkkGyS5OMnn2vgWSc5IcnX73rxn2sOTrExyVZK95ytmSZKkhWQ+j8gdClzZM34YcGZV7QSc2cZJsjOwP7ALsA/wwSQbzHGskiRJC868FHJJlgLPAz7a07wvcHwbPh7Yr6f9pKq6q6quBVYCe8xVrJIkSQvVfB2Rez/wl8A9PW1bV9VNAO17q9a+HXB9z3SrWpskSdKiNueFXJLnA7dU1YX9zjJFW03T98FJViRZsXr16nWOUZIkaRTMxxG5pwIvSHIdcBLwzCSfAm5Osg1A+76lTb8K2L5n/qXAjVN1XFXHVNXyqlq+ZMmSYcUvSZK0IMx5IVdVh1fV0qpaRncTw1eq6mXAacCBbbIDgVPb8GnA/kk2TrIjsBNwwRyHLUmStOBsON8B9HgXcHKSg4DvAS8GqKrLk5wMXAHcDRxSVWvmL0xJkqSFYV4Luao6CzirDf8QeNY00x0JHDlngUmSJI0A3+wgSZI0oizkJEmSRpSFnCRJ0oiykJMkSRpRFnKSJEkjykJOkiRpRFnISZIkjSgLOUmSpBFlISdJkjSiLOQkSZJGlIWcJEnSiLKQkyRJGlEWcpIkSSPKQk6SJGlEWchJkiSNKAs5SZKkEWUhJ0mSNKIs5CRJkkaUhZwkSdKIspCTJEkaURZykjRLSa5LcmmSS5KsaG1bJDkjydXte/Oe6Q9PsjLJVUn2nr/IJY0bCzlJWjfPqKrdqmp5Gz8MOLOqdgLObOMk2RnYH9gF2Af4YJIN5iNgSePHQk6SBmNf4Pg2fDywX0/7SVV1V1VdC6wE9piH+CSNIQs5SZq9Ar6U5MIkB7e2ravqJoD2vVVr3w64vmfeVa1NktbbhvMdgCSNoKdW1Y1JtgLOSPLfa5k2U7TV/SbqCsKDAXbYYYfBRClp7HlETpJmqapubN+3AKfQnSq9Ock2AO37ljb5KmD7ntmXAjdO0ecxVbW8qpYvWbJkmOFLGiMWcpI0C0kekmTTiWHg2cBlwGnAgW2yA4FT2/BpwP5JNk6yI7ATcMHcRi1pXHlqVZJmZ2vglCTQ5dATquoLSb4JnJzkIOB7wIsBquryJCcDVwB3A4dU1Zr5CV3SuLGQk6RZqKrvAE+Yov2HwLOmmedI4MghhyZpEZrzU6tJtk/y1SRXJrk8yaGt3YdpSpIkzcJ8XCN3N/CmqnocsCdwSHtgpg/TlCRJmoU5L+Sq6qaquqgN3wFcSfdMJR+mKUmSNAvzetdqkmXAE4Hz8WGakiRJszJvhVyShwKfAV5fVbevbdIp2u73MM3W58FJViRZsXr16kGEKUmStGDNSyGXZCO6Iu7TVfXZ1rxeD9MEH6gpSZIWl/m4azXAx4Arq+q9PT/5ME1JkqRZmI/nyD0V+CPg0iSXtLa3Au/Ch2lKkiT1bc4Luao6l6mvewMfpilJktQ337UqSZI0oizkJEmSRpSFnCRJ0oiykJMkSRpRFnKSJEkjykJOkiRpRFnISZIkjSgLOUmSpBFlISdJkjSi5uMVXYtLpnuJxQBUDa9vSZK04HlETpIkaURZyEmSJI0oCzlJkqQRZSEnSZI0oizkJEmSRpSFnCRJ0oiykJMkSRpRFnKSJEkjykJOkiRpRFnISZIkjSgLOUmSpBFlISdJkjSiLOQkSZJGlIWcJEnSiLKQkyRJGlEWcpIkSSPKQk6SJGlEbTjfAWg9JMPru2p4fUsaGaYZaWHziJwkSdKIGplCLsk+Sa5KsjLJYfMdjyT1y/wlaVhGopBLsgHwL8BzgJ2BA5LsPL9RjblkeB9pETF/SRqmkSjkgD2AlVX1nar6BXASsO88xyRJ/TB/SRqaUbnZYTvg+p7xVcBT5ikWra9hH5XzCuq55dXwMzF/TWNUD9CPx5+lxsWoFHJT/XO/3z+lJAcDB7fRO5Nc1Wf/WwI/WMfYBmmhxAELJ5bZxzG8/zsslG0CCyeW4cYxu/+Wjx1WGOtpseSvuTLv6zvHBei8r+88WGzrPLG+j1yXmUelkFsFbN8zvhS4cfJEVXUMcMxsO0+yoqqWr3t4g7FQ4oCFE8tCiQOMZSHHAV0s8x3DNBZF/porru/4W2zrvL7rOyrXyH0T2CnJjkkeCOwPnDbPMUlSP8xfkoZmJI7IVdXdSV4LfBHYADi2qi6f57AkaUbmL0nDNBKFHEBVnQ6cPqTuZ306Y0gWShywcGJZKHGAsUxlocQBCyuW+1gk+WuuuL7jb7Gt83qtb8rbbyRJkkbSqFwjJ0mSpEkWdSE336/NSXJdkkuTXDJxx12SLZKckeTq9r35EJZ7bJJbklzW0zbtcpMc3rbRVUn2noNYjkhyQ9sulyR57rBjSbJ9kq8muTLJ5UkObe1zvl3WEsucbpckmyS5IMm3Whx/29rnY5tMF8uc/60sFPOdv4ZlIeWnubCQcs9cWEh5ZS4l2SDJxUk+18YHt75VtSg/dBcdXwM8Cngg8C1g5zmO4Tpgy0lt/wAc1oYPA949hOU+HdgduGym5dK9UuhbwMbAjm2bbTDkWI4A3jzFtEOLBdgG2L0Nbwr8v7a8Od8ua4llTrcL3fPPHtqGNwLOB/acp20yXSxz/reyED4LIX8Ncd0WTH6ao/VdMLlnjtZ3weSVOV7vNwInAJ9r4wNb38V8RG6hvjZnX+D4Nnw8sN+gF1BV5wC39rncfYGTququqroWWEm37YYZy3SGFktV3VRVF7XhO4Ar6Z7IP+fbZS2xTGcosVTnzja6UfsU87NNpotlOkP9u10AFmr+Wm8LKT/NhYWUe+bCQsorcyXJUuB5wEd7mge2vou5kJvqtTlr+5/lMBTwpSQXpnuqO8DWVXUTdP/Aga3mKJbpljtf2+m1Sb7dTrNMHHKek1iSLAOeSLenOK/bZVIsMMfbpZ0OuAS4BTijquZtm0wTC8zj38o8Gvf1m2yh5aehWEi5Z5gWUl6ZI+8H/hK4p6dtYOu7mAu5vl6bM2RPrardgecAhyR5+hwvvx/zsZ2OBh4N7AbcBLxnrmJJ8lDgM8Drq+r2tU06D7HM+XapqjVVtRvd2wj2SLLr2kIeVhxriWXe/lbm2bivX7/GZjsspNwzbAsprwxbkucDt1TVhf3OMkXbWtd3MRdyfb02Z5iq6sb2fQtwCt3h05uTbAPQvm+Zo3CmW+6cb6equrn9Q78H+Aj3HlYeaixJNqJLpJ+uqs+25nnZLlPFMl/bpS37NuAsYB/m+W+lN5b53CbzbNzXb7IFk5+GYSHlnrm0kPLKED0VeEGS6+gugXhmkk8xwPVdzIXcvL42J8lDkmw6MQw8G7isxXBgm+xA4NQ5Cmm65Z4G7J9k4yQ7AjsBFwwzkIk/7uaFdNtlqLEkCfAx4Mqqem/PT3O+XaaLZa63S5IlSR7ehh8E/A7w38zPNpkylvn4W1kgFttrvxZMfhq0hZR75sJCyitzoaoOr6qlVbWM7t/pV6rqZQxyfdfl7otx+QDPpbtD6BrgbXO87EfR3ZnyLeDyieUDjwDOBK5u31sMYdkn0p2G+iVd9X/Q2pYLvK1to6uA58xBLJ8ELgW+3f6otxl2LMDT6A5ffxu4pH2eOx/bZS2xzOl2AR4PXNyWdxnwNzP9jQ5xm0wXy5z/rSyUz3zmryGv14LJT3O0vgsm98zR+i6YvDIP674X9961OrD19c0OkiRJI2oxn1qVJEkaaRZykiRJI8pCTpIkaURZyEmSJI0oCzlJkqQRZSGnGSW5c+apZt3nbkme2zN+RJI3r0d/L05yZZKvDibCdY7juiRbzmcMku5l/ppVHOavEWQhp/myG92zkgblIODPquoZA+xTkqZi/tKCYSGnWUnyF0m+2V5S/retbVnbm/xIksuTfKk9sZskT27TnpfkH5Nc1p5E/3fAS5NckuSlrfudk5yV5DtJXjfN8g9Icmnr592t7W/oHqr5oST/OGn6bZKc05ZzWZLfau1HJ1nR4v3bnumvS/L3Ld4VSXZP8sUk1yT50zbNXq3PU5JckeRDSe73bynJy5Jc0Jb94XQvit4gyXEtlkuTvGE9/5NI6pP5y/w1lub7Scd+Fv4HuLN9Pxs4hu6lvg8APgc8HVgG3A3s1qY7GXhZG74M+M02/C7gsjb8CuADPcs4Avg6sDGwJfBDYKNJcWwLfA9YAmwIfAXYr/12FrB8itjfxL1vzdgA2LQNb9HTdhbw+DZ+HfCaNvw+uqePb9qWeUtr3wv4Od3bOTYAzgBe1DP/lsDjgP+cWAfgg8DLgScBZ/TE9/D5/u/rx884f8xf5q9x/3hETrPx7Pa5GLgI+DW698ABXFtVl7ThC4Fl6d6nt2lVfb21nzBD//9VVXdV1Q/oXiC89aTfnwycVVWrq+pu4NN0iXhtvgm8MskRwK9X1R2t/SVJLmrrsguwc888E++svBQ4v6ruqKrVwM/bOgFcUFXfqao1dK8Uetqk5T6LLul9M8klbfxRwHeARyX55yT7ALfPEL+kwTB/mb/G0obzHYBGSoB3VtWH79OYLAPu6mlaAzyoTT8bk/uY/Pc52/6oqnOSPB14HvDJduria8CbgSdX1Y+SHAdsMkUc90yK6Z6emCa/227yeIDjq+rwyTEleQKwN3AI8BLgVbNdL0mzZv4yf40lj8hpNr4IvCrJQwGSbJdkq+kmrqofAXck2bM17d/z8x10h/xn43zgt5NsmWQD4ADg7LXNkOSRdKcUPgJ8DNgd2Az4CfDjJFsDz5llHAB7JNmxXVvyUuDcSb+fCbxoYvsk2SLJI9PdEfaAqvoM8NctHknDZ/66l/lrjHhETn2rqi8leRxwXhKAO4GX0e19Tucg4CNJfkJ3LcePW/tXgcPaYft39rn8m5Ic3uYNcHpVnTrDbHsBf5Hkly3el1fVtUkuBi6nO1Xwf/tZ/iTn0V0z8+vAOcApk2K9IslfAV9qyfKXdHuwPwM+3nNx8f32eCUNnvnrPsxfYyRVk4+oSoOT5KFVdWcbPgzYpqoOneew1kuSvYA3V9Xz5zsWScNj/tIo8Iichu15bS90Q+C7dHd7SdIoMH9pwfOInCRJ0ojyZgdJkqQRZSEnSZI0oizkJEmSRpSFnCRJ0oiykJMkSRpRFnKSJEkj6v8DL83Uglip/noAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
    "text_len = train_data[train_data['label']==1]['tokenized'].map(lambda x: len(x))\n",
    "ax1.hist(text_len, color='red')\n",
    "ax1.set_title('negative Reviews')\n",
    "ax1.set_xlabel('length of samples')\n",
    "ax1.set_ylabel('number of samples')\n",
    "print('부정 리뷰의 평균 길이 :', np.mean(text_len))\n",
    "\n",
    "text_len = train_data[train_data['label']==0]['tokenized'].map(lambda x: len(x))\n",
    "ax2.hist(text_len, color='blue')\n",
    "ax2.set_title('positive Reviews')\n",
    "fig.suptitle('Words in texts')\n",
    "ax2.set_xlabel('length of samples')\n",
    "ax2.set_ylabel('number of samples')\n",
    "print('긍정 리뷰의 평균 길이 :', np.mean(text_len))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:56.276189Z",
     "start_time": "2021-04-30T04:14:56.242281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4356,), (4356,), (1453,), (1453,))"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_data['tokenized'].values\n",
    "y_train = train_data['label'].values\n",
    "X_test= test_data['tokenized'].values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:57.905833Z",
     "start_time": "2021-04-30T04:14:56.721997Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "loaded_model = gensim.models.Word2Vec.load(\"aihub_review_6.model\") # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:57.995594Z",
     "start_time": "2021-04-30T04:14:57.908825Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-215-cfb6e38cd13b>:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  loaded_model.similar_by_word(\"ㅆㅂ\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('시바', 0.8410485982894897),\n",
       " ('시발', 0.8202505111694336),\n",
       " ('ㅆ발', 0.7903104424476624),\n",
       " ('ㅆㅍ', 0.7833770513534546),\n",
       " ('아오', 0.782518208026886),\n",
       " ('슈바', 0.7626177072525024),\n",
       " ('시밤', 0.7533204555511475),\n",
       " ('젠장', 0.7490739226341248),\n",
       " ('어휴', 0.7402772903442383),\n",
       " ('ㅆㅃ', 0.7374851703643799)]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.similar_by_word(\"ㅆㅂ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:58.010553Z",
     "start_time": "2021-04-30T04:14:58.000581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어의 수 :  87507\n"
     ]
    }
   ],
   "source": [
    "print(\"단어의 수 : \", len(loaded_model.wv.vocab))\n",
    "vocab_len = len(loaded_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:58.025513Z",
     "start_time": "2021-04-30T04:14:58.015540Z"
    }
   },
   "outputs": [],
   "source": [
    "def embedding(word):\n",
    "    if word in loaded_model.wv.vocab:\n",
    "        return loaded_model.wv[word]\n",
    "    else:\n",
    "        return np.random.normal(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:58.316734Z",
     "start_time": "2021-04-30T04:14:58.028505Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = [[embedding(word) for word in sentence] for sentence in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:16:22.074695Z",
     "start_time": "2021-04-30T04:16:21.724631Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.array([sum(words) / len(words) for words in X_train if  len(words)>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:16:22.684065Z",
     "start_time": "2021-04-30T04:16:22.673095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4356"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:14:59.620251Z",
     "start_time": "2021-04-30T04:14:59.601303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4356, 100)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:15:00.642518Z",
     "start_time": "2021-04-30T04:15:00.565724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21186052,  0.10753814,  0.67194259,  0.12835659,  0.12043203,\n",
       "        0.27579945,  0.45969378, -0.06918506, -0.03746542,  0.98649285,\n",
       "       -0.41998711,  1.21723735, -0.44302732,  0.30692644,  0.06787027,\n",
       "       -0.47605436,  0.00962376,  0.53227388,  0.6499727 ,  0.9043433 ,\n",
       "        0.05952054,  0.60590941,  0.0719095 ,  0.3747965 ,  0.5090032 ,\n",
       "       -0.12980861,  0.00510725, -0.66492552,  0.15397565, -0.03550625,\n",
       "        0.59130505,  0.75604099,  0.66600017, -0.33362395, -0.3944354 ,\n",
       "        0.18829417,  0.88376867,  0.04366343, -0.12906863, -0.55484822,\n",
       "       -0.10339615,  0.26320377, -0.32464736,  0.12481281,  0.12937014,\n",
       "        0.29035448, -0.3486104 ,  0.0545519 , -0.09689865, -0.43462767,\n",
       "        0.13063895, -1.04507958,  0.31071289, -0.07513076, -0.44898524,\n",
       "       -0.37257672,  0.43866389, -0.4176166 ,  0.09836004, -0.3511077 ,\n",
       "       -0.32713138, -0.2668603 ,  0.40655185,  0.11811819,  0.61070237,\n",
       "       -0.13244067, -0.6198865 , -0.472692  , -0.63528274, -0.38815785,\n",
       "        0.70870519, -0.5202803 ,  1.38047291, -0.77355558, -0.16543184,\n",
       "       -0.72278557, -0.55774633, -0.57589117,  0.19910099, -0.53721732,\n",
       "       -0.44107746, -0.73459874,  0.82868276,  0.41042033, -0.68973495,\n",
       "       -0.42427132,  0.37486313,  0.01770353, -0.05871369, -0.08583065,\n",
       "       -0.16216526, -0.47406272,  0.68720156, -0.75846886,  0.40120195,\n",
       "       -0.23516653,  0.0282768 ,  0.26053763, -0.68768008,  0.01062934])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X_scaled = preprocessing.scale(X_train_mean)\n",
    "X_scaled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:16:36.784437Z",
     "start_time": "2021-04-30T04:16:36.699150Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = [[embedding(word) for word in sentence] for sentence in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:16:40.086610Z",
     "start_time": "2021-04-30T04:16:40.047713Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = np.array([sum(words) / len(words) for words in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:16:44.636749Z",
     "start_time": "2021-04-30T04:16:44.624770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1453, 100)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:16:47.840178Z",
     "start_time": "2021-04-30T04:16:46.937045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 100\n",
      "리뷰의 평균 길이 : 100.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(l) for l in X_train_mean))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train_mean))/len(X_train_mean))\n",
    "plt.hist([len(s) for s in X_train_mean], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:16:49.551758Z",
     "start_time": "2021-04-30T04:16:49.531813Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional, Dropout,BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:41:01.128699Z",
     "start_time": "2021-04-30T04:41:01.121719Z"
    }
   },
   "outputs": [],
   "source": [
    "def DNN():\n",
    "    global loaded_model\n",
    "    # 모델 구조 정의하기\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu')) \n",
    "    model.add(layers.Dense(128, activation='relu')) #ReLU 활성화함수 채택\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train, y_train, epochs=100, callbacks=[es, mc], batch_size=32, validation_split=0.2)\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    score = loaded_model.evaluate(X_test, y_test)[1]\n",
    "    print(\"테스트 정확도: %.4f\" % (score))\n",
    "    y_pred = loaded_model.predict(X_test)\n",
    "    predicted = [round(float(pred),0) for pred in y_pred]\n",
    "    print(classification_report(y_test,predicted))\n",
    "#     test_result.append(('DNN',score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:41:08.590198Z",
     "start_time": "2021-04-30T04:41:01.465269Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "101/109 [==========================>...] - ETA: 0s - loss: 0.5343 - acc: 0.7280\n",
      "Epoch 00001: val_acc improved from -inf to 0.75459, saving model to best_model.h5\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 0.5337 - acc: 0.7296 - val_loss: 0.4819 - val_acc: 0.7546\n",
      "Epoch 2/100\n",
      "104/109 [===========================>..] - ETA: 0s - loss: 0.4430 - acc: 0.8017\n",
      "Epoch 00002: val_acc improved from 0.75459 to 0.77752, saving model to best_model.h5\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.4427 - acc: 0.8020 - val_loss: 0.4619 - val_acc: 0.7775\n",
      "Epoch 3/100\n",
      " 84/109 [======================>.......] - ETA: 0s - loss: 0.4197 - acc: 0.8136\n",
      "Epoch 00003: val_acc did not improve from 0.77752\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.4171 - acc: 0.8137 - val_loss: 0.5117 - val_acc: 0.7592\n",
      "Epoch 4/100\n",
      " 95/109 [=========================>....] - ETA: 0s - loss: 0.3939 - acc: 0.8276\n",
      "Epoch 00004: val_acc improved from 0.77752 to 0.77982, saving model to best_model.h5\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.3960 - acc: 0.8269 - val_loss: 0.4683 - val_acc: 0.7798\n",
      "Epoch 5/100\n",
      " 89/109 [=======================>......] - ETA: 0s - loss: 0.3766 - acc: 0.8381\n",
      "Epoch 00005: val_acc improved from 0.77982 to 0.78326, saving model to best_model.h5\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.3812 - acc: 0.8347 - val_loss: 0.4768 - val_acc: 0.7833\n",
      "Epoch 6/100\n",
      " 88/109 [=======================>......] - ETA: 0s - loss: 0.3599 - acc: 0.8441\n",
      "Epoch 00006: val_acc improved from 0.78326 to 0.79931, saving model to best_model.h5\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.3668 - acc: 0.8401 - val_loss: 0.4617 - val_acc: 0.7993\n",
      "Epoch 7/100\n",
      "102/109 [===========================>..] - ETA: 0s - loss: 0.3547 - acc: 0.8431\n",
      "Epoch 00007: val_acc did not improve from 0.79931\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.3531 - acc: 0.8433 - val_loss: 0.4758 - val_acc: 0.7959\n",
      "Epoch 8/100\n",
      " 88/109 [=======================>......] - ETA: 0s - loss: 0.3374 - acc: 0.8580\n",
      "Epoch 00008: val_acc did not improve from 0.79931\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.3379 - acc: 0.8559 - val_loss: 0.5373 - val_acc: 0.7764\n",
      "Epoch 9/100\n",
      " 99/109 [==========================>...] - ETA: 0s - loss: 0.3267 - acc: 0.8554\n",
      "Epoch 00009: val_acc did not improve from 0.79931\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.3286 - acc: 0.8559 - val_loss: 0.5133 - val_acc: 0.7821\n",
      "Epoch 10/100\n",
      "101/109 [==========================>...] - ETA: 0s - loss: 0.3115 - acc: 0.8648\n",
      "Epoch 00010: val_acc did not improve from 0.79931\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.3150 - acc: 0.8639 - val_loss: 0.4967 - val_acc: 0.7867\n",
      "Epoch 11/100\n",
      "104/109 [===========================>..] - ETA: 0s - loss: 0.3036 - acc: 0.8717\n",
      "Epoch 00011: val_acc did not improve from 0.79931\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.2986 - acc: 0.8749 - val_loss: 0.5185 - val_acc: 0.7982\n",
      "Epoch 12/100\n",
      " 82/109 [=====================>........] - ETA: 0s - loss: 0.2903 - acc: 0.8788\n",
      "Epoch 00012: val_acc did not improve from 0.79931\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.2869 - acc: 0.8806 - val_loss: 0.5149 - val_acc: 0.7993\n",
      "Epoch 13/100\n",
      " 57/109 [==============>...............] - ETA: 0s - loss: 0.2607 - acc: 0.8947\n",
      "Epoch 00013: val_acc did not improve from 0.79931\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.2729 - acc: 0.8895 - val_loss: 0.5215 - val_acc: 0.7970\n",
      "Epoch 14/100\n",
      " 53/109 [=============>................] - ETA: 0s - loss: 0.2552 - acc: 0.9015\n",
      "Epoch 00014: val_acc improved from 0.79931 to 0.80275, saving model to best_model.h5\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.2580 - acc: 0.8941 - val_loss: 0.5417 - val_acc: 0.8028\n",
      "Epoch 15/100\n",
      "106/109 [============================>.] - ETA: 0s - loss: 0.2495 - acc: 0.8956\n",
      "Epoch 00015: val_acc did not improve from 0.80275\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.2495 - acc: 0.8952 - val_loss: 0.5593 - val_acc: 0.7936\n",
      "Epoch 16/100\n",
      " 98/109 [=========================>....] - ETA: 0s - loss: 0.2420 - acc: 0.8986\n",
      "Epoch 00016: val_acc did not improve from 0.80275\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.2412 - acc: 0.8981 - val_loss: 0.5508 - val_acc: 0.7890\n",
      "Epoch 17/100\n",
      " 95/109 [=========================>....] - ETA: 0s - loss: 0.2240 - acc: 0.9105\n",
      "Epoch 00017: val_acc did not improve from 0.80275\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.2238 - acc: 0.9127 - val_loss: 0.5685 - val_acc: 0.7947\n",
      "Epoch 18/100\n",
      " 96/109 [=========================>....] - ETA: 0s - loss: 0.2072 - acc: 0.9157\n",
      "Epoch 00018: val_acc did not improve from 0.80275\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.2096 - acc: 0.9159 - val_loss: 0.5868 - val_acc: 0.7924\n",
      "Epoch 19/100\n",
      "103/109 [===========================>..] - ETA: 0s - loss: 0.1981 - acc: 0.9214\n",
      "Epoch 00019: val_acc improved from 0.80275 to 0.80390, saving model to best_model.h5\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.2021 - acc: 0.9205 - val_loss: 0.6149 - val_acc: 0.8039\n",
      "Epoch 20/100\n",
      "102/109 [===========================>..] - ETA: 0s - loss: 0.1832 - acc: 0.9311\n",
      "Epoch 00020: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.1879 - acc: 0.9288 - val_loss: 0.6561 - val_acc: 0.7959\n",
      "Epoch 21/100\n",
      "102/109 [===========================>..] - ETA: 0s - loss: 0.1771 - acc: 0.9421\n",
      "Epoch 00021: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.1794 - acc: 0.9400 - val_loss: 0.7221 - val_acc: 0.7867\n",
      "Epoch 22/100\n",
      "104/109 [===========================>..] - ETA: 0s - loss: 0.1637 - acc: 0.9405\n",
      "Epoch 00022: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.1631 - acc: 0.9409 - val_loss: 0.7045 - val_acc: 0.7924\n",
      "Epoch 23/100\n",
      "108/109 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9410\n",
      "Epoch 00023: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.1567 - acc: 0.9406 - val_loss: 0.7134 - val_acc: 0.7993\n",
      "Epoch 24/100\n",
      " 88/109 [=======================>......] - ETA: 0s - loss: 0.1413 - acc: 0.9460\n",
      "Epoch 00024: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.1401 - acc: 0.9480 - val_loss: 0.8141 - val_acc: 0.7706\n",
      "Epoch 25/100\n",
      " 92/109 [========================>.....] - ETA: 0s - loss: 0.1297 - acc: 0.9538\n",
      "Epoch 00025: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.1316 - acc: 0.9535 - val_loss: 0.7885 - val_acc: 0.7844\n",
      "Epoch 26/100\n",
      " 88/109 [=======================>......] - ETA: 0s - loss: 0.1164 - acc: 0.9570\n",
      "Epoch 00026: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.1180 - acc: 0.9590 - val_loss: 0.9195 - val_acc: 0.7833\n",
      "Epoch 27/100\n",
      " 79/109 [====================>.........] - ETA: 0s - loss: 0.1073 - acc: 0.9612\n",
      "Epoch 00027: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.1110 - acc: 0.9604 - val_loss: 0.8810 - val_acc: 0.7844\n",
      "Epoch 28/100\n",
      "107/109 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9623\n",
      "Epoch 00028: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.1068 - acc: 0.9624 - val_loss: 1.0226 - val_acc: 0.7764\n",
      "Epoch 29/100\n",
      " 56/109 [==============>...............] - ETA: 0s - loss: 0.0956 - acc: 0.9643\n",
      "Epoch 00029: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.0990 - acc: 0.9653 - val_loss: 1.0695 - val_acc: 0.7741\n",
      "Epoch 30/100\n",
      " 94/109 [========================>.....] - ETA: 0s - loss: 0.0835 - acc: 0.9721\n",
      "Epoch 00030: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.0879 - acc: 0.9707 - val_loss: 1.0016 - val_acc: 0.7856\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/109 [===========================>..] - ETA: 0s - loss: 0.0823 - acc: 0.9721\n",
      "Epoch 00031: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.0805 - acc: 0.9724 - val_loss: 1.0778 - val_acc: 0.7810\n",
      "Epoch 32/100\n",
      " 74/109 [===================>..........] - ETA: 0s - loss: 0.0866 - acc: 0.9688\n",
      "Epoch 00032: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.0797 - acc: 0.9699 - val_loss: 1.2338 - val_acc: 0.7787\n",
      "Epoch 33/100\n",
      " 92/109 [========================>.....] - ETA: 0s - loss: 0.0683 - acc: 0.9806\n",
      "Epoch 00033: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.0678 - acc: 0.9793 - val_loss: 1.1859 - val_acc: 0.7546\n",
      "Epoch 34/100\n",
      " 57/109 [==============>...............] - ETA: 0s - loss: 0.0598 - acc: 0.9814\n",
      "Epoch 00034: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.0706 - acc: 0.9776 - val_loss: 1.1337 - val_acc: 0.7775\n",
      "Epoch 35/100\n",
      " 92/109 [========================>.....] - ETA: 0s - loss: 0.0528 - acc: 0.9837\n",
      "Epoch 00035: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.0559 - acc: 0.9834 - val_loss: 1.2071 - val_acc: 0.7890\n",
      "Epoch 36/100\n",
      " 96/109 [=========================>....] - ETA: 0s - loss: 0.0555 - acc: 0.9827\n",
      "Epoch 00036: val_acc did not improve from 0.80390\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.0590 - acc: 0.9813 - val_loss: 1.3467 - val_acc: 0.7810\n",
      "Epoch 00036: early stopping\n",
      "46/46 [==============================] - 0s 542us/step - loss: 0.6194 - acc: 0.7873\n",
      "테스트 정확도: 0.7873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       948\n",
      "           1       0.71      0.66      0.68       505\n",
      "\n",
      "    accuracy                           0.79      1453\n",
      "   macro avg       0.77      0.76      0.76      1453\n",
      "weighted avg       0.78      0.79      0.79      1453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 랜포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:54:56.291906Z",
     "start_time": "2021-04-30T04:54:56.278915Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def model_evaluation(label, predict):\n",
    "    cf_matrix = confusion_matrix(label, predict)\n",
    "    Accuracy = (cf_matrix[0][0] + cf_matrix[1][1]) / sum(sum(cf_matrix))\n",
    "    Precision = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[0][1])\n",
    "    Recall = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[1][0])\n",
    "    Specificity = cf_matrix[0][0] / (cf_matrix[0][0] + cf_matrix[0][1])\n",
    "    F1_Score = (2 * Recall * Precision) / (Recall + Precision)\n",
    "    F2_Score = (5 * Recall * Precision) / (Recall + 4*Precision)\n",
    "    \n",
    "    print(\"Accuracy: \", Accuracy)\n",
    "    print(\"Precision: \", Precision)\n",
    "    print(\"Recall: \", Recall)\n",
    "    print(\"Specificity: \", Specificity)\n",
    "    print(\"F1-Score: \", F1_Score)\n",
    "    print(\"F2-Score: \", F2_Score)\n",
    "    print(\"auc score: \" , roc_auc_score(label, np.round(predict,0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:33:45.823590Z",
     "start_time": "2021-04-30T04:33:44.600305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 자체 정확도: 0.776\n",
      "테스트 정확도: 0.778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.96      0.85       948\n",
      "           1       0.84      0.45      0.58       505\n",
      "\n",
      "    accuracy                           0.78      1453\n",
      "   macro avg       0.80      0.70      0.72      1453\n",
      "weighted avg       0.79      0.78      0.76      1453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "rf = RandomForestClassifier( n_estimators = 100,oob_score=True, n_jobs = -1, random_state = 0)\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "predicted = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "\n",
    "print(f'모델 자체 정확도: {rf.oob_score_:.3}')\n",
    "print(f'테스트 정확도: {accuracy:.3}')\n",
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T05:27:48.006056Z",
     "start_time": "2021-04-30T05:22:53.512420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 하이퍼 파라미터:  {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-309-c8502e85856e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'최적 하이퍼 파라미터: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_x' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\n",
    "#rf.fit(X_train, y_train)\n",
    "\n",
    "#최적 파라미터 값 찾기\n",
    "params = { 'n_estimators' : [10, 100,200],\n",
    "           'max_depth' : [6, 8, 10],\n",
    "           'min_samples_leaf' : [3,5 ,7],\n",
    "           'min_samples_split' : [2,3,6]}\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state = 0, n_jobs = -1)\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0) #업샘플링할때\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0) #업샘플링 하지 않았을때\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid = params, cv = kfold, n_jobs = -1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T05:29:51.063232Z",
     "start_time": "2021-04-30T05:29:50.847809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[892  56]\n",
      " [267 238]]\n",
      "Accuracy:  0.7777013076393668\n",
      "Precision:  0.8095238095238095\n",
      "Recall:  0.47128712871287126\n",
      "Specificity:  0.9409282700421941\n",
      "F1-Score:  0.5957446808510638\n",
      "F2-Score:  0.5142610198789974\n",
      "auc score:  0.7061076993775327\n",
      "None\n",
      "최고 예측 정확도: 0.7796\n",
      "========================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predicted = grid_cv.predict(X_test)\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "print(model_evaluation(y_test, predicted))\n",
    "print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))\n",
    "print('='*40)\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T05:02:29.142150Z",
     "start_time": "2021-04-30T05:02:29.134172Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "#최적 파라미터 값 찾기\n",
    "param_grid = [ { 'C' : [0.1, 1, 10 ], 'kernel': [ 'rbf' ], 'gamma' : [ 1,0.1 ]},\n",
    "    {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "                     ]\n",
    "\n",
    "def svm_model(train_x, train_y,test_x, test_y,cv):\n",
    "    print(\"데이터셋 : \", train_x ,\" &  cv: \" , cv)\n",
    "    grid_search = GridSearchCV(SVC(),param_grid, cv=cv, return_train_score = True)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    print(\"best parameters : {}\".format(grid_search.best_params_))\n",
    "    predicted = grid_search.predict(test_x)\n",
    "    print(confusion_matrix(test_y, predicted))\n",
    "    print(model_evaluation(test_y, predicted))\n",
    "    print('최고 예측 정확도: {:.4f}'.format(grid_search.best_score_))\n",
    "    print('='*40)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T05:11:33.023480Z",
     "start_time": "2021-04-30T05:02:31.701311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 :  [[-0.04661505  0.16993053  0.27061811 ... -0.09436165  0.14314945\n",
      "  -0.14541295]\n",
      " [-0.07975016  0.136363    0.20714115 ... -0.15305319  0.34772229\n",
      "  -0.13613091]\n",
      " [ 0.08952134  0.29716524  0.17057715 ... -0.07201694  0.23210075\n",
      "  -0.1223828 ]\n",
      " ...\n",
      " [-0.27093104  0.17475013  0.17555496 ... -0.16118459  0.38636994\n",
      "  -0.15391727]\n",
      " [ 0.00759386  0.29934341  0.09445645 ... -0.23899582  0.30256429\n",
      "  -0.05873086]\n",
      " [ 0.01223171  0.13627512 -0.03923143 ... -0.15203705  0.11687265\n",
      "   0.0436728 ]]  &  cv:  3\n",
      "best parameters : {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "[[849  99]\n",
      " [218 287]]\n",
      "Accuracy:  0.7818306951135582\n",
      "Precision:  0.7435233160621761\n",
      "Recall:  0.5683168316831683\n",
      "Specificity:  0.8955696202531646\n",
      "F1-Score:  0.6442199775533108\n",
      "F2-Score:  0.5964256026600167\n",
      "auc score:  0.7319432259681665\n",
      "None\n",
      "최고 예측 정확도: 0.8021\n",
      "========================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_model(X_train, y_train,X_test, y_test,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\n",
    "#rf.fit(X_train, y_train)\n",
    "\n",
    "#최적 파라미터 값 찾기\n",
    "params = { 'n_estimators' : [10, 100,200],\n",
    "           'max_depth' : [6, 8, 10],\n",
    "           'min_samples_leaf' : [3,5 ,7],\n",
    "           'min_samples_split' : [2,3,6]}\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state = 0, n_jobs = -1)\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0) #업샘플링할때\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0) #업샘플링 하지 않았을때\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid = params, cv = kfold, n_jobs = -1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
    "print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:53:21.759642Z",
     "start_time": "2021-04-30T04:53:21.728755Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-297-6fc2f4ed6a2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m             }\n\u001b[0;32m      3\u001b[0m \u001b[0mrf_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgrid_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'roc_auc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mgrid_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "params = { 'n_estimators' : [10,20,30,40,100],\n",
    "            }\n",
    "rf_clf = RandomForestClassifier(random_state = 0, n_jobs = -1)\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid = params, cv = 3, n_jobs = -1, scoring='roc_auc')\n",
    "\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))\n",
    "\n",
    "pred = grid_cv.predict(X_test)\n",
    "y_pred.append(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T04:09:08.071123Z",
     "start_time": "2021-04-30T03:56:54.911030Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-8b8c6cdc2087>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mDNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-151-1054f4d53418>\u001b[0m in \u001b[0;36mDNN\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mmc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1084\u001b[0m       data_handler._initial_epoch = (  # pylint: disable=protected-access\n\u001b[0;32m   1085\u001b[0m           self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n\u001b[1;32m-> 1086\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1087\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1136\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1138\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1139\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m     \"\"\"\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec, job_token)\u001b[0m\n\u001b[0;32m    694\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    700\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[1;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mautotune\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m       \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ModelDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcpu_budget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[1;31m# (4) Apply stats aggregator options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, algorithm, cpu_budget)\u001b[0m\n\u001b[0;32m   4379\u001b[0m         \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4380\u001b[0m         \u001b[0mcpu_budget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcpu_budget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4381\u001b[1;33m         **self._flat_structure)\n\u001b[0m\u001b[0;32m   4382\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ModelDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_flat_structure\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \"\"\"\n\u001b[0;32m    560\u001b[0m     return {\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[1;34m\"output_shapes\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m     }\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_flat_shapes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    533\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShapes\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0melement\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mrepresentation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m     \"\"\"\n\u001b[1;32m--> 535\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_flat_tensor_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36mget_flat_tensor_shapes\u001b[1;34m(element_spec)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShapes\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0melement\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mrepresentation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m   \"\"\"\n\u001b[1;32m--> 283\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mspec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mget_flat_tensor_specs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36mget_flat_tensor_specs\u001b[1;34m(element_spec)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m   return functools.reduce(lambda state, value: state + value._flat_tensor_specs,\n\u001b[0m\u001b[0;32m    270\u001b[0m                           nest.flatten(element_spec), [])\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(state, value)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m   return functools.reduce(lambda state, value: state + value._flat_tensor_specs,\n\u001b[0m\u001b[0;32m    270\u001b[0m                           nest.flatten(element_spec), [])\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\type_spec.py\u001b[0m in \u001b[0;36m_flat_tensor_specs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         self._component_specs, tensor_list, expand_composites=True))\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_flat_tensor_specs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;34m\"\"\"A list of TensorSpecs compatible with self._to_tensor_list(v).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T01:43:01.960624Z",
     "start_time": "2021-04-30T01:43:01.950652Z"
    }
   },
   "outputs": [],
   "source": [
    "def DNN():\n",
    "    # 모델 구조 정의하기\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation='softmax')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(layers.Dense(128, activation='softmax')) #ReLU 활성화함수 채택\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(layers.Dense(128, activation='softmax')) #ReLU 활성화함수 채택\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(layers.Dense(128, activation='softmax')) #ReLU 활성화함수 채택\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train_mean, y_train, epochs=100, callbacks=[mc], batch_size=32, validation_split=0.2)\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    score = loaded_model.evaluate(X_test_mean, y_test)[1]\n",
    "    print(\"테스트 정확도: %.4f\" % (score))\n",
    "#     test_result.append(('DNN',score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T01:44:02.603622Z",
     "start_time": "2021-04-30T01:43:02.557625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.5799 - acc: 0.7023\n",
      "Epoch 00001: val_acc improved from -inf to 0.66934, saving model to best_model.h5\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.5798 - acc: 0.7025 - val_loss: 0.6347 - val_acc: 0.6693\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5006 - acc: 0.7524\n",
      "Epoch 00002: val_acc did not improve from 0.66934\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.5006 - acc: 0.7524 - val_loss: 0.6393 - val_acc: 0.6693\n",
      "Epoch 3/100\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.4931 - acc: 0.7555\n",
      "Epoch 00003: val_acc did not improve from 0.66934\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.4920 - acc: 0.7564 - val_loss: 0.6387 - val_acc: 0.6693\n",
      "Epoch 4/100\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.4970 - acc: 0.7485\n",
      "Epoch 00004: val_acc did not improve from 0.66934\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.4948 - acc: 0.7512 - val_loss: 0.6356 - val_acc: 0.6693\n",
      "Epoch 5/100\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.4864 - acc: 0.7609\n",
      "Epoch 00005: val_acc did not improve from 0.66934\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.4860 - acc: 0.7612 - val_loss: 0.6267 - val_acc: 0.6693\n",
      "Epoch 6/100\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 0.4905 - acc: 0.7539\n",
      "Epoch 00006: val_acc improved from 0.66934 to 0.67620, saving model to best_model.h5\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.4877 - acc: 0.7561 - val_loss: 0.6064 - val_acc: 0.6762\n",
      "Epoch 7/100\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 0.4815 - acc: 0.7607\n",
      "Epoch 00007: val_acc improved from 0.67620 to 0.68421, saving model to best_model.h5\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.4812 - acc: 0.7604 - val_loss: 0.6375 - val_acc: 0.6842\n",
      "Epoch 8/100\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 0.4666 - acc: 0.7677\n",
      "Epoch 00008: val_acc did not improve from 0.68421\n",
      "110/110 [==============================] - 0s 5ms/step - loss: 0.4672 - acc: 0.7678 - val_loss: 0.6440 - val_acc: 0.6648\n",
      "Epoch 9/100\n",
      " 93/110 [========================>.....] - ETA: 0s - loss: 0.4656 - acc: 0.7728\n",
      "Epoch 00009: val_acc did not improve from 0.68421\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4662 - acc: 0.7756 - val_loss: 0.6865 - val_acc: 0.6751\n",
      "Epoch 10/100\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.4600 - acc: 0.7764\n",
      "Epoch 00010: val_acc did not improve from 0.68421\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.4599 - acc: 0.7767 - val_loss: 0.7246 - val_acc: 0.6362\n",
      "Epoch 11/100\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 0.4544 - acc: 0.7819\n",
      "Epoch 00011: val_acc did not improve from 0.68421\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4542 - acc: 0.7821 - val_loss: 0.7362 - val_acc: 0.6773\n",
      "Epoch 12/100\n",
      " 94/110 [========================>.....] - ETA: 0s - loss: 0.4535 - acc: 0.7796\n",
      "Epoch 00012: val_acc did not improve from 0.68421\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4536 - acc: 0.7804 - val_loss: 0.6903 - val_acc: 0.6533\n",
      "Epoch 13/100\n",
      " 96/110 [=========================>....] - ETA: 0s - loss: 0.4454 - acc: 0.7865\n",
      "Epoch 00013: val_acc did not improve from 0.68421\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.4487 - acc: 0.7847 - val_loss: 0.6916 - val_acc: 0.6716\n",
      "Epoch 14/100\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 0.4443 - acc: 0.7864\n",
      "Epoch 00014: val_acc did not improve from 0.68421\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.4449 - acc: 0.7833 - val_loss: 0.7189 - val_acc: 0.6751\n",
      "Epoch 15/100\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.4384 - acc: 0.7907\n",
      "Epoch 00015: val_acc did not improve from 0.68421\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4382 - acc: 0.7910 - val_loss: 0.7772 - val_acc: 0.6819\n",
      "Epoch 16/100\n",
      " 95/110 [========================>.....] - ETA: 0s - loss: 0.4403 - acc: 0.7951\n",
      "Epoch 00016: val_acc did not improve from 0.68421\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.4380 - acc: 0.7933 - val_loss: 0.7516 - val_acc: 0.6556\n",
      "Epoch 17/100\n",
      " 94/110 [========================>.....] - ETA: 0s - loss: 0.4202 - acc: 0.8035\n",
      "Epoch 00017: val_acc did not improve from 0.68421\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4267 - acc: 0.7976 - val_loss: 0.7958 - val_acc: 0.6041\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4323 - acc: 0.7922\n",
      "Epoch 00018: val_acc did not improve from 0.68421\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4323 - acc: 0.7922 - val_loss: 0.7047 - val_acc: 0.6659\n",
      "Epoch 19/100\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.4250 - acc: 0.7951\n",
      "Epoch 00019: val_acc did not improve from 0.68421\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4256 - acc: 0.7950 - val_loss: 0.7597 - val_acc: 0.6670\n",
      "Epoch 20/100\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.4194 - acc: 0.8030\n",
      "Epoch 00020: val_acc improved from 0.68421 to 0.68879, saving model to best_model.h5\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4207 - acc: 0.8027 - val_loss: 0.7385 - val_acc: 0.6888\n",
      "Epoch 21/100\n",
      " 93/110 [========================>.....] - ETA: 0s - loss: 0.4249 - acc: 0.7940\n",
      "Epoch 00021: val_acc did not improve from 0.68879\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.4210 - acc: 0.7944 - val_loss: 0.7632 - val_acc: 0.6590\n",
      "Epoch 22/100\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.4217 - acc: 0.8001\n",
      "Epoch 00022: val_acc did not improve from 0.68879\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4215 - acc: 0.8005 - val_loss: 0.7361 - val_acc: 0.6648\n",
      "Epoch 23/100\n",
      " 94/110 [========================>.....] - ETA: 0s - loss: 0.4043 - acc: 0.8132\n",
      "Epoch 00023: val_acc improved from 0.68879 to 0.69451, saving model to best_model.h5\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4080 - acc: 0.8122 - val_loss: 0.7480 - val_acc: 0.6945\n",
      "Epoch 24/100\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.3994 - acc: 0.8113\n",
      "Epoch 00024: val_acc did not improve from 0.69451\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3975 - acc: 0.8136 - val_loss: 0.7642 - val_acc: 0.6762\n",
      "Epoch 25/100\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 0.3947 - acc: 0.8093\n",
      "Epoch 00025: val_acc did not improve from 0.69451\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3962 - acc: 0.8099 - val_loss: 0.8332 - val_acc: 0.6510\n",
      "Epoch 26/100\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 0.3890 - acc: 0.8191\n",
      "Epoch 00026: val_acc did not improve from 0.69451\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.3914 - acc: 0.8185 - val_loss: 0.7935 - val_acc: 0.6465\n",
      "Epoch 27/100\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.3906 - acc: 0.8191\n",
      "Epoch 00027: val_acc did not improve from 0.69451\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.3907 - acc: 0.8191 - val_loss: 0.7885 - val_acc: 0.6522\n",
      "Epoch 28/100\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8267\n",
      "Epoch 00028: val_acc did not improve from 0.69451\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.3775 - acc: 0.8262 - val_loss: 0.8856 - val_acc: 0.6190\n",
      "Epoch 29/100\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 0.3814 - acc: 0.8165\n",
      "Epoch 00029: val_acc did not improve from 0.69451\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.3844 - acc: 0.8151 - val_loss: 0.8038 - val_acc: 0.6545\n",
      "Epoch 30/100\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 0.3750 - acc: 0.8347\n",
      "Epoch 00030: val_acc did not improve from 0.69451\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.3735 - acc: 0.8354 - val_loss: 0.8237 - val_acc: 0.6899\n",
      "Epoch 31/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3717 - acc: 0.8277\n",
      "Epoch 00031: val_acc did not improve from 0.69451\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.3717 - acc: 0.8277 - val_loss: 0.8589 - val_acc: 0.6430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8360\n",
      "Epoch 00032: val_acc did not improve from 0.69451\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.3627 - acc: 0.8362 - val_loss: 0.9017 - val_acc: 0.6270\n",
      "Epoch 33/100\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.3628 - acc: 0.8350\n",
      "Epoch 00033: val_acc did not improve from 0.69451\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.3653 - acc: 0.8337 - val_loss: 0.8277 - val_acc: 0.6796\n",
      "Epoch 34/100\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.3620 - acc: 0.8380\n",
      "Epoch 00034: val_acc did not improve from 0.69451\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3617 - acc: 0.8382 - val_loss: 0.8482 - val_acc: 0.6545\n",
      "Epoch 35/100\n",
      " 95/110 [========================>.....] - ETA: 0s - loss: 0.3584 - acc: 0.8355\n",
      "Epoch 00035: val_acc did not improve from 0.69451\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3544 - acc: 0.8385 - val_loss: 0.8563 - val_acc: 0.6751\n",
      "Epoch 36/100\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 0.3544 - acc: 0.8317\n",
      "Epoch 00036: val_acc did not improve from 0.69451\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3530 - acc: 0.8334 - val_loss: 0.8352 - val_acc: 0.6854\n",
      "Epoch 37/100\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8498\n",
      "Epoch 00037: val_acc improved from 0.69451 to 0.69680, saving model to best_model.h5\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3461 - acc: 0.8491 - val_loss: 0.7920 - val_acc: 0.6968\n",
      "Epoch 38/100\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 0.3429 - acc: 0.8459\n",
      "Epoch 00038: val_acc did not improve from 0.69680\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3420 - acc: 0.8451 - val_loss: 0.8973 - val_acc: 0.6442\n",
      "Epoch 39/100\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8359\n",
      "Epoch 00039: val_acc did not improve from 0.69680\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3528 - acc: 0.8360 - val_loss: 0.8082 - val_acc: 0.6487\n",
      "Epoch 40/100\n",
      " 95/110 [========================>.....] - ETA: 0s - loss: 0.3398 - acc: 0.8434\n",
      "Epoch 00040: val_acc did not improve from 0.69680\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3371 - acc: 0.8448 - val_loss: 0.8624 - val_acc: 0.6304\n",
      "Epoch 41/100\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 0.3329 - acc: 0.8491\n",
      "Epoch 00041: val_acc did not improve from 0.69680\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3307 - acc: 0.8497 - val_loss: 0.8434 - val_acc: 0.6945\n",
      "Epoch 42/100\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 0.3261 - acc: 0.8588\n",
      "Epoch 00042: val_acc did not improve from 0.69680\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3267 - acc: 0.8563 - val_loss: 0.8720 - val_acc: 0.6831\n",
      "Epoch 43/100\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 0.3347 - acc: 0.8487\n",
      "Epoch 00043: val_acc improved from 0.69680 to 0.69794, saving model to best_model.h5\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3323 - acc: 0.8494 - val_loss: 0.8288 - val_acc: 0.6979\n",
      "Epoch 44/100\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.3204 - acc: 0.8521\n",
      "Epoch 00044: val_acc improved from 0.69794 to 0.69908, saving model to best_model.h5\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.3203 - acc: 0.8520 - val_loss: 0.8811 - val_acc: 0.6991\n",
      "Epoch 45/100\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 0.3239 - acc: 0.8498\n",
      "Epoch 00045: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3224 - acc: 0.8506 - val_loss: 0.8783 - val_acc: 0.6819\n",
      "Epoch 46/100\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 0.3160 - acc: 0.8583\n",
      "Epoch 00046: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.3174 - acc: 0.8569 - val_loss: 0.9204 - val_acc: 0.6362\n",
      "Epoch 47/100\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 0.3062 - acc: 0.8652\n",
      "Epoch 00047: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3063 - acc: 0.8657 - val_loss: 0.8803 - val_acc: 0.6854\n",
      "Epoch 48/100\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 0.3136 - acc: 0.8565\n",
      "Epoch 00048: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3129 - acc: 0.8580 - val_loss: 0.9211 - val_acc: 0.6304\n",
      "Epoch 49/100\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 0.3126 - acc: 0.8583\n",
      "Epoch 00049: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3112 - acc: 0.8600 - val_loss: 0.9599 - val_acc: 0.6110\n",
      "Epoch 50/100\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 0.3058 - acc: 0.8620\n",
      "Epoch 00050: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3055 - acc: 0.8626 - val_loss: 0.9420 - val_acc: 0.6545\n",
      "Epoch 51/100\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.3014 - acc: 0.8662\n",
      "Epoch 00051: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3001 - acc: 0.8663 - val_loss: 0.9339 - val_acc: 0.6934\n",
      "Epoch 52/100\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.8616\n",
      "Epoch 00052: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.2969 - acc: 0.8629 - val_loss: 1.0578 - val_acc: 0.5858\n",
      "Epoch 53/100\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 0.3056 - acc: 0.8652\n",
      "Epoch 00053: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.3029 - acc: 0.8666 - val_loss: 0.9137 - val_acc: 0.6888\n",
      "Epoch 54/100\n",
      " 96/110 [=========================>....] - ETA: 0s - loss: 0.2841 - acc: 0.8796\n",
      "Epoch 00054: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.2854 - acc: 0.8763 - val_loss: 0.9617 - val_acc: 0.6270\n",
      "Epoch 55/100\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 0.2892 - acc: 0.8710\n",
      "Epoch 00055: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.2874 - acc: 0.8700 - val_loss: 0.9312 - val_acc: 0.6854\n",
      "Epoch 56/100\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 0.3034 - acc: 0.8618\n",
      "Epoch 00056: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3040 - acc: 0.8617 - val_loss: 0.9772 - val_acc: 0.6945\n",
      "Epoch 57/100\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.8718\n",
      "Epoch 00057: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.2848 - acc: 0.8715 - val_loss: 1.2222 - val_acc: 0.5400\n",
      "Epoch 58/100\n",
      " 95/110 [========================>.....] - ETA: 0s - loss: 0.2833 - acc: 0.8766\n",
      "Epoch 00058: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.2864 - acc: 0.8726 - val_loss: 0.9693 - val_acc: 0.6751\n",
      "Epoch 59/100\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 0.2680 - acc: 0.8817\n",
      "Epoch 00059: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.2742 - acc: 0.8795 - val_loss: 1.0220 - val_acc: 0.6556\n",
      "Epoch 60/100\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.8782\n",
      "Epoch 00060: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.2771 - acc: 0.8780 - val_loss: 1.0055 - val_acc: 0.6934\n",
      "Epoch 61/100\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 0.2687 - acc: 0.8835\n",
      "Epoch 00061: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.2688 - acc: 0.8841 - val_loss: 1.0927 - val_acc: 0.6076\n",
      "Epoch 62/100\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 0.2789 - acc: 0.8738\n",
      "Epoch 00062: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.2767 - acc: 0.8755 - val_loss: 0.9399 - val_acc: 0.6465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 0.2664 - acc: 0.8846\n",
      "Epoch 00063: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2658 - acc: 0.8849 - val_loss: 0.9982 - val_acc: 0.6751\n",
      "Epoch 64/100\n",
      " 93/110 [========================>.....] - ETA: 0s - loss: 0.2814 - acc: 0.8753\n",
      "Epoch 00064: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2836 - acc: 0.8737 - val_loss: 1.0034 - val_acc: 0.6579\n",
      "Epoch 65/100\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.8899\n",
      "Epoch 00065: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2527 - acc: 0.8904 - val_loss: 1.0408 - val_acc: 0.6533\n",
      "Epoch 66/100\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 0.2580 - acc: 0.8834\n",
      "Epoch 00066: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2564 - acc: 0.8838 - val_loss: 1.0754 - val_acc: 0.6556\n",
      "Epoch 67/100\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.2556 - acc: 0.8892- ETA: 0s - loss: 0.2551 - acc\n",
      "Epoch 00067: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.2577 - acc: 0.8883 - val_loss: 1.1384 - val_acc: 0.6213\n",
      "Epoch 68/100\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.2574 - acc: 0.8838\n",
      "Epoch 00068: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.2586 - acc: 0.8835 - val_loss: 1.0886 - val_acc: 0.6533\n",
      "Epoch 69/100\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 0.2546 - acc: 0.8878\n",
      "Epoch 00069: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.2547 - acc: 0.8875 - val_loss: 1.1660 - val_acc: 0.5938\n",
      "Epoch 70/100\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.2433 - acc: 0.8933\n",
      "Epoch 00070: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.2425 - acc: 0.8932 - val_loss: 0.9544 - val_acc: 0.6865\n",
      "Epoch 71/100\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.8949\n",
      "Epoch 00071: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.2413 - acc: 0.8949 - val_loss: 1.3626 - val_acc: 0.5561\n",
      "Epoch 72/100\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 0.2498 - acc: 0.8917\n",
      "Epoch 00072: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.2496 - acc: 0.8912 - val_loss: 1.0241 - val_acc: 0.6522\n",
      "Epoch 73/100\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.2288 - acc: 0.9015\n",
      "Epoch 00073: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.2294 - acc: 0.9009 - val_loss: 1.0510 - val_acc: 0.6659\n",
      "Epoch 74/100\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.2427 - acc: 0.8936\n",
      "Epoch 00074: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 0.2425 - acc: 0.8941 - val_loss: 1.1091 - val_acc: 0.6510\n",
      "Epoch 75/100\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 0.2282 - acc: 0.8984\n",
      "Epoch 00075: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.2348 - acc: 0.8967 - val_loss: 1.0640 - val_acc: 0.6796\n",
      "Epoch 76/100\n",
      " 94/110 [========================>.....] - ETA: 0s - loss: 0.2500 - acc: 0.8863\n",
      "Epoch 00076: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.2491 - acc: 0.8861 - val_loss: 1.0914 - val_acc: 0.6739\n",
      "Epoch 77/100\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 0.2329 - acc: 0.9023\n",
      "Epoch 00077: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2331 - acc: 0.9024 - val_loss: 1.2837 - val_acc: 0.6041\n",
      "Epoch 78/100\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 0.2348 - acc: 0.8991\n",
      "Epoch 00078: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.2354 - acc: 0.8987 - val_loss: 1.1068 - val_acc: 0.6670\n",
      "Epoch 79/100\n",
      " 95/110 [========================>.....] - ETA: 0s - loss: 0.2443 - acc: 0.8908\n",
      "Epoch 00079: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2374 - acc: 0.8958 - val_loss: 1.2657 - val_acc: 0.6178\n",
      "Epoch 80/100\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.8988\n",
      "Epoch 00080: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2349 - acc: 0.8987 - val_loss: 1.0361 - val_acc: 0.6499\n",
      "Epoch 81/100\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 0.2213 - acc: 0.9044\n",
      "Epoch 00081: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2238 - acc: 0.9050 - val_loss: 1.1436 - val_acc: 0.6865\n",
      "Epoch 82/100\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 0.2208 - acc: 0.9003\n",
      "Epoch 00082: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2226 - acc: 0.8989 - val_loss: 1.1343 - val_acc: 0.6876\n",
      "Epoch 83/100\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 0.2333 - acc: 0.8981\n",
      "Epoch 00083: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2350 - acc: 0.8972 - val_loss: 1.2218 - val_acc: 0.6430\n",
      "Epoch 84/100\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9054\n",
      "Epoch 00084: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2178 - acc: 0.9058 - val_loss: 1.1016 - val_acc: 0.6842\n",
      "Epoch 85/100\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 0.2132 - acc: 0.9069\n",
      "Epoch 00085: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2180 - acc: 0.9064 - val_loss: 1.1283 - val_acc: 0.6579\n",
      "Epoch 86/100\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9005\n",
      "Epoch 00086: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2167 - acc: 0.9009 - val_loss: 1.1734 - val_acc: 0.6568\n",
      "Epoch 87/100\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9103\n",
      "Epoch 00087: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.2135 - acc: 0.9098 - val_loss: 1.1846 - val_acc: 0.6888\n",
      "Epoch 88/100\n",
      "105/110 [===========================>..] - ETA: 0s - loss: 0.2135 - acc: 0.9080\n",
      "Epoch 00088: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.2148 - acc: 0.9072 - val_loss: 1.1836 - val_acc: 0.6808\n",
      "Epoch 89/100\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.2157 - acc: 0.9054\n",
      "Epoch 00089: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.2156 - acc: 0.9055 - val_loss: 1.1873 - val_acc: 0.6670\n",
      "Epoch 90/100\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9166\n",
      "Epoch 00090: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.1955 - acc: 0.9167 - val_loss: 1.3515 - val_acc: 0.6739\n",
      "Epoch 91/100\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 0.2092 - acc: 0.9123\n",
      "Epoch 00091: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.2099 - acc: 0.9113 - val_loss: 1.2620 - val_acc: 0.6178\n",
      "Epoch 92/100\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 0.2219 - acc: 0.9105\n",
      "Epoch 00092: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.2241 - acc: 0.9098 - val_loss: 1.3494 - val_acc: 0.6053\n",
      "Epoch 93/100\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 0.1918 - acc: 0.9144\n",
      "Epoch 00093: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.1908 - acc: 0.9147 - val_loss: 1.2440 - val_acc: 0.6922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 0.1997 - acc: 0.9145\n",
      "Epoch 00094: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 0.1991 - acc: 0.9138 - val_loss: 1.2906 - val_acc: 0.6579\n",
      "Epoch 95/100\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 0.1937 - acc: 0.9156\n",
      "Epoch 00095: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.1911 - acc: 0.9167 - val_loss: 1.3632 - val_acc: 0.6705\n",
      "Epoch 96/100\n",
      " 93/110 [========================>.....] - ETA: 0s - loss: 0.1987 - acc: 0.9126\n",
      "Epoch 00096: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2049 - acc: 0.9098 - val_loss: 1.2898 - val_acc: 0.6213\n",
      "Epoch 97/100\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9133\n",
      "Epoch 00097: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2002 - acc: 0.9124 - val_loss: 1.2497 - val_acc: 0.6796\n",
      "Epoch 98/100\n",
      " 91/110 [=======================>......] - ETA: 0s - loss: 0.2016 - acc: 0.9166\n",
      "Epoch 00098: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.1950 - acc: 0.9207 - val_loss: 1.3266 - val_acc: 0.6693\n",
      "Epoch 99/100\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 0.1830 - acc: 0.9195\n",
      "Epoch 00099: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.1825 - acc: 0.9210 - val_loss: 1.4226 - val_acc: 0.6819\n",
      "Epoch 100/100\n",
      "106/110 [===========================>..] - ETA: 0s - loss: 0.1956 - acc: 0.9172\n",
      "Epoch 00100: val_acc did not improve from 0.69908\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.1985 - acc: 0.9161 - val_loss: 1.2554 - val_acc: 0.6865\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.6076 - acc: 0.7514\n",
      "테스트 정확도: 0.7514\n"
     ]
    }
   ],
   "source": [
    "DNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T03:46:54.498177Z",
     "start_time": "2021-04-30T03:46:54.485213Z"
    }
   },
   "outputs": [],
   "source": [
    "def DNN():\n",
    "    # 모델 구조 정의하기\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(32, activation='softmax')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(layers.Dense(32, activation='softmax')) #ReLU 활성화함수 채택\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(layers.Dense(32, activation='softmax')) #ReLU 활성화함수 채택\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(layers.Dense(32, activation='softmax')) #ReLU 활성화함수 채택\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train_mean, y_train, epochs=100, callbacks=[mc], batch_size=32, validation_split=0.2)\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    score = loaded_model.evaluate(X_test_mean, y_test)[1]\n",
    "    print(\"테스트 정확도: %.4f\" % (score))\n",
    "#     test_result.append(('DNN',score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T03:47:20.640216Z",
     "start_time": "2021-04-30T03:47:01.104439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.5872 - acc: 0.6968\n",
      "Epoch 00001: val_acc improved from -inf to 0.66934, saving model to best_model.h5\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5875 - acc: 0.6962 - val_loss: 0.6359 - val_acc: 0.6693\n",
      "Epoch 2/100\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 0.4924 - acc: 0.7560\n",
      "Epoch 00002: val_acc did not improve from 0.66934\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4955 - acc: 0.7561 - val_loss: 0.6351 - val_acc: 0.6693\n",
      "Epoch 3/100\n",
      " 77/110 [====================>.........] - ETA: 0s - loss: 0.4665 - acc: 0.7776\n",
      "Epoch 00003: val_acc did not improve from 0.66934\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4732 - acc: 0.7747 - val_loss: 0.6353 - val_acc: 0.6693\n",
      "Epoch 4/100\n",
      " 96/110 [=========================>....] - ETA: 0s - loss: 0.4593 - acc: 0.7832\n",
      "Epoch 00004: val_acc did not improve from 0.66934\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4578 - acc: 0.7839 - val_loss: 0.6333 - val_acc: 0.6693\n",
      "Epoch 5/100\n",
      " 86/110 [======================>.......] - ETA: 0s - loss: 0.4502 - acc: 0.7878\n",
      "Epoch 00005: val_acc did not improve from 0.66934\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.4410 - acc: 0.7922 - val_loss: 0.6212 - val_acc: 0.6693\n",
      "Epoch 6/100\n",
      " 85/110 [======================>.......] - ETA: 0s - loss: 0.4257 - acc: 0.7989\n",
      "Epoch 00006: val_acc improved from 0.66934 to 0.69794, saving model to best_model.h5\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4289 - acc: 0.7993 - val_loss: 0.6012 - val_acc: 0.6979\n",
      "Epoch 7/100\n",
      " 86/110 [======================>.......] - ETA: 0s - loss: 0.4177 - acc: 0.8027\n",
      "Epoch 00007: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4137 - acc: 0.8068 - val_loss: 0.6299 - val_acc: 0.6854\n",
      "Epoch 8/100\n",
      " 75/110 [===================>..........] - ETA: 0s - loss: 0.4077 - acc: 0.8121\n",
      "Epoch 00008: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4075 - acc: 0.8113 - val_loss: 0.6950 - val_acc: 0.6739\n",
      "Epoch 9/100\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8162\n",
      "Epoch 00009: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3952 - acc: 0.8159 - val_loss: 0.7843 - val_acc: 0.6876\n",
      "Epoch 10/100\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 0.3861 - acc: 0.8258\n",
      "Epoch 00010: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3863 - acc: 0.8254 - val_loss: 0.8348 - val_acc: 0.6785\n",
      "Epoch 11/100\n",
      " 83/110 [=====================>........] - ETA: 0s - loss: 0.3648 - acc: 0.8321\n",
      "Epoch 00011: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3815 - acc: 0.8194 - val_loss: 0.7749 - val_acc: 0.6785\n",
      "Epoch 12/100\n",
      " 88/110 [=======================>......] - ETA: 0s - loss: 0.3773 - acc: 0.8303\n",
      "Epoch 00012: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3763 - acc: 0.8311 - val_loss: 0.8507 - val_acc: 0.6716\n",
      "Epoch 13/100\n",
      " 88/110 [=======================>......] - ETA: 0s - loss: 0.3667 - acc: 0.8327\n",
      "Epoch 00013: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3679 - acc: 0.8354 - val_loss: 0.8131 - val_acc: 0.6751\n",
      "Epoch 14/100\n",
      " 87/110 [======================>.......] - ETA: 0s - loss: 0.3590 - acc: 0.8355\n",
      "Epoch 00014: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3676 - acc: 0.8319 - val_loss: 0.8464 - val_acc: 0.6739\n",
      "Epoch 15/100\n",
      " 88/110 [=======================>......] - ETA: 0s - loss: 0.3684 - acc: 0.8249\n",
      "Epoch 00015: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3635 - acc: 0.8274 - val_loss: 0.8330 - val_acc: 0.6899\n",
      "Epoch 16/100\n",
      " 84/110 [=====================>........] - ETA: 0s - loss: 0.3482 - acc: 0.8419\n",
      "Epoch 00016: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3491 - acc: 0.8431 - val_loss: 0.8473 - val_acc: 0.6659\n",
      "Epoch 17/100\n",
      " 87/110 [======================>.......] - ETA: 0s - loss: 0.3348 - acc: 0.8441\n",
      "Epoch 00017: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3401 - acc: 0.8437 - val_loss: 0.8667 - val_acc: 0.6842\n",
      "Epoch 18/100\n",
      " 83/110 [=====================>........] - ETA: 0s - loss: 0.3376 - acc: 0.8434\n",
      "Epoch 00018: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3394 - acc: 0.8451 - val_loss: 0.8977 - val_acc: 0.6728\n",
      "Epoch 19/100\n",
      " 83/110 [=====================>........] - ETA: 0s - loss: 0.3350 - acc: 0.8486\n",
      "Epoch 00019: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3351 - acc: 0.8468 - val_loss: 0.8703 - val_acc: 0.6636\n",
      "Epoch 20/100\n",
      " 84/110 [=====================>........] - ETA: 0s - loss: 0.3188 - acc: 0.8579\n",
      "Epoch 00020: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3289 - acc: 0.8497 - val_loss: 0.8985 - val_acc: 0.6739\n",
      "Epoch 21/100\n",
      " 88/110 [=======================>......] - ETA: 0s - loss: 0.3225 - acc: 0.8558\n",
      "Epoch 00021: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3235 - acc: 0.8566 - val_loss: 0.9485 - val_acc: 0.6682\n",
      "Epoch 22/100\n",
      " 85/110 [======================>.......] - ETA: 0s - loss: 0.3273 - acc: 0.8555\n",
      "Epoch 00022: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3199 - acc: 0.8577 - val_loss: 0.9445 - val_acc: 0.6613\n",
      "Epoch 23/100\n",
      " 80/110 [====================>.........] - ETA: 0s - loss: 0.3182 - acc: 0.8598\n",
      "Epoch 00023: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3117 - acc: 0.8632 - val_loss: 0.9495 - val_acc: 0.6728\n",
      "Epoch 24/100\n",
      " 76/110 [===================>..........] - ETA: 0s - loss: 0.3087 - acc: 0.8651\n",
      "Epoch 00024: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3073 - acc: 0.8646 - val_loss: 0.9250 - val_acc: 0.6808\n",
      "Epoch 25/100\n",
      " 97/110 [=========================>....] - ETA: 0s - loss: 0.2921 - acc: 0.8676\n",
      "Epoch 00025: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2988 - acc: 0.8637 - val_loss: 0.9766 - val_acc: 0.6602\n",
      "Epoch 26/100\n",
      " 71/110 [==================>...........] - ETA: 0s - loss: 0.2919 - acc: 0.8754\n",
      "Epoch 00026: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2899 - acc: 0.8732 - val_loss: 1.0008 - val_acc: 0.6522\n",
      "Epoch 27/100\n",
      " 83/110 [=====================>........] - ETA: 0s - loss: 0.2816 - acc: 0.8765\n",
      "Epoch 00027: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.2902 - acc: 0.8729 - val_loss: 0.9741 - val_acc: 0.6808\n",
      "Epoch 28/100\n",
      " 74/110 [===================>..........] - ETA: 0s - loss: 0.2620 - acc: 0.8860\n",
      "Epoch 00028: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2797 - acc: 0.8763 - val_loss: 1.0253 - val_acc: 0.6579\n",
      "Epoch 29/100\n",
      " 86/110 [======================>.......] - ETA: 0s - loss: 0.2752 - acc: 0.8786\n",
      "Epoch 00029: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.2844 - acc: 0.8740 - val_loss: 0.9832 - val_acc: 0.6545\n",
      "Epoch 30/100\n",
      " 83/110 [=====================>........] - ETA: 0s - loss: 0.2746 - acc: 0.8746\n",
      "Epoch 00030: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.2714 - acc: 0.8772 - val_loss: 1.0174 - val_acc: 0.6705\n",
      "Epoch 31/100\n",
      " 76/110 [===================>..........] - ETA: 0s - loss: 0.2673 - acc: 0.8738\n",
      "Epoch 00031: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2646 - acc: 0.8783 - val_loss: 1.0534 - val_acc: 0.6785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      " 81/110 [=====================>........] - ETA: 0s - loss: 0.2646 - acc: 0.8873\n",
      "Epoch 00032: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.2640 - acc: 0.8843 - val_loss: 1.0671 - val_acc: 0.6682\n",
      "Epoch 33/100\n",
      " 83/110 [=====================>........] - ETA: 0s - loss: 0.2424 - acc: 0.8942\n",
      "Epoch 00033: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.2577 - acc: 0.8881 - val_loss: 1.0420 - val_acc: 0.6728\n",
      "Epoch 34/100\n",
      " 79/110 [====================>.........] - ETA: 0s - loss: 0.2352 - acc: 0.8972\n",
      "Epoch 00034: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.2412 - acc: 0.8935 - val_loss: 1.1474 - val_acc: 0.6510\n",
      "Epoch 35/100\n",
      " 82/110 [=====================>........] - ETA: 0s - loss: 0.2611 - acc: 0.8868\n",
      "Epoch 00035: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2599 - acc: 0.8855 - val_loss: 1.1092 - val_acc: 0.6648\n",
      "Epoch 36/100\n",
      " 73/110 [==================>...........] - ETA: 0s - loss: 0.2399 - acc: 0.8985\n",
      "Epoch 00036: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2491 - acc: 0.8932 - val_loss: 1.0860 - val_acc: 0.6602\n",
      "Epoch 37/100\n",
      " 81/110 [=====================>........] - ETA: 0s - loss: 0.2384 - acc: 0.8958\n",
      "Epoch 00037: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.2479 - acc: 0.8909 - val_loss: 1.0728 - val_acc: 0.6648\n",
      "Epoch 38/100\n",
      " 74/110 [===================>..........] - ETA: 0s - loss: 0.2284 - acc: 0.8982\n",
      "Epoch 00038: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2462 - acc: 0.8881 - val_loss: 1.1089 - val_acc: 0.6648\n",
      "Epoch 39/100\n",
      " 88/110 [=======================>......] - ETA: 0s - loss: 0.2164 - acc: 0.9087\n",
      "Epoch 00039: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.2161 - acc: 0.9084 - val_loss: 1.1632 - val_acc: 0.6579\n",
      "Epoch 40/100\n",
      " 89/110 [=======================>......] - ETA: 0s - loss: 0.2197 - acc: 0.9055\n",
      "Epoch 00040: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.2234 - acc: 0.9021 - val_loss: 1.2048 - val_acc: 0.6545\n",
      "Epoch 41/100\n",
      " 75/110 [===================>..........] - ETA: 0s - loss: 0.2211 - acc: 0.9067\n",
      "Epoch 00041: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2341 - acc: 0.8989 - val_loss: 1.1621 - val_acc: 0.6579\n",
      "Epoch 42/100\n",
      " 88/110 [=======================>......] - ETA: 0s - loss: 0.2325 - acc: 0.8999\n",
      "Epoch 00042: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.2339 - acc: 0.9001 - val_loss: 1.2037 - val_acc: 0.6625\n",
      "Epoch 43/100\n",
      " 89/110 [=======================>......] - ETA: 0s - loss: 0.2171 - acc: 0.9038\n",
      "Epoch 00043: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.2302 - acc: 0.8975 - val_loss: 1.1945 - val_acc: 0.6487\n",
      "Epoch 44/100\n",
      " 79/110 [====================>.........] - ETA: 0s - loss: 0.2311 - acc: 0.8999\n",
      "Epoch 00044: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2248 - acc: 0.9038 - val_loss: 1.1713 - val_acc: 0.6670\n",
      "Epoch 45/100\n",
      "104/110 [===========================>..] - ETA: 0s - loss: 0.2216 - acc: 0.9035\n",
      "Epoch 00045: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2233 - acc: 0.9027 - val_loss: 1.1787 - val_acc: 0.6648\n",
      "Epoch 46/100\n",
      " 84/110 [=====================>........] - ETA: 0s - loss: 0.2015 - acc: 0.9148\n",
      "Epoch 00046: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.2060 - acc: 0.9118 - val_loss: 1.2059 - val_acc: 0.6602\n",
      "Epoch 47/100\n",
      " 88/110 [=======================>......] - ETA: 0s - loss: 0.2034 - acc: 0.9176\n",
      "Epoch 00047: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.2060 - acc: 0.9161 - val_loss: 1.2590 - val_acc: 0.6453\n",
      "Epoch 48/100\n",
      "100/110 [==========================>...] - ETA: 0s - loss: 0.2098 - acc: 0.9072\n",
      "Epoch 00048: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2091 - acc: 0.9081 - val_loss: 1.2898 - val_acc: 0.6362\n",
      "Epoch 49/100\n",
      " 88/110 [=======================>......] - ETA: 0s - loss: 0.2024 - acc: 0.9126\n",
      "Epoch 00049: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2023 - acc: 0.9141 - val_loss: 1.2447 - val_acc: 0.6625\n",
      "Epoch 50/100\n",
      " 95/110 [========================>.....] - ETA: 0s - loss: 0.2019 - acc: 0.9122\n",
      "Epoch 00050: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2005 - acc: 0.9127 - val_loss: 1.2572 - val_acc: 0.6556\n",
      "Epoch 51/100\n",
      " 92/110 [========================>.....] - ETA: 0s - loss: 0.2109 - acc: 0.9096\n",
      "Epoch 00051: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2105 - acc: 0.9101 - val_loss: 1.2819 - val_acc: 0.6636\n",
      "Epoch 52/100\n",
      " 95/110 [========================>.....] - ETA: 0s - loss: 0.2040 - acc: 0.9112\n",
      "Epoch 00052: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2094 - acc: 0.9084 - val_loss: 1.2709 - val_acc: 0.6693\n",
      "Epoch 53/100\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 0.1969 - acc: 0.9164\n",
      "Epoch 00053: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1988 - acc: 0.9158 - val_loss: 1.2645 - val_acc: 0.6625\n",
      "Epoch 54/100\n",
      " 86/110 [======================>.......] - ETA: 0s - loss: 0.1876 - acc: 0.9204\n",
      "Epoch 00054: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1886 - acc: 0.9207 - val_loss: 1.2839 - val_acc: 0.6625\n",
      "Epoch 55/100\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 0.2018 - acc: 0.9162\n",
      "Epoch 00055: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2062 - acc: 0.9147 - val_loss: 1.3177 - val_acc: 0.6384\n",
      "Epoch 56/100\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 0.1888 - acc: 0.9178\n",
      "Epoch 00056: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1906 - acc: 0.9155 - val_loss: 1.2954 - val_acc: 0.6545\n",
      "Epoch 57/100\n",
      "103/110 [===========================>..] - ETA: 0s - loss: 0.1789 - acc: 0.9266\n",
      "Epoch 00057: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1792 - acc: 0.9267 - val_loss: 1.2983 - val_acc: 0.6556\n",
      "Epoch 58/100\n",
      " 98/110 [=========================>....] - ETA: 0s - loss: 0.1861 - acc: 0.9232\n",
      "Epoch 00058: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1838 - acc: 0.9236 - val_loss: 1.3434 - val_acc: 0.6442\n",
      "Epoch 59/100\n",
      " 95/110 [========================>.....] - ETA: 0s - loss: 0.1882 - acc: 0.9217\n",
      "Epoch 00059: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1893 - acc: 0.9198 - val_loss: 1.3341 - val_acc: 0.6568\n",
      "Epoch 60/100\n",
      " 90/110 [=======================>......] - ETA: 0s - loss: 0.1878 - acc: 0.9236\n",
      "Epoch 00060: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1944 - acc: 0.9210 - val_loss: 1.3438 - val_acc: 0.6499\n",
      "Epoch 61/100\n",
      " 96/110 [=========================>....] - ETA: 0s - loss: 0.1834 - acc: 0.9264\n",
      "Epoch 00061: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1883 - acc: 0.9236 - val_loss: 1.2781 - val_acc: 0.6682\n",
      "Epoch 62/100\n",
      " 87/110 [======================>.......] - ETA: 0s - loss: 0.1788 - acc: 0.9271\n",
      "Epoch 00062: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1738 - acc: 0.9313 - val_loss: 1.3302 - val_acc: 0.6533\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 88/110 [=======================>......] - ETA: 0s - loss: 0.1913 - acc: 0.9201\n",
      "Epoch 00063: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1907 - acc: 0.9187 - val_loss: 1.3452 - val_acc: 0.6568\n",
      "Epoch 64/100\n",
      " 86/110 [======================>.......] - ETA: 0s - loss: 0.1807 - acc: 0.9237\n",
      "Epoch 00064: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1805 - acc: 0.9247 - val_loss: 1.3737 - val_acc: 0.6533\n",
      "Epoch 65/100\n",
      " 89/110 [=======================>......] - ETA: 0s - loss: 0.1805 - acc: 0.9284\n",
      "Epoch 00065: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1880 - acc: 0.9233 - val_loss: 1.3495 - val_acc: 0.6419\n",
      "Epoch 66/100\n",
      " 86/110 [======================>.......] - ETA: 0s - loss: 0.1847 - acc: 0.9244\n",
      "Epoch 00066: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1848 - acc: 0.9233 - val_loss: 1.2978 - val_acc: 0.6762\n",
      "Epoch 67/100\n",
      " 81/110 [=====================>........] - ETA: 0s - loss: 0.1743 - acc: 0.9248\n",
      "Epoch 00067: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1723 - acc: 0.9281 - val_loss: 1.3258 - val_acc: 0.6682\n",
      "Epoch 68/100\n",
      " 85/110 [======================>.......] - ETA: 0s - loss: 0.1748 - acc: 0.9228\n",
      "Epoch 00068: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1755 - acc: 0.9244 - val_loss: 1.3628 - val_acc: 0.6533\n",
      "Epoch 69/100\n",
      " 86/110 [======================>.......] - ETA: 0s - loss: 0.1702 - acc: 0.9237\n",
      "Epoch 00069: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1742 - acc: 0.9236 - val_loss: 1.3620 - val_acc: 0.6693\n",
      "Epoch 70/100\n",
      " 70/110 [==================>...........] - ETA: 0s - loss: 0.1572 - acc: 0.9371\n",
      "Epoch 00070: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1703 - acc: 0.9307 - val_loss: 1.3633 - val_acc: 0.6625\n",
      "Epoch 71/100\n",
      " 82/110 [=====================>........] - ETA: 0s - loss: 0.1625 - acc: 0.9325\n",
      "Epoch 00071: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1639 - acc: 0.9316 - val_loss: 1.3680 - val_acc: 0.6579\n",
      "Epoch 72/100\n",
      " 83/110 [=====================>........] - ETA: 0s - loss: 0.1602 - acc: 0.9319\n",
      "Epoch 00072: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1619 - acc: 0.9296 - val_loss: 1.3664 - val_acc: 0.6579\n",
      "Epoch 73/100\n",
      " 86/110 [======================>.......] - ETA: 0s - loss: 0.1628 - acc: 0.9317\n",
      "Epoch 00073: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1613 - acc: 0.9330 - val_loss: 1.3957 - val_acc: 0.6510\n",
      "Epoch 74/100\n",
      " 82/110 [=====================>........] - ETA: 0s - loss: 0.1539 - acc: 0.9386\n",
      "Epoch 00074: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1561 - acc: 0.9370 - val_loss: 1.3856 - val_acc: 0.6556\n",
      "Epoch 75/100\n",
      " 80/110 [====================>.........] - ETA: 0s - loss: 0.1559 - acc: 0.9352\n",
      "Epoch 00075: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1696 - acc: 0.9284 - val_loss: 1.3931 - val_acc: 0.6499\n",
      "Epoch 76/100\n",
      " 73/110 [==================>...........] - ETA: 0s - loss: 0.1609 - acc: 0.9341\n",
      "Epoch 00076: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1554 - acc: 0.9379 - val_loss: 1.3832 - val_acc: 0.6648\n",
      "Epoch 77/100\n",
      " 80/110 [====================>.........] - ETA: 0s - loss: 0.1493 - acc: 0.9438\n",
      "Epoch 00077: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1541 - acc: 0.9410 - val_loss: 1.3907 - val_acc: 0.6648\n",
      "Epoch 78/100\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9395\n",
      "Epoch 00078: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1460 - acc: 0.9396 - val_loss: 1.4365 - val_acc: 0.6487\n",
      "Epoch 79/100\n",
      " 83/110 [=====================>........] - ETA: 0s - loss: 0.1567 - acc: 0.9394\n",
      "Epoch 00079: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1541 - acc: 0.9390 - val_loss: 1.3975 - val_acc: 0.6602\n",
      "Epoch 80/100\n",
      " 86/110 [======================>.......] - ETA: 0s - loss: 0.1419 - acc: 0.9462\n",
      "Epoch 00080: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1436 - acc: 0.9462 - val_loss: 1.4242 - val_acc: 0.6670\n",
      "Epoch 81/100\n",
      " 88/110 [=======================>......] - ETA: 0s - loss: 0.1397 - acc: 0.9393\n",
      "Epoch 00081: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1444 - acc: 0.9399 - val_loss: 1.4397 - val_acc: 0.6579\n",
      "Epoch 82/100\n",
      " 81/110 [=====================>........] - ETA: 0s - loss: 0.1507 - acc: 0.9356\n",
      "Epoch 00082: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1441 - acc: 0.9390 - val_loss: 1.4712 - val_acc: 0.6636\n",
      "Epoch 83/100\n",
      " 85/110 [======================>.......] - ETA: 0s - loss: 0.1345 - acc: 0.9445\n",
      "Epoch 00083: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1443 - acc: 0.9393 - val_loss: 1.4969 - val_acc: 0.6510\n",
      "Epoch 84/100\n",
      " 89/110 [=======================>......] - ETA: 0s - loss: 0.1461 - acc: 0.9442\n",
      "Epoch 00084: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1467 - acc: 0.9419 - val_loss: 1.4721 - val_acc: 0.6430\n",
      "Epoch 85/100\n",
      " 85/110 [======================>.......] - ETA: 0s - loss: 0.1341 - acc: 0.9511\n",
      "Epoch 00085: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1406 - acc: 0.9473 - val_loss: 1.4973 - val_acc: 0.6499\n",
      "Epoch 86/100\n",
      " 86/110 [======================>.......] - ETA: 0s - loss: 0.1332 - acc: 0.9466\n",
      "Epoch 00086: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1338 - acc: 0.9459 - val_loss: 1.4992 - val_acc: 0.6476\n",
      "Epoch 87/100\n",
      " 84/110 [=====================>........] - ETA: 0s - loss: 0.1299 - acc: 0.9475\n",
      "Epoch 00087: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1368 - acc: 0.9468 - val_loss: 1.5571 - val_acc: 0.6327\n",
      "Epoch 88/100\n",
      "101/110 [==========================>...] - ETA: 0s - loss: 0.1217 - acc: 0.9524\n",
      "Epoch 00088: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1245 - acc: 0.9502 - val_loss: 1.5697 - val_acc: 0.6373\n",
      "Epoch 89/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1279 - acc: 0.9462\n",
      "Epoch 00089: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1279 - acc: 0.9462 - val_loss: 1.5963 - val_acc: 0.6499\n",
      "Epoch 90/100\n",
      " 75/110 [===================>..........] - ETA: 0s - loss: 0.1379 - acc: 0.9463\n",
      "Epoch 00090: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1474 - acc: 0.9402 - val_loss: 1.5615 - val_acc: 0.6602\n",
      "Epoch 91/100\n",
      " 75/110 [===================>..........] - ETA: 0s - loss: 0.1285 - acc: 0.9475\n",
      "Epoch 00091: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1275 - acc: 0.9493 - val_loss: 1.5534 - val_acc: 0.6487\n",
      "Epoch 92/100\n",
      " 80/110 [====================>.........] - ETA: 0s - loss: 0.1465 - acc: 0.9438\n",
      "Epoch 00092: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1519 - acc: 0.9402 - val_loss: 1.5892 - val_acc: 0.6350\n",
      "Epoch 93/100\n",
      " 74/110 [===================>..........] - ETA: 0s - loss: 0.1358 - acc: 0.9447\n",
      "Epoch 00093: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1356 - acc: 0.9436 - val_loss: 1.5892 - val_acc: 0.6407\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81/110 [=====================>........] - ETA: 0s - loss: 0.1328 - acc: 0.9406\n",
      "Epoch 00094: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1329 - acc: 0.9430 - val_loss: 1.6066 - val_acc: 0.6430\n",
      "Epoch 95/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1236 - acc: 0.9499\n",
      "Epoch 00095: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1236 - acc: 0.9499 - val_loss: 1.6259 - val_acc: 0.6453\n",
      "Epoch 96/100\n",
      "109/110 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9464\n",
      "Epoch 00096: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1451 - acc: 0.9465 - val_loss: 1.5579 - val_acc: 0.6533\n",
      "Epoch 97/100\n",
      " 79/110 [====================>.........] - ETA: 0s - loss: 0.1177 - acc: 0.9525\n",
      "Epoch 00097: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1163 - acc: 0.9530 - val_loss: 1.6046 - val_acc: 0.6430\n",
      "Epoch 98/100\n",
      " 83/110 [=====================>........] - ETA: 0s - loss: 0.1306 - acc: 0.9462\n",
      "Epoch 00098: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1325 - acc: 0.9447 - val_loss: 1.6186 - val_acc: 0.6396\n",
      "Epoch 99/100\n",
      " 82/110 [=====================>........] - ETA: 0s - loss: 0.1254 - acc: 0.9474\n",
      "Epoch 00099: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1207 - acc: 0.9505 - val_loss: 1.5928 - val_acc: 0.6362\n",
      "Epoch 100/100\n",
      " 81/110 [=====================>........] - ETA: 0s - loss: 0.1290 - acc: 0.9506\n",
      "Epoch 00100: val_acc did not improve from 0.69794\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1282 - acc: 0.9493 - val_loss: 1.6327 - val_acc: 0.6419\n",
      "46/46 [==============================] - 0s 585us/step - loss: 0.5634 - acc: 0.7012\n",
      "테스트 정확도: 0.7012\n"
     ]
    }
   ],
   "source": [
    "DNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T03:54:02.816592Z",
     "start_time": "2021-04-30T03:54:02.802630Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "def cnn_1D():\n",
    "    model = Sequential()    \n",
    "    model.add(Conv1D(256, 32, padding='valid', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    print(model.summary)\n",
    "    history = model.fit(X_train_mean, y_train, epochs=100, callbacks=[mc], batch_size=32, validation_split=0.2)\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    score = loaded_model.evaluate(X_test_mean, y_test)[1]\n",
    "    print(\"테스트 정확도: %.4f\" % (score))\n",
    "    test_result.append(('1D-CNN',score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T03:54:04.428777Z",
     "start_time": "2021-04-30T03:54:04.308101Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Model.summary of <tensorflow.python.keras.engine.sequential.Sequential object at 0x000001804E70EC10>>\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:191 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_19 is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: [None, 100]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-e0cea29e3c43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcnn_1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-149-b42eb9fb5db2>\u001b[0m in \u001b[0;36mcnn_1D\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:191 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_19 is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: [None, 100]\n"
     ]
    }
   ],
   "source": [
    "cnn_1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T06:36:40.545352Z",
     "start_time": "2021-04-16T06:36:40.468716Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(100,))) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(100,))) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.Dense(64, activation='relu')) #ReLU 활성화함수 채택\n",
    "model.add(layers.Dense(3, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T06:36:59.775431Z",
     "start_time": "2021-04-16T06:36:41.136583Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 1.0596 - acc: 0.4442\n",
      "Epoch 00001: val_acc improved from -inf to 0.54177, saving model to best_model.h5\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.0494 - acc: 0.4511 - val_loss: 0.9663 - val_acc: 0.5418\n",
      "Epoch 2/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.9765 - acc: 0.5150\n",
      "Epoch 00002: val_acc improved from 0.54177 to 0.55359, saving model to best_model.h5\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.9765 - acc: 0.5149 - val_loss: 0.9294 - val_acc: 0.5536\n",
      "Epoch 3/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 0.9282 - acc: 0.5447\n",
      "Epoch 00003: val_acc improved from 0.55359 to 0.56540, saving model to best_model.h5\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.9293 - acc: 0.5453 - val_loss: 0.9152 - val_acc: 0.5654\n",
      "Epoch 4/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 0.8873 - acc: 0.5801\n",
      "Epoch 00004: val_acc did not improve from 0.56540\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.8912 - acc: 0.5772 - val_loss: 0.9061 - val_acc: 0.5595\n",
      "Epoch 5/100\n",
      "49/75 [==================>...........] - ETA: 0s - loss: 0.8908 - acc: 0.5788\n",
      "Epoch 00005: val_acc improved from 0.56540 to 0.57637, saving model to best_model.h5\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.8965 - acc: 0.5712 - val_loss: 0.9017 - val_acc: 0.5764\n",
      "Epoch 6/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.8674 - acc: 0.5921\n",
      "Epoch 00006: val_acc did not improve from 0.57637\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.8674 - acc: 0.5921 - val_loss: 0.8997 - val_acc: 0.5764\n",
      "Epoch 7/100\n",
      "51/75 [===================>..........] - ETA: 0s - loss: 0.8666 - acc: 0.5833\n",
      "Epoch 00007: val_acc improved from 0.57637 to 0.58143, saving model to best_model.h5\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.8634 - acc: 0.5913 - val_loss: 0.8967 - val_acc: 0.5814\n",
      "Epoch 8/100\n",
      "52/75 [===================>..........] - ETA: 0s - loss: 0.8384 - acc: 0.5992\n",
      "Epoch 00008: val_acc did not improve from 0.58143\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.8373 - acc: 0.6014 - val_loss: 0.8992 - val_acc: 0.5738\n",
      "Epoch 9/100\n",
      "53/75 [====================>.........] - ETA: 0s - loss: 0.8472 - acc: 0.6052\n",
      "Epoch 00009: val_acc improved from 0.58143 to 0.58734, saving model to best_model.h5\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.8461 - acc: 0.6063 - val_loss: 0.8974 - val_acc: 0.5873\n",
      "Epoch 10/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.8258 - acc: 0.6128\n",
      "Epoch 00010: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.8314 - acc: 0.6080 - val_loss: 0.9067 - val_acc: 0.5662\n",
      "Epoch 11/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.8089 - acc: 0.6224\n",
      "Epoch 00011: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.8200 - acc: 0.6126 - val_loss: 0.8985 - val_acc: 0.5797\n",
      "Epoch 12/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.8217 - acc: 0.6169\n",
      "Epoch 00012: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.8147 - acc: 0.6209 - val_loss: 0.8951 - val_acc: 0.5738\n",
      "Epoch 13/100\n",
      "72/75 [===========================>..] - ETA: 0s - loss: 0.8035 - acc: 0.6189\n",
      "Epoch 00013: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.8033 - acc: 0.6185 - val_loss: 0.8985 - val_acc: 0.5738\n",
      "Epoch 14/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 0.7948 - acc: 0.6273\n",
      "Epoch 00014: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.7939 - acc: 0.6297 - val_loss: 0.8993 - val_acc: 0.5688\n",
      "Epoch 15/100\n",
      "55/75 [=====================>........] - ETA: 0s - loss: 0.7662 - acc: 0.6440\n",
      "Epoch 00015: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.7808 - acc: 0.6380 - val_loss: 0.9107 - val_acc: 0.5578\n",
      "Epoch 16/100\n",
      "55/75 [=====================>........] - ETA: 0s - loss: 0.7764 - acc: 0.6250\n",
      "Epoch 00016: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.7755 - acc: 0.6285 - val_loss: 0.8996 - val_acc: 0.5738\n",
      "Epoch 17/100\n",
      "72/75 [===========================>..] - ETA: 0s - loss: 0.7682 - acc: 0.6341\n",
      "Epoch 00017: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.7695 - acc: 0.6352 - val_loss: 0.9073 - val_acc: 0.5679\n",
      "Epoch 18/100\n",
      "53/75 [====================>.........] - ETA: 0s - loss: 0.7655 - acc: 0.6565\n",
      "Epoch 00018: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.7685 - acc: 0.6477 - val_loss: 0.9105 - val_acc: 0.5637\n",
      "Epoch 19/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.7494 - acc: 0.6515\n",
      "Epoch 00019: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.7485 - acc: 0.6521 - val_loss: 0.9124 - val_acc: 0.5781\n",
      "Epoch 20/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.7481 - acc: 0.6529\n",
      "Epoch 00020: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.7480 - acc: 0.6529 - val_loss: 0.9189 - val_acc: 0.5730\n",
      "Epoch 21/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 0.7459 - acc: 0.6625\n",
      "Epoch 00021: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.7446 - acc: 0.6624 - val_loss: 0.9325 - val_acc: 0.5485\n",
      "Epoch 22/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.7323 - acc: 0.6652\n",
      "Epoch 00022: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.7325 - acc: 0.6656 - val_loss: 0.9220 - val_acc: 0.5620\n",
      "Epoch 23/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 0.7175 - acc: 0.6685\n",
      "Epoch 00023: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.7155 - acc: 0.6703 - val_loss: 0.9231 - val_acc: 0.5578\n",
      "Epoch 24/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.7214 - acc: 0.6677\n",
      "Epoch 00024: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.7212 - acc: 0.6677 - val_loss: 0.9320 - val_acc: 0.5662\n",
      "Epoch 25/100\n",
      "51/75 [===================>..........] - ETA: 0s - loss: 0.6903 - acc: 0.6866\n",
      "Epoch 00025: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.7038 - acc: 0.6779 - val_loss: 0.9289 - val_acc: 0.5705\n",
      "Epoch 26/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.7010 - acc: 0.6834\n",
      "Epoch 00026: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.6977 - acc: 0.6869 - val_loss: 0.9418 - val_acc: 0.5595\n",
      "Epoch 27/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.6863 - acc: 0.6840\n",
      "Epoch 00027: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.6936 - acc: 0.6817 - val_loss: 0.9300 - val_acc: 0.5671\n",
      "Epoch 28/100\n",
      "53/75 [====================>.........] - ETA: 0s - loss: 0.6723 - acc: 0.6975\n",
      "Epoch 00028: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.6773 - acc: 0.6924 - val_loss: 0.9288 - val_acc: 0.5781\n",
      "Epoch 29/100\n",
      "50/75 [===================>..........] - ETA: 0s - loss: 0.6803 - acc: 0.6913\n",
      "Epoch 00029: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.6848 - acc: 0.6893 - val_loss: 0.9328 - val_acc: 0.5654\n",
      "Epoch 30/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 0.6854 - acc: 0.6930\n",
      "Epoch 00030: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.6874 - acc: 0.6922 - val_loss: 0.9371 - val_acc: 0.5612\n",
      "Epoch 31/100\n",
      "67/75 [=========================>....] - ETA: 0s - loss: 0.6738 - acc: 0.6903\n",
      "Epoch 00031: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.6790 - acc: 0.6901 - val_loss: 0.9365 - val_acc: 0.5527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "55/75 [=====================>........] - ETA: 0s - loss: 0.6692 - acc: 0.6935\n",
      "Epoch 00032: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.6683 - acc: 0.6971 - val_loss: 0.9485 - val_acc: 0.5688\n",
      "Epoch 33/100\n",
      "57/75 [=====================>........] - ETA: 0s - loss: 0.6470 - acc: 0.6987\n",
      "Epoch 00033: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.6532 - acc: 0.6994 - val_loss: 0.9582 - val_acc: 0.5519\n",
      "Epoch 34/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.6607 - acc: 0.7025\n",
      "Epoch 00034: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.6611 - acc: 0.7023 - val_loss: 0.9546 - val_acc: 0.5662\n",
      "Epoch 35/100\n",
      "50/75 [===================>..........] - ETA: 0s - loss: 0.6298 - acc: 0.7284\n",
      "Epoch 00035: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.6442 - acc: 0.7142 - val_loss: 0.9437 - val_acc: 0.5696\n",
      "Epoch 36/100\n",
      "52/75 [===================>..........] - ETA: 0s - loss: 0.6368 - acc: 0.7127\n",
      "Epoch 00036: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.6458 - acc: 0.7026 - val_loss: 0.9629 - val_acc: 0.5654\n",
      "Epoch 37/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 0.6381 - acc: 0.7183\n",
      "Epoch 00037: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.6379 - acc: 0.7194 - val_loss: 0.9719 - val_acc: 0.5629\n",
      "Epoch 38/100\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.6177 - acc: 0.7254\n",
      "Epoch 00038: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.6177 - acc: 0.7254 - val_loss: 0.9738 - val_acc: 0.5722\n",
      "Epoch 39/100\n",
      "52/75 [===================>..........] - ETA: 0s - loss: 0.6016 - acc: 0.7293\n",
      "Epoch 00039: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.6101 - acc: 0.7287 - val_loss: 0.9678 - val_acc: 0.5646\n",
      "Epoch 40/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.6192 - acc: 0.7252\n",
      "Epoch 00040: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.6197 - acc: 0.7251 - val_loss: 0.9805 - val_acc: 0.5629\n",
      "Epoch 41/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.6193 - acc: 0.7269\n",
      "Epoch 00041: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.6169 - acc: 0.7277 - val_loss: 0.9714 - val_acc: 0.5570\n",
      "Epoch 42/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 0.5958 - acc: 0.7376\n",
      "Epoch 00042: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5947 - acc: 0.7387 - val_loss: 0.9846 - val_acc: 0.5468\n",
      "Epoch 43/100\n",
      "51/75 [===================>..........] - ETA: 0s - loss: 0.5988 - acc: 0.7451\n",
      "Epoch 00043: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.6018 - acc: 0.7382 - val_loss: 0.9843 - val_acc: 0.5553\n",
      "Epoch 44/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.5840 - acc: 0.7410\n",
      "Epoch 00044: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5888 - acc: 0.7401 - val_loss: 1.0005 - val_acc: 0.5502\n",
      "Epoch 45/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 0.5869 - acc: 0.7463\n",
      "Epoch 00045: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5839 - acc: 0.7486 - val_loss: 1.0016 - val_acc: 0.5477\n",
      "Epoch 46/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.5861 - acc: 0.7428\n",
      "Epoch 00046: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5849 - acc: 0.7437 - val_loss: 0.9961 - val_acc: 0.5485\n",
      "Epoch 47/100\n",
      "72/75 [===========================>..] - ETA: 0s - loss: 0.5630 - acc: 0.7522\n",
      "Epoch 00047: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5643 - acc: 0.7517 - val_loss: 1.0290 - val_acc: 0.5578\n",
      "Epoch 48/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 0.5759 - acc: 0.7443\n",
      "Epoch 00048: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5763 - acc: 0.7444 - val_loss: 1.0099 - val_acc: 0.5460\n",
      "Epoch 49/100\n",
      "72/75 [===========================>..] - ETA: 0s - loss: 0.5643 - acc: 0.7600\n",
      "Epoch 00049: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5651 - acc: 0.7600 - val_loss: 1.0216 - val_acc: 0.5637\n",
      "Epoch 50/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.5440 - acc: 0.7688\n",
      "Epoch 00050: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5595 - acc: 0.7598 - val_loss: 1.0116 - val_acc: 0.5603\n",
      "Epoch 51/100\n",
      "64/75 [========================>.....] - ETA: 0s - loss: 0.5562 - acc: 0.7637\n",
      "Epoch 00051: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5602 - acc: 0.7619 - val_loss: 1.0307 - val_acc: 0.5536\n",
      "Epoch 52/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 0.5877 - acc: 0.7458\n",
      "Epoch 00052: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5862 - acc: 0.7469 - val_loss: 1.0094 - val_acc: 0.5536\n",
      "Epoch 53/100\n",
      "58/75 [======================>.......] - ETA: 0s - loss: 0.5554 - acc: 0.7570\n",
      "Epoch 00053: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5545 - acc: 0.7566 - val_loss: 1.0389 - val_acc: 0.5527\n",
      "Epoch 54/100\n",
      "58/75 [======================>.......] - ETA: 0s - loss: 0.5454 - acc: 0.7605\n",
      "Epoch 00054: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5498 - acc: 0.7585 - val_loss: 1.0264 - val_acc: 0.5586\n",
      "Epoch 55/100\n",
      "52/75 [===================>..........] - ETA: 0s - loss: 0.5312 - acc: 0.7704\n",
      "Epoch 00055: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5376 - acc: 0.7703 - val_loss: 1.0353 - val_acc: 0.5603\n",
      "Epoch 56/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 0.5351 - acc: 0.7693\n",
      "Epoch 00056: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5300 - acc: 0.7714 - val_loss: 1.0537 - val_acc: 0.5544\n",
      "Epoch 57/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.7762\n",
      "Epoch 00057: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5177 - acc: 0.7762 - val_loss: 1.0573 - val_acc: 0.5519\n",
      "Epoch 58/100\n",
      "49/75 [==================>...........] - ETA: 0s - loss: 0.5122 - acc: 0.7812\n",
      "Epoch 00058: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5148 - acc: 0.7786 - val_loss: 1.0595 - val_acc: 0.5477\n",
      "Epoch 59/100\n",
      "57/75 [=====================>........] - ETA: 0s - loss: 0.5015 - acc: 0.7930\n",
      "Epoch 00059: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4976 - acc: 0.7935 - val_loss: 1.0878 - val_acc: 0.5544\n",
      "Epoch 60/100\n",
      "50/75 [===================>..........] - ETA: 0s - loss: 0.5061 - acc: 0.7934\n",
      "Epoch 00060: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5165 - acc: 0.7853 - val_loss: 1.0641 - val_acc: 0.5612\n",
      "Epoch 61/100\n",
      "56/75 [=====================>........] - ETA: 0s - loss: 0.5131 - acc: 0.7891\n",
      "Epoch 00061: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5164 - acc: 0.7891 - val_loss: 1.0485 - val_acc: 0.5511\n",
      "Epoch 62/100\n",
      "65/75 [=========================>....] - ETA: 0s - loss: 0.5066 - acc: 0.7803\n",
      "Epoch 00062: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5111 - acc: 0.7773 - val_loss: 1.0799 - val_acc: 0.5561\n",
      "Epoch 63/100\n",
      "53/75 [====================>.........] - ETA: 0s - loss: 0.5102 - acc: 0.7860\n",
      "Epoch 00063: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5095 - acc: 0.7843 - val_loss: 1.0936 - val_acc: 0.5485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100\n",
      "48/75 [==================>...........] - ETA: 0s - loss: 0.4867 - acc: 0.7891\n",
      "Epoch 00064: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4972 - acc: 0.7874 - val_loss: 1.0874 - val_acc: 0.5570\n",
      "Epoch 65/100\n",
      "74/75 [============================>.] - ETA: 0s - loss: 0.4957 - acc: 0.7941\n",
      "Epoch 00065: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4956 - acc: 0.7942 - val_loss: 1.1008 - val_acc: 0.5561\n",
      "Epoch 66/100\n",
      "58/75 [======================>.......] - ETA: 0s - loss: 0.4795 - acc: 0.8017\n",
      "Epoch 00066: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4879 - acc: 0.7954 - val_loss: 1.0969 - val_acc: 0.5451\n",
      "Epoch 67/100\n",
      "55/75 [=====================>........] - ETA: 0s - loss: 0.4694 - acc: 0.8114\n",
      "Epoch 00067: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4769 - acc: 0.8056 - val_loss: 1.1191 - val_acc: 0.5511\n",
      "Epoch 68/100\n",
      "56/75 [=====================>........] - ETA: 0s - loss: 0.4707 - acc: 0.8027\n",
      "Epoch 00068: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4762 - acc: 0.8022 - val_loss: 1.1339 - val_acc: 0.5451\n",
      "Epoch 69/100\n",
      "56/75 [=====================>........] - ETA: 0s - loss: 0.4809 - acc: 0.7960\n",
      "Epoch 00069: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4901 - acc: 0.7912 - val_loss: 1.1201 - val_acc: 0.5460\n",
      "Epoch 70/100\n",
      "55/75 [=====================>........] - ETA: 0s - loss: 0.4716 - acc: 0.8045\n",
      "Epoch 00070: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4682 - acc: 0.8087 - val_loss: 1.1261 - val_acc: 0.5519\n",
      "Epoch 71/100\n",
      "58/75 [======================>.......] - ETA: 0s - loss: 0.4718 - acc: 0.8128\n",
      "Epoch 00071: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4678 - acc: 0.8109 - val_loss: 1.1258 - val_acc: 0.5418\n",
      "Epoch 72/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.4582 - acc: 0.8090\n",
      "Epoch 00072: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4588 - acc: 0.8081 - val_loss: 1.1483 - val_acc: 0.5477\n",
      "Epoch 73/100\n",
      "55/75 [=====================>........] - ETA: 0s - loss: 0.4555 - acc: 0.8082\n",
      "Epoch 00073: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4632 - acc: 0.8045 - val_loss: 1.1454 - val_acc: 0.5468\n",
      "Epoch 74/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.4629 - acc: 0.8032\n",
      "Epoch 00074: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4569 - acc: 0.8066 - val_loss: 1.1646 - val_acc: 0.5460\n",
      "Epoch 75/100\n",
      "56/75 [=====================>........] - ETA: 0s - loss: 0.4754 - acc: 0.7972\n",
      "Epoch 00075: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4568 - acc: 0.8043 - val_loss: 1.1461 - val_acc: 0.5376\n",
      "Epoch 76/100\n",
      "57/75 [=====================>........] - ETA: 0s - loss: 0.4660 - acc: 0.8106\n",
      "Epoch 00076: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4602 - acc: 0.8125 - val_loss: 1.1500 - val_acc: 0.5392\n",
      "Epoch 77/100\n",
      "57/75 [=====================>........] - ETA: 0s - loss: 0.4300 - acc: 0.8251\n",
      "Epoch 00077: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4261 - acc: 0.8271 - val_loss: 1.1957 - val_acc: 0.5350\n",
      "Epoch 78/100\n",
      "56/75 [=====================>........] - ETA: 0s - loss: 0.4418 - acc: 0.8209\n",
      "Epoch 00078: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4408 - acc: 0.8147 - val_loss: 1.1912 - val_acc: 0.5519\n",
      "Epoch 79/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.4227 - acc: 0.8186\n",
      "Epoch 00079: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4312 - acc: 0.8176 - val_loss: 1.1863 - val_acc: 0.5553\n",
      "Epoch 80/100\n",
      "56/75 [=====================>........] - ETA: 0s - loss: 0.4277 - acc: 0.8259\n",
      "Epoch 00080: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4317 - acc: 0.8242 - val_loss: 1.1766 - val_acc: 0.5460\n",
      "Epoch 81/100\n",
      "56/75 [=====================>........] - ETA: 0s - loss: 0.4072 - acc: 0.8284\n",
      "Epoch 00081: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4259 - acc: 0.8199 - val_loss: 1.1952 - val_acc: 0.5494\n",
      "Epoch 82/100\n",
      "57/75 [=====================>........] - ETA: 0s - loss: 0.4401 - acc: 0.8251\n",
      "Epoch 00082: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4365 - acc: 0.8248 - val_loss: 1.1938 - val_acc: 0.5443\n",
      "Epoch 83/100\n",
      "56/75 [=====================>........] - ETA: 0s - loss: 0.4156 - acc: 0.8318\n",
      "Epoch 00083: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4184 - acc: 0.8303 - val_loss: 1.1973 - val_acc: 0.5460\n",
      "Epoch 84/100\n",
      "53/75 [====================>.........] - ETA: 0s - loss: 0.4205 - acc: 0.8308\n",
      "Epoch 00084: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4222 - acc: 0.8284 - val_loss: 1.2077 - val_acc: 0.5409\n",
      "Epoch 85/100\n",
      "55/75 [=====================>........] - ETA: 0s - loss: 0.3961 - acc: 0.8384\n",
      "Epoch 00085: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4020 - acc: 0.8358 - val_loss: 1.2262 - val_acc: 0.5435\n",
      "Epoch 86/100\n",
      "52/75 [===================>..........] - ETA: 0s - loss: 0.3999 - acc: 0.8353\n",
      "Epoch 00086: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4086 - acc: 0.8326 - val_loss: 1.2403 - val_acc: 0.5426\n",
      "Epoch 87/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.4101 - acc: 0.8394\n",
      "Epoch 00087: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4058 - acc: 0.8370 - val_loss: 1.2085 - val_acc: 0.5460\n",
      "Epoch 88/100\n",
      "55/75 [=====================>........] - ETA: 0s - loss: 0.3909 - acc: 0.8446\n",
      "Epoch 00088: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4018 - acc: 0.8364 - val_loss: 1.2418 - val_acc: 0.5392\n",
      "Epoch 89/100\n",
      "72/75 [===========================>..] - ETA: 0s - loss: 0.4102 - acc: 0.8279\n",
      "Epoch 00089: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.4126 - acc: 0.8273 - val_loss: 1.2164 - val_acc: 0.5409\n",
      "Epoch 90/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 0.4183 - acc: 0.8304\n",
      "Epoch 00090: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.4195 - acc: 0.8299 - val_loss: 1.2332 - val_acc: 0.5485\n",
      "Epoch 91/100\n",
      "55/75 [=====================>........] - ETA: 0s - loss: 0.4064 - acc: 0.8395\n",
      "Epoch 00091: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4119 - acc: 0.8336 - val_loss: 1.1979 - val_acc: 0.5451\n",
      "Epoch 92/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.4339 - acc: 0.8278\n",
      "Epoch 00092: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4383 - acc: 0.8250 - val_loss: 1.1996 - val_acc: 0.5384\n",
      "Epoch 93/100\n",
      "52/75 [===================>..........] - ETA: 0s - loss: 0.4083 - acc: 0.8368\n",
      "Epoch 00093: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4069 - acc: 0.8343 - val_loss: 1.2400 - val_acc: 0.5401\n",
      "Epoch 94/100\n",
      "56/75 [=====================>........] - ETA: 0s - loss: 0.3812 - acc: 0.8474\n",
      "Epoch 00094: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3912 - acc: 0.8406 - val_loss: 1.2562 - val_acc: 0.5333\n",
      "Epoch 95/100\n",
      "56/75 [=====================>........] - ETA: 0s - loss: 0.3753 - acc: 0.8449\n",
      "Epoch 00095: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3839 - acc: 0.8444 - val_loss: 1.2894 - val_acc: 0.5342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "57/75 [=====================>........] - ETA: 0s - loss: 0.3730 - acc: 0.8575\n",
      "Epoch 00096: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3798 - acc: 0.8535 - val_loss: 1.2857 - val_acc: 0.5401\n",
      "Epoch 97/100\n",
      "55/75 [=====================>........] - ETA: 0s - loss: 0.3674 - acc: 0.8483\n",
      "Epoch 00097: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3703 - acc: 0.8505 - val_loss: 1.2977 - val_acc: 0.5443\n",
      "Epoch 98/100\n",
      "55/75 [=====================>........] - ETA: 0s - loss: 0.3621 - acc: 0.8605\n",
      "Epoch 00098: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3682 - acc: 0.8539 - val_loss: 1.3011 - val_acc: 0.5536\n",
      "Epoch 99/100\n",
      "56/75 [=====================>........] - ETA: 0s - loss: 0.3744 - acc: 0.8538\n",
      "Epoch 00099: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3762 - acc: 0.8512 - val_loss: 1.2866 - val_acc: 0.5544\n",
      "Epoch 100/100\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.3740 - acc: 0.8452\n",
      "Epoch 00100: val_acc did not improve from 0.58734\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3756 - acc: 0.8465 - val_loss: 1.2795 - val_acc: 0.5494\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-4db5c15d4434>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# loaded_model = load_model('best_model.h5')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"테스트 정확도: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_scaled, y_train, epochs=100, callbacks=[mc], batch_size=64, validation_split=0.2)\n",
    "\n",
    "# loaded_model = load_model('best_model.h5')\n",
    "print(\"테스트 정확도: %.4f\" % (model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T06:14:55.121101Z",
     "start_time": "2021-04-16T06:14:55.104582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5922"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T06:15:00.424197Z",
     "start_time": "2021-04-16T06:14:59.228517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 200)          1184400   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 100, 256)          336896    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 1,923,859\n",
      "Trainable params: 1,923,859\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM,Bidirectional ,Embedding, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "model = Sequential([\n",
    "    Embedding(len(X_train_mean), 200, input_length=100),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    Dropout(0.25),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(3,activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T06:15:08.423409Z",
     "start_time": "2021-04-16T06:15:02.164085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 1/75 [..............................] - ETA: 0s - loss: 1.0983 - acc: 0.3125"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " indices[46,40] = -1 is not in [0, 5922)\n\t [[node sequential_5/embedding_2/embedding_lookup (defined at <ipython-input-69-c40fdff1707b>:4) ]] [Op:__inference_train_function_198155]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential_5/embedding_2/embedding_lookup:\n sequential_5/embedding_2/embedding_lookup/194011 (defined at C:\\Users\\rlagy\\anaconda3\\lib\\contextlib.py:113)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-c40fdff1707b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  indices[46,40] = -1 is not in [0, 5922)\n\t [[node sequential_5/embedding_2/embedding_lookup (defined at <ipython-input-69-c40fdff1707b>:4) ]] [Op:__inference_train_function_198155]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential_5/embedding_2/embedding_lookup:\n sequential_5/embedding_2/embedding_lookup/194011 (defined at C:\\Users\\rlagy\\anaconda3\\lib\\contextlib.py:113)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train_mean, y_train, epochs=100, callbacks=[mc], batch_size=64, validation_split=0.2)\n",
    "\n",
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:48:04.412515Z",
     "start_time": "2021-04-15T02:48:04.383591Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[0;32m   2349\u001b[0m     \"\"\"\n\u001b[0;32m   2350\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2351\u001b[1;33m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[0;32m   2352\u001b[0m                        \u001b[1;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2353\u001b[0m                        \u001b[1;34m'`fit()` with some data, or specify '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "# Sequeatial Model \n",
    "model.add(LSTM(20, input_shape=(12, 1))) # (timestep, feature) \n",
    "model.add(Dense(1)) \n",
    "# output = 1\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T03:16:34.411841Z",
     "start_time": "2021-04-15T03:16:34.030864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, None, 64)          5600448   \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, None, 32)          12416     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 5,613,443\n",
      "Trainable params: 5,613,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, GlobalMaxPool1D, Dropout\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_len, output_dim=64))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T03:29:41.008590Z",
     "start_time": "2021-04-15T03:17:06.590164Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0979 - acc: 0.4125\n",
      "Epoch 00001: val_acc improved from -inf to 0.45000, saving model to best_model.h5\n",
      "2/2 [==============================] - 37s 18s/step - loss: 1.0979 - acc: 0.4125 - val_loss: 1.0974 - val_acc: 0.4500\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0976 - acc: 0.4250\n",
      "Epoch 00002: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0976 - acc: 0.4250 - val_loss: 1.0960 - val_acc: 0.4500\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0970 - acc: 0.4000\n",
      "Epoch 00003: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0970 - acc: 0.4000 - val_loss: 1.0950 - val_acc: 0.4500\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0963 - acc: 0.4000\n",
      "Epoch 00004: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0963 - acc: 0.4000 - val_loss: 1.0942 - val_acc: 0.4500\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0965 - acc: 0.4000\n",
      "Epoch 00005: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0965 - acc: 0.4000 - val_loss: 1.0934 - val_acc: 0.4500\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0942 - acc: 0.4000\n",
      "Epoch 00006: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0942 - acc: 0.4000 - val_loss: 1.0925 - val_acc: 0.4500\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0953 - acc: 0.4000\n",
      "Epoch 00007: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0953 - acc: 0.4000 - val_loss: 1.0915 - val_acc: 0.4500\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0937 - acc: 0.4000\n",
      "Epoch 00008: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0937 - acc: 0.4000 - val_loss: 1.0905 - val_acc: 0.4500\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0949 - acc: 0.4000\n",
      "Epoch 00009: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0949 - acc: 0.4000 - val_loss: 1.0897 - val_acc: 0.4500\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0935 - acc: 0.4000\n",
      "Epoch 00010: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0935 - acc: 0.4000 - val_loss: 1.0888 - val_acc: 0.4500\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0931 - acc: 0.4000\n",
      "Epoch 00011: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0931 - acc: 0.4000 - val_loss: 1.0881 - val_acc: 0.4500\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0940 - acc: 0.4000\n",
      "Epoch 00012: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0940 - acc: 0.4000 - val_loss: 1.0874 - val_acc: 0.4500\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0979 - acc: 0.4000\n",
      "Epoch 00013: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0979 - acc: 0.4000 - val_loss: 1.0869 - val_acc: 0.4500\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0917 - acc: 0.4000\n",
      "Epoch 00014: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0917 - acc: 0.4000 - val_loss: 1.0865 - val_acc: 0.4500\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0931 - acc: 0.4000\n",
      "Epoch 00015: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0931 - acc: 0.4000 - val_loss: 1.0860 - val_acc: 0.4500\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0928 - acc: 0.4000\n",
      "Epoch 00016: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0928 - acc: 0.4000 - val_loss: 1.0856 - val_acc: 0.4500\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0922 - acc: 0.4000\n",
      "Epoch 00017: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0922 - acc: 0.4000 - val_loss: 1.0851 - val_acc: 0.4500\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0905 - acc: 0.4000\n",
      "Epoch 00018: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0905 - acc: 0.4000 - val_loss: 1.0846 - val_acc: 0.4500\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0928 - acc: 0.4000\n",
      "Epoch 00019: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0928 - acc: 0.4000 - val_loss: 1.0840 - val_acc: 0.4500\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0914 - acc: 0.4000\n",
      "Epoch 00020: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0914 - acc: 0.4000 - val_loss: 1.0835 - val_acc: 0.4500\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0908 - acc: 0.4000\n",
      "Epoch 00021: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0908 - acc: 0.4000 - val_loss: 1.0830 - val_acc: 0.4500\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0922 - acc: 0.4000\n",
      "Epoch 00022: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0922 - acc: 0.4000 - val_loss: 1.0824 - val_acc: 0.4500\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0899 - acc: 0.4000\n",
      "Epoch 00023: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0899 - acc: 0.4000 - val_loss: 1.0818 - val_acc: 0.4500\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0906 - acc: 0.4000\n",
      "Epoch 00024: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0906 - acc: 0.4000 - val_loss: 1.0813 - val_acc: 0.4500\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0907 - acc: 0.4000\n",
      "Epoch 00025: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0907 - acc: 0.4000 - val_loss: 1.0808 - val_acc: 0.4500\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0880 - acc: 0.4000\n",
      "Epoch 00026: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0880 - acc: 0.4000 - val_loss: 1.0802 - val_acc: 0.4500\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0895 - acc: 0.4000\n",
      "Epoch 00027: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 4s/step - loss: 1.0895 - acc: 0.4000 - val_loss: 1.0797 - val_acc: 0.4500\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0893 - acc: 0.4000\n",
      "Epoch 00028: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 8s 4s/step - loss: 1.0893 - acc: 0.4000 - val_loss: 1.0793 - val_acc: 0.4500\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0869 - acc: 0.4000\n",
      "Epoch 00029: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0869 - acc: 0.4000 - val_loss: 1.0790 - val_acc: 0.4500\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0881 - acc: 0.4000\n",
      "Epoch 00030: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0881 - acc: 0.4000 - val_loss: 1.0785 - val_acc: 0.4500\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0900 - acc: 0.4000\n",
      "Epoch 00031: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0900 - acc: 0.4000 - val_loss: 1.0780 - val_acc: 0.4500\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0915 - acc: 0.4000\n",
      "Epoch 00032: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0915 - acc: 0.4000 - val_loss: 1.0776 - val_acc: 0.4500\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 1.0893 - acc: 0.4000\n",
      "Epoch 00033: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0893 - acc: 0.4000 - val_loss: 1.0773 - val_acc: 0.4500\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0905 - acc: 0.4000\n",
      "Epoch 00034: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0905 - acc: 0.4000 - val_loss: 1.0770 - val_acc: 0.4500\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0889 - acc: 0.4000\n",
      "Epoch 00035: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0889 - acc: 0.4000 - val_loss: 1.0766 - val_acc: 0.4500\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0882 - acc: 0.4000\n",
      "Epoch 00036: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0882 - acc: 0.4000 - val_loss: 1.0761 - val_acc: 0.4500\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0886 - acc: 0.4000\n",
      "Epoch 00037: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0886 - acc: 0.4000 - val_loss: 1.0755 - val_acc: 0.4500\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0921 - acc: 0.4000\n",
      "Epoch 00038: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0921 - acc: 0.4000 - val_loss: 1.0749 - val_acc: 0.4500\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0963 - acc: 0.4000\n",
      "Epoch 00039: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0963 - acc: 0.4000 - val_loss: 1.0744 - val_acc: 0.4500\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0851 - acc: 0.4000\n",
      "Epoch 00040: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0851 - acc: 0.4000 - val_loss: 1.0738 - val_acc: 0.4500\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0821 - acc: 0.4000\n",
      "Epoch 00041: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0821 - acc: 0.4000 - val_loss: 1.0731 - val_acc: 0.4500\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0867 - acc: 0.4000\n",
      "Epoch 00042: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0867 - acc: 0.4000 - val_loss: 1.0727 - val_acc: 0.4500\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0897 - acc: 0.4000\n",
      "Epoch 00043: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0897 - acc: 0.4000 - val_loss: 1.0726 - val_acc: 0.4500\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0929 - acc: 0.4000\n",
      "Epoch 00044: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0929 - acc: 0.4000 - val_loss: 1.0726 - val_acc: 0.4500\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0911 - acc: 0.4000\n",
      "Epoch 00045: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0911 - acc: 0.4000 - val_loss: 1.0727 - val_acc: 0.4500\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0847 - acc: 0.4000\n",
      "Epoch 00046: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0847 - acc: 0.4000 - val_loss: 1.0727 - val_acc: 0.4500\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0870 - acc: 0.4000\n",
      "Epoch 00047: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0870 - acc: 0.4000 - val_loss: 1.0728 - val_acc: 0.4500\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0938 - acc: 0.4000\n",
      "Epoch 00048: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0938 - acc: 0.4000 - val_loss: 1.0729 - val_acc: 0.4500\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0919 - acc: 0.4000\n",
      "Epoch 00049: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0919 - acc: 0.4000 - val_loss: 1.0728 - val_acc: 0.4500\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0872 - acc: 0.4000\n",
      "Epoch 00050: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0872 - acc: 0.4000 - val_loss: 1.0728 - val_acc: 0.4500\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0835 - acc: 0.4000\n",
      "Epoch 00051: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0835 - acc: 0.4000 - val_loss: 1.0727 - val_acc: 0.4500\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0985 - acc: 0.4000\n",
      "Epoch 00052: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0985 - acc: 0.4000 - val_loss: 1.0727 - val_acc: 0.4500\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0884 - acc: 0.4000\n",
      "Epoch 00053: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0884 - acc: 0.4000 - val_loss: 1.0727 - val_acc: 0.4500\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0841 - acc: 0.4000\n",
      "Epoch 00054: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0841 - acc: 0.4000 - val_loss: 1.0728 - val_acc: 0.4500\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0876 - acc: 0.4000\n",
      "Epoch 00055: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0876 - acc: 0.4000 - val_loss: 1.0729 - val_acc: 0.4500\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0881 - acc: 0.4000\n",
      "Epoch 00056: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0881 - acc: 0.4000 - val_loss: 1.0730 - val_acc: 0.4500\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0934 - acc: 0.4000\n",
      "Epoch 00057: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0934 - acc: 0.4000 - val_loss: 1.0730 - val_acc: 0.4500\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0862 - acc: 0.4000\n",
      "Epoch 00058: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0862 - acc: 0.4000 - val_loss: 1.0730 - val_acc: 0.4500\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0919 - acc: 0.4000\n",
      "Epoch 00059: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0919 - acc: 0.4000 - val_loss: 1.0732 - val_acc: 0.4500\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0924 - acc: 0.4000\n",
      "Epoch 00060: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0924 - acc: 0.4000 - val_loss: 1.0732 - val_acc: 0.4500\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0917 - acc: 0.4000\n",
      "Epoch 00061: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0917 - acc: 0.4000 - val_loss: 1.0733 - val_acc: 0.4500\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0875 - acc: 0.4000\n",
      "Epoch 00062: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0875 - acc: 0.4000 - val_loss: 1.0734 - val_acc: 0.4500\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0872 - acc: 0.4000\n",
      "Epoch 00063: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0872 - acc: 0.4000 - val_loss: 1.0735 - val_acc: 0.4500\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0884 - acc: 0.4000\n",
      "Epoch 00064: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0884 - acc: 0.4000 - val_loss: 1.0738 - val_acc: 0.4500\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0961 - acc: 0.4000\n",
      "Epoch 00065: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0961 - acc: 0.4000 - val_loss: 1.0742 - val_acc: 0.4500\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0865 - acc: 0.4000\n",
      "Epoch 00066: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 4s/step - loss: 1.0865 - acc: 0.4000 - val_loss: 1.0746 - val_acc: 0.4500\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0888 - acc: 0.4000\n",
      "Epoch 00067: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0888 - acc: 0.4000 - val_loss: 1.0752 - val_acc: 0.4500\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0918 - acc: 0.4000\n",
      "Epoch 00068: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0918 - acc: 0.4000 - val_loss: 1.0757 - val_acc: 0.4500\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0839 - acc: 0.4000\n",
      "Epoch 00069: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0839 - acc: 0.4000 - val_loss: 1.0762 - val_acc: 0.4500\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0855 - acc: 0.4000\n",
      "Epoch 00070: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0855 - acc: 0.4000 - val_loss: 1.0763 - val_acc: 0.4500\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0869 - acc: 0.4000\n",
      "Epoch 00071: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0869 - acc: 0.4000 - val_loss: 1.0764 - val_acc: 0.4500\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0942 - acc: 0.4000\n",
      "Epoch 00072: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0942 - acc: 0.4000 - val_loss: 1.0765 - val_acc: 0.4500\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0930 - acc: 0.4000\n",
      "Epoch 00073: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0930 - acc: 0.4000 - val_loss: 1.0766 - val_acc: 0.4500\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0860 - acc: 0.4000\n",
      "Epoch 00074: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0860 - acc: 0.4000 - val_loss: 1.0766 - val_acc: 0.4500\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0857 - acc: 0.4000\n",
      "Epoch 00075: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0857 - acc: 0.4000 - val_loss: 1.0765 - val_acc: 0.4500\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0880 - acc: 0.4000\n",
      "Epoch 00076: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0880 - acc: 0.4000 - val_loss: 1.0764 - val_acc: 0.4500\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0918 - acc: 0.4000\n",
      "Epoch 00077: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0918 - acc: 0.4000 - val_loss: 1.0762 - val_acc: 0.4500\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0858 - acc: 0.4000\n",
      "Epoch 00078: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0858 - acc: 0.4000 - val_loss: 1.0762 - val_acc: 0.4500\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0903 - acc: 0.4000\n",
      "Epoch 00079: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0903 - acc: 0.4000 - val_loss: 1.0761 - val_acc: 0.4500\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0883 - acc: 0.4000\n",
      "Epoch 00080: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0883 - acc: 0.4000 - val_loss: 1.0761 - val_acc: 0.4500\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0882 - acc: 0.4000\n",
      "Epoch 00081: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0882 - acc: 0.4000 - val_loss: 1.0760 - val_acc: 0.4500\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0895 - acc: 0.4000\n",
      "Epoch 00082: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0895 - acc: 0.4000 - val_loss: 1.0759 - val_acc: 0.4500\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0904 - acc: 0.4000\n",
      "Epoch 00083: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0904 - acc: 0.4000 - val_loss: 1.0759 - val_acc: 0.4500\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0888 - acc: 0.4000\n",
      "Epoch 00084: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0888 - acc: 0.4000 - val_loss: 1.0760 - val_acc: 0.4500\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0895 - acc: 0.4000\n",
      "Epoch 00085: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0895 - acc: 0.4000 - val_loss: 1.0761 - val_acc: 0.4500\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0881 - acc: 0.4000\n",
      "Epoch 00086: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0881 - acc: 0.4000 - val_loss: 1.0762 - val_acc: 0.4500\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0886 - acc: 0.4000\n",
      "Epoch 00087: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0886 - acc: 0.4000 - val_loss: 1.0763 - val_acc: 0.4500\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0857 - acc: 0.4000\n",
      "Epoch 00088: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0857 - acc: 0.4000 - val_loss: 1.0763 - val_acc: 0.4500\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0951 - acc: 0.4000\n",
      "Epoch 00089: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0951 - acc: 0.4000 - val_loss: 1.0764 - val_acc: 0.4500\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0873 - acc: 0.4000\n",
      "Epoch 00090: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0873 - acc: 0.4000 - val_loss: 1.0765 - val_acc: 0.4500\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0853 - acc: 0.4000\n",
      "Epoch 00091: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0853 - acc: 0.4000 - val_loss: 1.0766 - val_acc: 0.4500\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0946 - acc: 0.4000\n",
      "Epoch 00092: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0946 - acc: 0.4000 - val_loss: 1.0767 - val_acc: 0.4500\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0907 - acc: 0.4000\n",
      "Epoch 00093: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0907 - acc: 0.4000 - val_loss: 1.0768 - val_acc: 0.4500\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0827 - acc: 0.4000\n",
      "Epoch 00094: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0827 - acc: 0.4000 - val_loss: 1.0769 - val_acc: 0.4500\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0931 - acc: 0.4000\n",
      "Epoch 00095: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0931 - acc: 0.4000 - val_loss: 1.0771 - val_acc: 0.4500\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0880 - acc: 0.4000\n",
      "Epoch 00096: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0880 - acc: 0.4000 - val_loss: 1.0774 - val_acc: 0.4500\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 1.0911 - acc: 0.4000\n",
      "Epoch 00097: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0911 - acc: 0.4000 - val_loss: 1.0775 - val_acc: 0.4500\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0905 - acc: 0.4000\n",
      "Epoch 00098: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0905 - acc: 0.4000 - val_loss: 1.0775 - val_acc: 0.4500\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0885 - acc: 0.4000\n",
      "Epoch 00099: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0885 - acc: 0.4000 - val_loss: 1.0774 - val_acc: 0.4500\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0922 - acc: 0.4000\n",
      "Epoch 00100: val_acc did not improve from 0.45000\n",
      "2/2 [==============================] - 7s 3s/step - loss: 1.0922 - acc: 0.4000 - val_loss: 1.0774 - val_acc: 0.4500\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-c40fdff1707b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"테스트 정확도: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train_mean, y_train, epochs=100, callbacks=[mc], batch_size=64, validation_split=0.2)\n",
    "\n",
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T00:02:02.278966Z",
     "start_time": "2021-04-15T00:02:02.184221Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(100, 256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(256, 3, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T00:02:02.730758Z",
     "start_time": "2021-04-15T00:02:02.720785Z"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T00:02:31.371045Z",
     "start_time": "2021-04-15T00:02:03.084811Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:372 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:385 call\n        return self._run_internal_graph(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py:247 call\n        outputs = self._convolution_op(inputs, self.kernel)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1010 convolution_v2\n        return convolution_internal(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1140 convolution_internal\n        return op(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:574 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:574 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1880 conv1d\n        result = gen_nn_ops.conv2d(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:975 conv2d\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:742 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:591 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3477 _create_op_internal\n        ret = Operation(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1974 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Negative dimension size caused by subtracting 3 from 1 for '{{node sequential_2/conv1d_1/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_2/conv1d_1/conv1d/ExpandDims, sequential_2/conv1d_1/conv1d/ExpandDims_1)' with input shapes: [?,1,1,256], [1,3,256,256].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-61daf753b9c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:372 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:385 call\n        return self._run_internal_graph(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py:247 call\n        outputs = self._convolution_op(inputs, self.kernel)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1010 convolution_v2\n        return convolution_internal(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1140 convolution_internal\n        return op(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:574 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:574 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1880 conv1d\n        result = gen_nn_ops.conv2d(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:975 conv2d\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:742 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:591 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3477 _create_op_internal\n        ret = Operation(\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1974 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\Users\\rlagy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Negative dimension size caused by subtracting 3 from 1 for '{{node sequential_2/conv1d_1/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_2/conv1d_1/conv1d/ExpandDims, sequential_2/conv1d_1/conv1d/ExpandDims_1)' with input shapes: [?,1,1,256], [1,3,256,256].\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train_mean, y_train, epochs=100, callbacks=[mc], batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T05:01:02.345302Z",
     "start_time": "2021-04-09T05:01:01.086668Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100\n  y sizes: 1974\nPlease provide data which shares the same first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-448417521361>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"테스트 정확도: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1342\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m         \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1344\u001b[1;33m         data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1345\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[0;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100\n  y sizes: 1974\nPlease provide data which shares the same first dimension."
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test_mean, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:13:21.881003Z",
     "start_time": "2021-03-31T17:13:17.218412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39774117, 0.4180528 , 0.18420602],\n",
       "       [0.93720895, 0.05255724, 0.01023376],\n",
       "       [0.53255975, 0.3266842 , 0.14075604],\n",
       "       ...,\n",
       "       [0.27636996, 0.466064  , 0.2575661 ],\n",
       "       [0.2051393 , 0.4949639 , 0.29989678],\n",
       "       [0.39454758, 0.3854387 , 0.22001368]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = loaded_model.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 평가 참고 \n",
    "\n",
    "- https://ayoteralab.tistory.com/entry/Iris-dataset-classification-with-Keras?category=873956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T16:21:22.083815Z",
     "start_time": "2021-04-01T16:21:20.936885Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a74692dd2eda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:13:31.208117Z",
     "start_time": "2021-03-31T17:13:30.879991Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-2f3d6900dd4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scatter with Sepal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scatter with Petal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAFpCAYAAABUNF3EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAT3ElEQVR4nO3dX4jl93nf8c/T3QgaJ41MtAmu/hC1yFZ0YRV7opiStEpDa0m9WAK+kBwiIgJC1Aq5tCg0ufBNc1EIxnKWxQjhm+iiEcmmKBGFkrjgqtUKbNmykdnKRNrKICkOLthQsfbTi5m205lzZs7snplZPfN6wcD+zvlq5jtfds+j95wzM9XdAQAAmOTvHPcGAAAA1k3oAAAA4wgdAABgHKEDAACMI3QAAIBxhA4AADDOvqFTVU9V1VtV9fUl91dVfbaqLlXVy1X1kfVvEwAWM6cAWGSVZ3SeTnLfHvffn+SOrbdHk/zhtW8LAFb2dMwpAHbYN3S6+0tJvrvHkrNJvtibXkhyY1V9YF0bBIC9mFMALLKO79G5Ockb264vb90GANcDcwrgBDq9hvdRC27rhQurHs3mywbyvve976N33nnnGj48AFfrpZdeeqe7zxz3Pg6ZOQXwHnUtc2odoXM5ya3brm9J8uaihd19Psn5JNnY2OiLFy+u4cMDcLWq6q+Pew9HwJwCeI+6ljm1jpeuXUjy8NZPtflYku9193fW8H4BYB3MKYATaN9ndKrqj5Lcm+Smqrqc5PeS/FiSdPe5JM8leSDJpSQ/SPLIYW0WAHYypwBYZN/Q6e6H9rm/k3xqbTsCgAMwpwBYZB0vXQMAALiuCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYZ6XQqar7qurVqrpUVU8suP+nqurPquqrVfVKVT2y/q0CwGLmFAA77Rs6VXUqyZNJ7k9yV5KHququHcs+leQb3X13knuT/LuqumHNewWAXcwpABZZ5Rmde5Jc6u7XuvvdJM8kObtjTSf5yaqqJD+R5LtJrqx1pwCwmDkFwC6rhM7NSd7Ydn1567btPpfk55O8meRrSX6nu3+0lh0CwN7MKQB2WSV0asFtveP640m+kuTvJ/lHST5XVX9v1zuqerSqLlbVxbfffvvAmwWABcwpAHZZJXQuJ7l12/Ut2fyK2HaPJHm2N11K8u0kd+58R919vrs3unvjzJkzV7tnANjOnAJgl1VC58Ukd1TV7VvfuPlgkgs71rye5FeTpKp+NsmHkry2zo0CwBLmFAC7nN5vQXdfqarHkzyf5FSSp7r7lap6bOv+c0k+k+TpqvpaNl9C8OnufucQ9w0AScwpABbbN3SSpLufS/LcjtvObfvzm0n+xXq3BgCrMacA2GmlXxgKAADwXiJ0AACAcYQOAAAwjtABAADGEToAAMA4QgcAABhH6AAAAOMIHQAAYByhAwAAjCN0AACAcYQOAAAwjtABAADGEToAAMA4QgcAABhH6AAAAOMIHQAAYByhAwAAjCN0AACAcYQOAAAwjtABAADGEToAAMA4QgcAABhH6AAAAOMIHQAAYByhAwAAjCN0AACAcYQOAAAwjtABAADGEToAAMA4QgcAABhH6AAAAOMIHQAAYByhAwAAjCN0AACAcYQOAAAwjtABAADGEToAAMA4QgcAABhH6AAAAOMIHQAAYByhAwAAjCN0AACAcYQOAAAwjtABAADGEToAAMA4QgcAABhH6AAAAOMIHQAAYByhAwAAjCN0AACAcYQOAAAwjtABAADGEToAAMA4QgcAABhH6AAAAOMIHQAAYByhAwAAjLNS6FTVfVX1alVdqqonlqy5t6q+UlWvVNVfrXebALCcOQXATqf3W1BVp5I8meSfJ7mc5MWqutDd39i25sYkn09yX3e/XlU/c1gbBoDtzCkAFlnlGZ17klzq7te6+90kzyQ5u2PNJ5M8292vJ0l3v7XebQLAUuYUALusEjo3J3lj2/Xlrdu2+2CS91fVX1bVS1X18KJ3VFWPVtXFqrr49ttvX92OAeD/Z04BsMsqoVMLbusd16eTfDTJv0zy8ST/pqo+uOs/6j7f3RvdvXHmzJkDbxYAFjCnANhl3+/RyeZXxm7ddn1LkjcXrHmnu7+f5PtV9aUkdyf51lp2CQDLmVMA7LLKMzovJrmjqm6vqhuSPJjkwo41f5rkl6vqdFX9eJJfTPLN9W4VABYypwDYZd9ndLr7SlU9nuT5JKeSPNXdr1TVY1v3n+vub1bVXyR5OcmPknyhu79+mBsHgMScAmCx6t75MuajsbGx0RcvXjyWjw3Apqp6qbs3jnsf1yNzCuD4XcucWukXhgIAALyXCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYZ6XQqar7qurVqrpUVU/sse4XquqHVfWJ9W0RAPZmTgGw076hU1WnkjyZ5P4kdyV5qKruWrLu95M8v+5NAsAy5hQAi6zyjM49SS5192vd/W6SZ5KcXbDut5P8cZK31rg/ANiPOQXALquEzs1J3th2fXnrtv+rqm5O8mtJzu31jqrq0aq6WFUX33777YPuFQAWMacA2GWV0KkFt/WO6z9I8unu/uFe76i7z3f3RndvnDlzZtU9AsBezCkAdjm9wprLSW7ddn1Lkjd3rNlI8kxVJclNSR6oqivd/Sdr2SUALGdOAbDLKqHzYpI7qur2JP8jyYNJPrl9QXff/n/+XFVPJ/kPhgcAR8ScAmCXfUOnu69U1ePZ/Ck1p5I81d2vVNVjW/fv+XpnADhM5hQAi6zyjE66+7kkz+24beHg6O7fvPZtAcDqzCkAdlrpF4YCAAC8lwgdAABgHKEDAACMI3QAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGEfoAAAA4wgdAABgHKEDAACMI3QAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGEfoAAAA4wgdAABgHKEDAACMI3QAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGEfoAAAA4wgdAABgHKEDAACMI3QAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGEfoAAAA4wgdAABgHKEDAACMI3QAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGEfoAAAA4wgdAABgHKEDAACMI3QAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGEfoAAAA4wgdAABgHKEDAACMI3QAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGGel0Kmq+6rq1aq6VFVPLLj/16vq5a23L1fV3evfKgAsZk4BsNO+oVNVp5I8meT+JHcleaiq7tqx7NtJ/ml3fzjJZ5KcX/dGAWARcwqARVZ5RueeJJe6+7XufjfJM0nObl/Q3V/u7r/dunwhyS3r3SYALGVOAbDLKqFzc5I3tl1f3rptmd9K8ufXsikAOABzCoBdTq+wphbc1gsXVv1KNgfILy25/9EkjybJbbfdtuIWAWBP5hQAu6zyjM7lJLduu74lyZs7F1XVh5N8IcnZ7v6bRe+ou89390Z3b5w5c+Zq9gsAO5lTAOyySui8mOSOqrq9qm5I8mCSC9sXVNVtSZ5N8hvd/a31bxMAljKnANhl35eudfeVqno8yfNJTiV5qrtfqarHtu4/l+R3k/x0ks9XVZJc6e6Nw9s2AGwypwBYpLoXvoz50G1sbPTFixeP5WMDsKmqXvI//IuZUwDH71rm1Eq/MBQAAOC9ROgAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOEIHAAAYR+gAAADjCB0AAGAcoQMAAIwjdAAAgHGEDgAAMI7QAQAAxhE6AADAOCuFTlXdV1WvVtWlqnpiwf1VVZ/duv/lqvrI+rcKAIuZUwDstG/oVNWpJE8muT/JXUkeqqq7diy7P8kdW2+PJvnDNe8TABYypwBYZJVndO5Jcqm7X+vud5M8k+TsjjVnk3yxN72Q5Maq+sCa9woAi5hTAOyySujcnOSNbdeXt2476BoAOAzmFAC7nF5hTS24ra9iTarq0Wy+ZCBJ/ldVfX2Fj38S3ZTknePexHXK2SznbJZzNst96Lg3sAbm1NHzb2o5Z7Ocs1nO2Sx31XNqldC5nOTWbde3JHnzKtaku88nOZ8kVXWxuzcOtNsTwtks52yWczbLOZvlqurice9hDcypI+ZslnM2yzmb5ZzNctcyp1Z56dqLSe6oqtur6oYkDya5sGPNhSQPb/1Um48l+V53f+dqNwUAB2BOAbDLvs/odPeVqno8yfNJTiV5qrtfqarHtu4/l+S5JA8kuZTkB0keObwtA8D/Y04BsMgqL11Ldz+XzSGx/bZz2/7cST51wI99/oDrTxJns5yzWc7ZLOdslhtxNubUkXM2yzmb5ZzNcs5muas+m9p87AcAAJhjle/RAQAAeE859NCpqvuq6tWqulRVTyy4v6rqs1v3v1xVHznsPV0vVjibX986k5er6stVdfdx7PM47Hc229b9QlX9sKo+cZT7O06rnE1V3VtVX6mqV6rqr456j8dlhX9TP1VVf1ZVX906mxPxfRpV9VRVvbXsRyWf5MfhxJzaizm1nDm1nDm1nDm13KHMqu4+tLdsflPof0/yD5LckOSrSe7aseaBJH+ezd9x8LEk//Uw93S9vK14Nv84yfu3/ny/s1m47j9l83X5nzjufV8vZ5PkxiTfSHLb1vXPHPe+r6Oz+ddJfn/rz2eSfDfJDce99yM4m3+S5CNJvr7k/hP5OHyAvzcn8nzMqWs7m23rzClz6iBncyLn1Nbnu/ZZddjP6NyT5FJ3v9bd7yZ5JsnZHWvOJvlib3ohyY1V9YFD3tf1YN+z6e4vd/ffbl2+kM3f+3ASrPL3Jkl+O8kfJ3nrKDd3zFY5m08meba7X0+S7j4p57PK2XSSn6yqSvIT2RwgV452m0evu7+Uzc91mZP6OJyYU3sxp5Yzp5Yzp5Yzp/ZwGLPqsEPn5iRvbLu+vHXbQddMdNDP+7eyWbEnwb5nU1U3J/m1JOdysqzy9+aDSd5fVX9ZVS9V1cNHtrvjtcrZfC7Jz2fzF0V+LcnvdPePjmZ717WT+jicmFN7MaeWM6eWM6eWM6euzYEfi1f68dLXoBbctvPHvK2yZqKVP++q+pVsDpBfOtQdXT9WOZs/SPLp7v7h5hc9ToxVzuZ0ko8m+dUkfzfJf6mqF7r7W4e9uWO2ytl8PMlXkvyzJP8wyX+sqv/c3f/zsDd3nTupj8OJObUXc2o5c2o5c2o5c+raHPix+LBD53KSW7dd35LNQj3omolW+ryr6sNJvpDk/u7+myPa23Fb5Ww2kjyzNTxuSvJAVV3p7j85mi0em1X/Tb3T3d9P8v2q+lKSu5NMHyCrnM0jSf5tb77Y91JVfTvJnUn+29Fs8bp1Uh+HE3NqL+bUcubUcubUcubUtTnwY/Fhv3TtxSR3VNXtVXVDkgeTXNix5kKSh7d+ksLHknyvu79zyPu6Hux7NlV1W5Jnk/zGCfgqx3b7nk13397dP9fdP5fk3yf5VydgeCSr/Zv60yS/XFWnq+rHk/xikm8e8T6Pwypn83o2v4KYqvrZJB9K8tqR7vL6dFIfhxNzai/m1HLm1HLm1HLm1LU58GPxoT6j091XqurxJM9n8ydNPNXdr1TVY1v3n8vmTyJ5IMmlJD/IZsmOt+LZ/G6Sn07y+a2vCF3p7o3j2vNRWfFsTqRVzqa7v1lVf5Hk5SQ/SvKF7l74oxonWfHvzWeSPF1VX8vmU+Cf7u53jm3TR6Sq/ijJvUluqqrLSX4vyY8lJ/txODGn9mJOLWdOLWdOLWdO7e0wZlVtPjMGAAAwx6H/wlAAAICjJnQAAIBxhA4AADCO0AEAAMYROgAAwDhCBwAAGEfoAAAA4wgdAABgnP8NSAWtsCxOj8UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(14,6))\n",
    "ax[0].scatter(dataset.data[:,0], dataset.data[:,1], c=dataset.target)\n",
    "ax[0].set_title('scatter with Sepal')\n",
    "ax[1].scatter(dataset.data[:,2], dataset.data[:,3], c=dataset.target)\n",
    "ax[1].set_title('scatter with Petal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
